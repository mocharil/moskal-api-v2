{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aecf78a",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff89b76",
   "metadata": {},
   "source": [
    "Topics dan KOL belum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cbd40",
   "metadata": {},
   "source": [
    "## keyword trends V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81741b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:16:58.774503Z",
     "start_time": "2025-04-17T11:16:58.746274Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from utils.keyword_trends import get_keyword_trends\n",
    "result = get_keyword_trends(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    \n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6439c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:27:50.138012Z",
     "start_time": "2025-04-17T05:27:50.111428Z"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3a0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c08cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e82c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73cb8ba9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## context of discussion V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8260d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:28:36.279645Z",
     "start_time": "2025-04-17T05:28:36.235043Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.context_of_disccusion import get_context_of_discussion\n",
    "result = get_context_of_discussion(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b92b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:28:39.122675Z",
     "start_time": "2025-04-17T05:28:39.107211Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b341e9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "024831ff",
   "metadata": {},
   "source": [
    "## list of mentions Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f6800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:24:46.142578Z",
     "start_time": "2025-04-17T11:24:46.090992Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "\n",
    "# Contoh penggunaan dasar\n",
    "result = get_mentions(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    source = [\"post_created_at\",\"issue\"],\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],\n",
    "    page=1,\n",
    "    page_size=3000,\n",
    "    sort_type=\"relevant\",  # Sort berdasarkan viral_score\n",
    "    sort_order=\"desc\",\n",
    ")\n",
    "\n",
    "result['pagination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d75ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:24:48.067773Z",
     "start_time": "2025-04-17T11:24:48.059738Z"
    },
    "code_folding": [
     21,
     49
    ]
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829e66b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# analysis DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e75235",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## overview X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94126ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:40:20.199454Z",
     "start_time": "2025-04-17T10:40:20.084438Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from utils.analysis_overview import get_social_media_matrix\n",
    "matrix = get_social_media_matrix(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\",'dan'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-04-01',\n",
    "    end_date='2025-04-10',)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c00d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1208a174",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## mentions by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e757553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:36.121428Z",
     "start_time": "2025-04-17T08:45:36.058923Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_category_analytics.py - Script untuk mendapatkan analisis kategori dan sentimen\n",
    "\n",
    "Script ini mengambil data tentang mentions berdasarkan kategori dan sentiment berdasarkan\n",
    "kategori dari Elasticsearch untuk visualisasi.\n",
    "\"\"\"\n",
    "from utils.analysis_sentiment_mentions import get_category_analytics\n",
    "\n",
    "data= get_category_analytics(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e76c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:37.702530Z",
     "start_time": "2025-04-17T08:45:37.685295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30cdd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:15.528427Z",
     "start_time": "2025-04-17T08:45:15.515683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint({\n",
    "    \"mentions_by_category\":mentions_by_category,\n",
    "    'sentiemnt_by_category':sentiment_by_category,\n",
    "    'sentiment_breakdown':sentiment_breakdown\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401578ff",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## presence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897d36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:47:36.652282Z",
     "start_time": "2025-04-17T08:47:36.555309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.presence_score import get_presence_score, format_presence_score_data\n",
    "presence_data = get_presence_score(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])\n",
    "\n",
    "# Format data untuk tampilan yang lebih baik\n",
    "formatted_data = format_presence_score_data(presence_data)\n",
    "\n",
    "\n",
    "formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b336a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:38:06.490527Z",
     "start_time": "2025-04-17T04:38:06.429218Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4892b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:37:45.705193Z",
     "start_time": "2025-04-17T04:37:45.700253Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "presence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a49fd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:15:11.174708Z",
     "start_time": "2025-04-16T05:15:11.140661Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## most share of voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727b6b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:22:34.500572Z",
     "start_time": "2025-04-17T07:22:34.488736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.share_of_voice import get_share_of_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df367653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:43:57.180454Z",
     "start_time": "2025-04-17T04:43:57.160373Z"
    },
    "code_folding": [
     16,
     42
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47995ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:22:35.422668Z",
     "start_time": "2025-04-17T07:22:35.387444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_share_of_voice(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0fdcb7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## most followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf773e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:43:49.408511Z",
     "start_time": "2025-04-17T07:43:49.389139Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.most_followers import get_most_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc2869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:45:48.044686Z",
     "start_time": "2025-04-17T07:45:48.022524Z"
    },
    "code_folding": [
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "most_followers.py - Script untuk mendapatkan akun dengan followers/koneksi terbanyak\n",
    "\n",
    "Script ini menganalisis data dari Elasticsearch untuk menentukan akun\n",
    "dengan jumlah followers/koneksi terbanyak yang membicarakan suatu topik,\n",
    "dengan dukungan pagination.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Literal, Optional, Union\n",
    "\n",
    "# Import utilitas dari paket utils\n",
    "from utils.es_client import get_elasticsearch_client\n",
    "from utils.es_query_builder import get_date_range\n",
    "\n",
    "def get_most_followers(\n",
    "    es_host='localhost:9200',\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None,\n",
    "    limit=10,\n",
    "    page=1,\n",
    "    page_size=10,\n",
    "    include_total_count=True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Mendapatkan daftar akun dengan jumlah followers terbanyak\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    es_host : str\n",
    "        Host Elasticsearch\n",
    "    es_username : str, optional\n",
    "        Username Elasticsearch\n",
    "    es_password : str, optional\n",
    "        Password Elasticsearch\n",
    "    use_ssl : bool, optional\n",
    "        Gunakan SSL untuk koneksi\n",
    "    verify_certs : bool, optional\n",
    "        Verifikasi sertifikat SSL\n",
    "    ca_certs : str, optional\n",
    "        Path ke sertifikat CA\n",
    "    keywords : list, optional\n",
    "        Daftar keyword untuk filter\n",
    "    search_exact_phrases : bool, optional\n",
    "        Jika True, gunakan match_phrase untuk pencarian keyword, jika False gunakan match AND\n",
    "    case_sensitive : bool, optional\n",
    "        Jika True, pencarian keyword bersifat case-sensitive, jika False tidak memperhatikan huruf besar/kecil\n",
    "    sentiment : list, optional\n",
    "        Daftar sentiment ['positive', 'negative', 'neutral']\n",
    "    start_date : str, optional\n",
    "        Tanggal awal format YYYY-MM-DD\n",
    "    end_date : str, optional\n",
    "        Tanggal akhir format YYYY-MM-DD\n",
    "    date_filter : str, optional\n",
    "        Filter tanggal untuk digunakan jika start_date dan end_date tidak disediakan\n",
    "    custom_start_date : str, optional\n",
    "        Tanggal awal kustom jika date_filter adalah \"custom\"\n",
    "    custom_end_date : str, optional\n",
    "        Tanggal akhir kustom jika date_filter adalah \"custom\"\n",
    "    channels : list, optional\n",
    "        Daftar channel ['twitter', 'news', 'instagram', dll]\n",
    "    importance : str, optional\n",
    "        'important mentions' atau 'all mentions'\n",
    "    influence_score_min : float, optional\n",
    "        Skor pengaruh minimum (0-100)\n",
    "    influence_score_max : float, optional\n",
    "        Skor pengaruh maksimum (0-100)\n",
    "    region : list, optional\n",
    "        Daftar region\n",
    "    language : list, optional\n",
    "        Daftar bahasa\n",
    "    domain : list, optional\n",
    "        Daftar domain untuk filter\n",
    "    limit : int, optional\n",
    "        Jumlah total akun yang akan dianalisis (untuk keseluruhan dataset)\n",
    "    page : int, optional\n",
    "        Halaman yang akan diambil (untuk pagination)\n",
    "    page_size : int, optional\n",
    "        Jumlah item per halaman (untuk pagination)\n",
    "    include_total_count : bool, optional\n",
    "        Sertakan jumlah total akun di hasil\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary berisi daftar akun dengan jumlah followers terbanyak dengan dukungan pagination\n",
    "    \"\"\"\n",
    "    # Buat koneksi Elasticsearch\n",
    "    es = get_elasticsearch_client(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs\n",
    "    )\n",
    "    \n",
    "    if not es:\n",
    "        return {\n",
    "            \"data\": [],\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": 0,\n",
    "                \"total_items\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Definisikan semua channel yang mungkin\n",
    "    default_channels = [\"twitter\", \"linkedin\", \"reddit\", \"youtube\", \"news\"]\n",
    "    \n",
    "    # Filter channels jika disediakan\n",
    "    if channels:\n",
    "        selected_channels = [ch for ch in channels if ch in default_channels]\n",
    "    else:\n",
    "        selected_channels = default_channels\n",
    "    \n",
    "    # Mapping channel ke index Elasticsearch\n",
    "    channel_to_index = {\n",
    "        \"twitter\": \"twitter_data\",\n",
    "        \"instagram\": \"instagram_data\",\n",
    "        \"linkedin\": \"linkedin_data\",\n",
    "        \"reddit\": \"reddit_data\",\n",
    "        \"youtube\": \"youtube_data\",\n",
    "        \"tiktok\": \"tiktok_data\",\n",
    "        \"news\": \"news_data\",\n",
    "        \"blogs\": \"blogs_data\",\n",
    "        \"facebook\": \"facebook_data\",\n",
    "        \"podcasts\": \"podcasts_data\",\n",
    "        \"videos\": \"videos_data\",\n",
    "        \"web\": \"web_data\"\n",
    "    }\n",
    "    \n",
    "    # Dapatkan indeks yang akan di-query\n",
    "    indices = [channel_to_index[ch] for ch in selected_channels if ch in channel_to_index]\n",
    "    \n",
    "    if not indices:\n",
    "        print(\"Error: No valid indices\")\n",
    "        return {\n",
    "            \"data\": [],\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": 0,\n",
    "                \"total_items\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Dapatkan rentang tanggal jika tidak disediakan\n",
    "    if not start_date or not end_date:\n",
    "        start_date, end_date = get_date_range(\n",
    "            date_filter=date_filter,\n",
    "            custom_start_date=custom_start_date,\n",
    "            custom_end_date=custom_end_date\n",
    "        )\n",
    "    \n",
    "    # Bangun query untuk mendapatkan akun dengan followers terbanyak\n",
    "    must_conditions = [\n",
    "        {\n",
    "            \"range\": {\n",
    "                \"post_created_at\": {\n",
    "                    \"gte\": start_date,\n",
    "                    \"lte\": end_date\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Tambahkan filter keywords jika ada\n",
    "    if keywords:\n",
    "        # Konversi keywords ke list jika belum\n",
    "        keyword_list = keywords if isinstance(keywords, list) else [keywords]\n",
    "        keyword_should_conditions = []\n",
    "        \n",
    "        # Tentukan field yang akan digunakan berdasarkan case_sensitive\n",
    "        caption_field = \"post_caption.keyword\" if case_sensitive else \"post_caption\"\n",
    "        issue_field = \"issue.keyword\" if case_sensitive else \"issue\"\n",
    "        \n",
    "        if search_exact_phrases:\n",
    "            # Gunakan match_phrase untuk exact matching\n",
    "            for kw in keyword_list:\n",
    "                keyword_should_conditions.append({\"match_phrase\": {caption_field: kw}})\n",
    "                keyword_should_conditions.append({\"match_phrase\": {issue_field: kw}})\n",
    "        else:\n",
    "            # Gunakan match dengan operator AND\n",
    "            for kw in keyword_list:\n",
    "                keyword_should_conditions.append({\"match\": {caption_field: {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "                keyword_should_conditions.append({\"match\": {issue_field: {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        \n",
    "        keyword_condition = {\n",
    "            \"bool\": {\n",
    "                \"should\": keyword_should_conditions,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        must_conditions.append(keyword_condition)\n",
    "    \n",
    "    # Bangun filter untuk query\n",
    "    filter_conditions = []\n",
    "    \n",
    "    # Filter untuk sentiment\n",
    "    if sentiment:\n",
    "        sentiment_condition = {\n",
    "            \"terms\": {\n",
    "                \"sentiment\": sentiment if isinstance(sentiment, list) else [sentiment]\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(sentiment_condition)\n",
    "    \n",
    "    # Filter untuk importance\n",
    "    if importance == \"important mentions\":\n",
    "        filter_conditions.append({\n",
    "            \"range\": {\n",
    "                \"influence_score\": {\n",
    "                    \"gt\": 50\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    # Filter untuk influence score\n",
    "    if influence_score_min is not None or influence_score_max is not None:\n",
    "        influence_condition = {\"range\": {\"influence_score\": {}}}\n",
    "        if influence_score_min is not None:\n",
    "            influence_condition[\"range\"][\"influence_score\"][\"gte\"] = influence_score_min\n",
    "        if influence_score_max is not None:\n",
    "            influence_condition[\"range\"][\"influence_score\"][\"lte\"] = influence_score_max\n",
    "        filter_conditions.append(influence_condition)\n",
    "        \n",
    "    # Filter untuk region menggunakan wildcard\n",
    "    if region:\n",
    "        region_conditions = []\n",
    "        region_list = region if isinstance(region, list) else [region]\n",
    "        \n",
    "        for r in region_list:\n",
    "            region_conditions.append({\"wildcard\": {\"region\": f\"*{r}*\"}})\n",
    "        \n",
    "        region_filter = {\n",
    "            \"bool\": {\n",
    "                \"should\": region_conditions,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(region_filter)\n",
    "        \n",
    "    # Filter untuk language menggunakan wildcard\n",
    "    if language:\n",
    "        language_conditions = []\n",
    "        language_list = language if isinstance(language, list) else [language]\n",
    "        \n",
    "        for l in language_list:\n",
    "            language_conditions.append({\"wildcard\": {\"language\": f\"*{l}*\"}})\n",
    "        \n",
    "        language_filter = {\n",
    "            \"bool\": {\n",
    "                \"should\": language_conditions,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(language_filter)\n",
    "        \n",
    "    # Filter untuk domain\n",
    "    if domain:\n",
    "        domain_condition = {\n",
    "            \"bool\": {\n",
    "                \"should\": [{\"wildcard\": {\"link_post\": f\"*{d}*\"}} for d in (domain if isinstance(domain, list) else [domain])],\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(domain_condition)\n",
    "    \n",
    "    # Gabungkan semua kondisi ke dalam query utama\n",
    "    query = {\n",
    "        \"size\": 0,  # Kita hanya perlu agregasi\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": must_conditions\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"by_channel\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": \"channel\",\n",
    "                    \"size\": len(selected_channels)\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"by_username\": {\n",
    "                        \"terms\": {\n",
    "                            \"field\": \"username\",\n",
    "                            \"size\": limit\n",
    "                        },\n",
    "                        \"aggs\": {\n",
    "                            \"subscribers\": {\n",
    "                                \"max\": {\n",
    "                                    \"field\": \"subscriber\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"followers\": {\n",
    "                                \"max\": {\n",
    "                                    \"field\": \"user_followers\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"connections\": {\n",
    "                                \"max\": {\n",
    "                                    \"field\": \"user_connections\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"influence_score\": {\n",
    "                                \"avg\": {\n",
    "                                    \"field\": \"user_influence_score\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"total_reach\": {\n",
    "                                \"sum\": {\n",
    "                                    \"field\": \"reach_score\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"total_mentions\": {\n",
    "                \"value_count\": {\n",
    "                    \"field\": \"link_post\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Tambahkan filter jika ada\n",
    "    if filter_conditions:\n",
    "        query[\"query\"][\"bool\"][\"filter\"] = filter_conditions\n",
    "    \n",
    "    \n",
    "    print(query)\n",
    "    try:\n",
    "        # Jalankan query\n",
    "        response = es.search(\n",
    "            index=\",\".join(indices),\n",
    "            body=query\n",
    "        )\n",
    "        \n",
    "        # Proses hasil untuk mendapatkan akun dengan followers terbanyak\n",
    "        channel_buckets = response[\"aggregations\"][\"by_channel\"][\"buckets\"]\n",
    "        total_mentions = response[\"aggregations\"][\"total_mentions\"][\"value\"]\n",
    "        \n",
    "        # Kumpulkan data dari semua channel\n",
    "        followers_data = []\n",
    "        \n",
    "        for channel_bucket in channel_buckets:\n",
    "            channel = channel_bucket[\"key\"]\n",
    "            username_buckets = channel_bucket[\"by_username\"][\"buckets\"]\n",
    "            \n",
    "            for username_bucket in username_buckets:\n",
    "                username = username_bucket[\"key\"]\n",
    "                mentions = username_bucket[\"doc_count\"]\n",
    "                subscribers = username_bucket[\"subscribers\"][\"value\"]\n",
    "                followers = username_bucket[\"followers\"][\"value\"]\n",
    "                connections = username_bucket[\"connections\"][\"value\"]\n",
    "                influence_score = username_bucket[\"influence_score\"][\"value\"] or 0\n",
    "                reach = username_bucket[\"total_reach\"][\"value\"]\n",
    "                \n",
    "                followers_data.append({\n",
    "                    \"channel\": channel,\n",
    "                    \"username\": username,\n",
    "                    \"followers\": followers or connections or subscribers or 0,\n",
    "                    \"influence_score\": influence_score,\n",
    "                    \"total_mentions\": mentions,\n",
    "                    \"total_reach\": reach\n",
    "                })\n",
    "        \n",
    "        \n",
    "        print(followers_data)\n",
    "        # Sortir berdasarkan jumlah followers\n",
    "        followers_data.sort(key=lambda x: x[\"followers\"], reverse=True)\n",
    "        \n",
    "        # Pagination\n",
    "        total_items = len(followers_data)\n",
    "        total_pages = (total_items + page_size - 1) // page_size  # ceiling division\n",
    "        \n",
    "        start_index = (page - 1) * page_size\n",
    "        end_index = min(start_index + page_size, total_items)\n",
    "        \n",
    "        paginated_data = followers_data[start_index:end_index]\n",
    "        \n",
    "        # Format username\n",
    "        for item in paginated_data:\n",
    "            if item[\"channel\"] == \"twitter\" and not item[\"username\"].startswith(\"@\"):\n",
    "                item[\"username\"] = f\"@{item['username']}\"\n",
    "        \n",
    "        # Buat hasil dengan informasi pagination\n",
    "        result = {\n",
    "            \"data\": paginated_data,\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": total_pages,\n",
    "                \"total_items\": total_items\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Tambahkan daftar channel yang digunakan\n",
    "        result[\"channels\"] = selected_channels\n",
    "        \n",
    "        # Tambahkan total mentions keseluruhan jika diminta\n",
    "        if include_total_count:\n",
    "            result[\"total_mentions\"] = total_mentions\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Elasticsearch: {e}\")\n",
    "        return {\n",
    "            \"data\": [],\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": 0,\n",
    "                \"total_items\": 0\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d832d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:45:48.589233Z",
     "start_time": "2025-04-17T07:45:48.522116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_most_followers(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-21',\n",
    "    end_date='2025-04-30',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128d8ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c6d1a2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## trending hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d25ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:53:19.070408Z",
     "start_time": "2025-04-17T08:53:18.992762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.trending_hashtags import get_trending_hashtags\n",
    "results = get_trending_hashtags(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "   page=1,\n",
    "    page_size=5)\n",
    "\n",
    "# Display pagination info\n",
    "print(f\"Showing page {results['pagination']['page']} of {results['pagination']['total_pages']}\")\n",
    "print(f\"Total hashtags found: {results['pagination']['total_items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3894dc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## trending links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db731b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:52:25.282384Z",
     "start_time": "2025-04-16T06:52:24.884484Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.trending_links import get_trending_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906598f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:56:03.231977Z",
     "start_time": "2025-04-17T04:56:03.175531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfffcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:56:03.616692Z",
     "start_time": "2025-04-17T04:56:03.518361Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_trending_links(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfc973",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139be06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078c7575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:50:21.228378Z",
     "start_time": "2025-04-16T06:50:21.216129Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## popular emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21661eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:16:34.651513Z",
     "start_time": "2025-04-16T07:16:34.498486Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.popular_emojis import get_popular_emojis\n",
    "\n",
    "# Get top emojis from Twitter content\n",
    "emojis = get_popular_emojis(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"ruu tni\"],\n",
    "    channels=[\"twitter\",\"youtube\",'news','reddit'],\n",
    "    page=1,\n",
    "    page_size=50\n",
    ")\n",
    "\n",
    "emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2df5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:58:49.239498Z",
     "start_time": "2025-04-17T04:58:49.214744Z"
    },
    "code_folding": [
     17,
     65,
     91
    ],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac1b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:58:58.310464Z",
     "start_time": "2025-04-17T04:58:58.066456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_popular_emojis(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f89ff4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5c103",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5913947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:45:45.468884Z",
     "start_time": "2025-04-16T07:45:45.346504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.analysis_overview import get_social_media_matrix\n",
    "matrix = get_social_media_matrix(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"prabowo danantara\", \"dan\"],\n",
    "    start_date=\"2025-04-01\",\n",
    "    end_date=\"2025-04-02\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1483718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:45:48.478448Z",
     "start_time": "2025-04-16T07:45:48.458423Z"
    },
    "hidden": true
   },
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7061b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:43:57.433068Z",
     "start_time": "2025-04-16T09:43:57.350271Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.summary_stats import get_stats_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2b433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:03:36.568355Z",
     "start_time": "2025-04-17T05:03:36.523711Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_stats_summary(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dbea7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65f8694",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fba336",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sama seperti sebelumnya, tapi yg dimasukin adalah list issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a50f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## sentiment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91506f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:03:57.948660Z",
     "start_time": "2025-04-17T10:03:57.780940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "\n",
    "# Contoh penggunaan dasar\n",
    "post_positive = get_mentions(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    language = ['Indo'],\n",
    "    page=1,\n",
    "    page_size=50,\n",
    "    sentiment = ['positive'],\n",
    "    source = [\"post_caption\"],\n",
    "    sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "    sort_order=\"desc\",\n",
    ")\n",
    "\n",
    "post_negative = get_mentions(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    language = ['Indo'],\n",
    "    page=1,\n",
    "    page_size=50,\n",
    "    sentiment = ['negative'],\n",
    "    source = [\"post_caption\"],\n",
    "    sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "    sort_order=\"desc\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9a591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:15:54.284202Z",
     "start_time": "2025-04-17T10:15:54.273169Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_mentions\n",
    "from utils.gemini import call_gemini\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def get_topics_sentiment_analysis(\n",
    "    es_host='localhost:9200',\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Mengambil analisis topik berdasarkan sentimen (positif dan negatif) \n",
    "    menggunakan model Gemini untuk menganalisis konten.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary berisi rangkuman topik positif dan negatif:\n",
    "        {\n",
    "            'positive_topics': \"...\",\n",
    "            'negative_topics': \"...\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Mendapatkan post positif\n",
    "    post_positive = get_mentions(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs,\n",
    "        keywords=keywords,\n",
    "        search_exact_phrases=search_exact_phrases,\n",
    "        case_sensitive=case_sensitive,\n",
    "        sentiment=['positive'],\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        date_filter=date_filter,\n",
    "        custom_start_date=custom_start_date,\n",
    "        custom_end_date=custom_end_date,\n",
    "        channels=channels,\n",
    "        importance=importance,\n",
    "        influence_score_min=influence_score_min,\n",
    "        influence_score_max=influence_score_max,\n",
    "        region=region,\n",
    "        language=language,\n",
    "        domain=domain,\n",
    "        sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "        sort_order=\"desc\",\n",
    "    page=1,\n",
    "    page_size=50\n",
    "    )\n",
    "\n",
    "    # Mendapatkan post negatif\n",
    "    post_negative = get_mentions(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs,\n",
    "        keywords=keywords,\n",
    "        search_exact_phrases=search_exact_phrases,\n",
    "        case_sensitive=case_sensitive,\n",
    "        sentiment=['negative'],\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        date_filter=date_filter,\n",
    "        custom_start_date=custom_start_date,\n",
    "        custom_end_date=custom_end_date,\n",
    "        channels=channels,\n",
    "        importance=importance,\n",
    "        influence_score_min=influence_score_min,\n",
    "        influence_score_max=influence_score_max,\n",
    "        region=region,\n",
    "        language=language,\n",
    "        domain=domain,\n",
    "        sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "        sort_order=\"desc\",\n",
    "        page=1,\n",
    "    page_size=50\n",
    "    )\n",
    "\n",
    "    # Menyusun prompt untuk Gemini\n",
    "    prompt = f\"\"\"You are a Social Media Analyst Expert. Your task is to analyze and summarize the content based on the list of social media posts provided below. The posts are divided into two categories based on sentiment:\n",
    "\n",
    "    POSITIVE POSTS\n",
    "    {post_positive['data']}\n",
    "\n",
    "    NEGATIVE POSTS\n",
    "    {post_negative['data']}\n",
    "\n",
    "    OUTPUT (in JSON format):\n",
    "    {{\n",
    "      \"positive_topics\": \"<Provide a concise summary (2–3 sentences) that captures the key topics or themes discussed in the positive posts.>\",\n",
    "      \"negative_topics\": \"<Provide a concise summary (2–3 sentences) that captures the key topics or concerns raised in the negative posts.>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Memanggil Gemini API\n",
    "    prediction = call_gemini(prompt)\n",
    "    \n",
    "    try:\n",
    "        # Mencoba parse JSON dari respons\n",
    "        json_result = re.findall(r'\\{.*\\}', prediction, flags=re.I|re.S)[0]\n",
    "        return json.loads(json_result)\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        # Menangani error parsing\n",
    "        return {\n",
    "            \"positive_topics\": \"Error analyzing positive topics.\",\n",
    "            \"negative_topics\": \"Error analyzing negative topics.\",\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb268e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:16:05.886141Z",
     "start_time": "2025-04-17T10:16:01.293707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_topics_sentiment_analysis( es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    language = ['Indo']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3603a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c12875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:59:57.087681Z",
     "start_time": "2025-04-16T09:59:57.046298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.keyword_trends import get_keyword_trends\n",
    "get_keyword_trends(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b1fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:59:59.492179Z",
     "start_time": "2025-04-16T09:59:59.474909Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ed2a3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## channel shares and sentiment overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed698cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:04:42.292798Z",
     "start_time": "2025-04-17T05:04:42.242243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_category_analytics.py - Script untuk mendapatkan analisis kategori dan sentimen\n",
    "\n",
    "Script ini mengambil data tentang mentions berdasarkan kategori dan sentiment berdasarkan\n",
    "kategori dari Elasticsearch untuk visualisasi.\n",
    "\"\"\"\n",
    "from utils.analysis_sentiment_mentions import get_category_analytics\n",
    "\n",
    "channel_shares, _ ,sentiment_breakdown = get_category_analytics(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"prabowo danantara\", \"gibran\"],\n",
    "    start_date=\"2025-03-18\",\n",
    "    end_date=\"2025-04-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634db486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:04:42.822090Z",
     "start_time": "2025-04-17T05:04:42.816568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "channel_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58c385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:02:40.488684Z",
     "start_time": "2025-04-16T10:02:40.473506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentiment_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51923a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## emotion, intent and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070e77f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:56:51.701972Z",
     "start_time": "2025-04-17T08:56:51.387559Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.intent_emotions_region import get_intents_emotions_region_share\n",
    "analysis = get_intents_emotions_region_share(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"gibran\"],\n",
    "    date_filter=\"last 100 days\"\n",
    ")\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abf9a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:36:28.154966Z",
     "start_time": "2025-04-17T07:36:28.097335Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_intents_emotions_region_share(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5908ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa8ad5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d72c68",
   "metadata": {},
   "source": [
    "# TOPICCCCCCCCCCCCCCCCCCCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58bda1",
   "metadata": {},
   "source": [
    "## scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdbb53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:15:14.233323Z",
     "start_time": "2025-04-20T16:14:43.363761Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from utils.gemini import call_gemini\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "\n",
    "# Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "current_date = datetime.now()\n",
    "date_120_days_ago = current_date - timedelta(days=30)\n",
    "# Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "date_120_days_ago_str = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "db_host = \"34.101.146.213\"\n",
    "db_port = 3306\n",
    "db_user = \"arilindra21\"\n",
    "db_password = \"sukabumi030495\"\n",
    "db_name = \"auth_api_db\"\n",
    "       \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class About_MySQL:\n",
    "    def __init__(self, db_host, db_port, db_user, db_password, db_name):\n",
    "        # Membuat URL koneksi dengan format SQLAlchemy\n",
    "        self.database_url = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "        \n",
    "        # Membuat engine SQLAlchemy\n",
    "        self.engine = create_engine(self.database_url)\n",
    "\n",
    "    def to_pull_data(self, query):\n",
    "        # Mengambil data menggunakan query yang diberikan dan mengonversinya ke DataFrame\n",
    "        with self.engine.connect() as connection:\n",
    "            # Menjalankan query dan mengonversi hasilnya ke DataFrame\n",
    "            df = pd.read_sql(text(query), connection)\n",
    "        return df\n",
    "    \n",
    "    def to_push_data(self, dataframe: pd.DataFrame, table_name: str, if_exist: str = 'replace'):\n",
    "        \"\"\"\n",
    "        Menyimpan DataFrame ke tabel MySQL.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: pd.DataFrame yang akan disimpan\n",
    "        - table_name: nama tabel tujuan\n",
    "        - if_exist: 'replace' untuk mengganti tabel, 'append' untuk menambahkan data\n",
    "        \"\"\"\n",
    "        assert if_exist in ['replace', 'append'], \"Parameter 'if_exist' harus 'replace' atau 'append'\"\n",
    "        \n",
    "        dataframe.to_sql(\n",
    "            name=table_name,\n",
    "            con=self.engine,\n",
    "            if_exists=if_exist,\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        print(f\"✅ Data berhasil dipush ke tabel `{table_name}` dengan mode `{if_exist}`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a688268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:15:14.372681Z",
     "start_time": "2025-04-20T16:15:14.245421Z"
    },
    "code_folding": [
     1,
     50,
     64,
     79,
     121,
     162,
     196,
     248,
     447
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_to_elasticsearch(data: Union[Dict[str, Any], List[Dict[str, Any]]],\n",
    "                           hosts: Union[str, List[str]] = 'http://localhost:9200',\n",
    "                           index: str = 'my_index',\n",
    "                           bulk_size: int = 1000,\n",
    "                           id_field: Optional[str] = None,\n",
    "                           **es_kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ingest data into Elasticsearch using the official Elasticsearch Python client\n",
    "    \n",
    "    Args:\n",
    "        data: Single document (dict) or list of documents to ingest\n",
    "        hosts: Elasticsearch host URL or list of hosts\n",
    "        index: Name of the index\n",
    "        bulk_size: Number of documents to send in each bulk request\n",
    "        id_field: Field to use as document ID\n",
    "        es_kwargs: Additional keyword arguments for Elasticsearch client\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the bulk operation\n",
    "    \"\"\"\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch(hosts=hosts, **es_kwargs)\n",
    "    \n",
    "    # Convert single document to list if needed\n",
    "    documents = data if isinstance(data, list) else [data]\n",
    "    \n",
    "    # Prepare documents for bulk operation\n",
    "    actions = []\n",
    "    for doc in documents:\n",
    "        action = {\n",
    "            \"_index\": index,\n",
    "            \"_source\": doc\n",
    "        }\n",
    "        \n",
    "        # Add document ID if provided\n",
    "        if id_field and id_field in doc:\n",
    "            action[\"_id\"] = doc[id_field]\n",
    "            \n",
    "        actions.append(action)\n",
    "    \n",
    "    # Execute bulk operation\n",
    "    result = helpers.bulk(es, actions, chunk_size=bulk_size)\n",
    "    \n",
    "    return {\n",
    "        \"success_count\": result[0],\n",
    "        \"error_count\": result[1],\n",
    "        \"total_documents\": len(documents)\n",
    "    }\n",
    "\n",
    "def chunk_list(data: List, chunk_size: int) -> List[List]:\n",
    "    \"\"\"\n",
    "    Membagi list menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi chunk-chunk dari list\n",
    "    \"\"\"\n",
    "    # Membagi list data menjadi chunk sesuai dengan ukuran chunk_size\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Membagi DataFrame menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk (baris)\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi DataFrame chunked\n",
    "    \"\"\"\n",
    "    # Menghitung jumlah chunk yang dibutuhkan\n",
    "    chunked_data = [df.iloc[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "    return chunked_data\n",
    "\n",
    "class ElasticsearchHelper:\n",
    "    def __init__(self, host: str):\n",
    "        self.host = host\n",
    "        self.es = self.connect()\n",
    "\n",
    "    def connect(self) -> Elasticsearch:\n",
    "        \"\"\"Membuat koneksi ke Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            es = Elasticsearch(\n",
    "                self.host,\n",
    "                verify_certs=False  # Hanya untuk development, non-TLS\n",
    "            )\n",
    "            return es\n",
    "        except ElasticsearchException as e:\n",
    "            print(f\"Failed to connect to Elasticsearch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_data(self, index: str, query: dict, size: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tarik data dari Elasticsearch dengan scan helpers untuk data besar\"\"\"\n",
    "        try:\n",
    "            if not self.es:\n",
    "                raise ConnectionError(\"Tidak dapat terkoneksi dengan Elasticsearch.\")\n",
    "            \n",
    "            # Gunakan helpers.scan untuk menarik data besar\n",
    "            scan_response = helpers.scan(\n",
    "                client=self.es,\n",
    "                index=index,\n",
    "                query=query,\n",
    "                size=size,\n",
    "                scroll=\"2m\"  # Menggunakan scroll selama 2 menit untuk mengambil data\n",
    "            )\n",
    "\n",
    "            # Ambil hasil dari scan\n",
    "            all_hits = [doc[\"_source\"] for doc in scan_response]\n",
    "\n",
    "            return all_hits\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error umum: {e}\")\n",
    "            return []\n",
    "        \n",
    "def get_relevan_data(keywords):\n",
    "    es_helper = ElasticsearchHelper(host=\"http://localhost:9200\")\n",
    "    query = {\n",
    "        \"_source\": [\"issue\", \"post_caption\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\", \"channel\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                                \"query\": keyword,\n",
    "                                \"operator\": \"and\"  # Semua kata dalam keyword harus ada\n",
    "                            }\n",
    "                        }\n",
    "                    } for keyword in keywords\n",
    "                ],\n",
    "              \"must\": [\n",
    "                            {\n",
    "                                \"range\": {\n",
    "                                    \"post_created_at\": {\n",
    "                                        \"gte\": date_120_days_ago_str,  # Greater than or equal to 120 hari yang lalu\n",
    "                                        \"lte\": current_date.strftime(\"%Y-%m-%d %H:%M:%S\")  # Less than or equal to sekarang\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"issue\": \"Not Specified\"  # Filter: 'issue' tidak boleh \"Not Specified\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1  # Minimal satu keyword yang cocok\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es_helper.fetch_data(index=\"twitter_data,linkedin_data,news_data,reddit_data,youtube_data\",\n",
    "                                query=query, size=10000)\n",
    "    \n",
    "def get_df_issue(df):\n",
    "    issue_total_posts = df.groupby('issue').size().reset_index(name='total_posts')\n",
    "\n",
    "    # Hitung sum viral_score dan reach_score per issue\n",
    "    issue_scores = df.groupby('issue').agg({\n",
    "        'viral_score': 'sum',\n",
    "        'reach_score': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Hitung total per jenis sentiment per issue\n",
    "    sentiment_counts = pd.crosstab(df['issue'], df['sentiment']).reset_index()\n",
    "\n",
    "    # 2. Gabungkan semua metrik\n",
    "    result = issue_total_posts.merge(issue_scores, on='issue')\n",
    "    result = result.merge(sentiment_counts, on='issue')\n",
    "\n",
    "    # 3. Buat fungsi untuk memastikan semua jenis sentiment ada (termasuk jika nilai 0)\n",
    "    def ensure_sentiment_columns(df, sentiments=['positive', 'negative', 'neutral']):\n",
    "        for sentiment in sentiments:\n",
    "            if sentiment not in df.columns:\n",
    "                df[sentiment] = 0\n",
    "        return df\n",
    "\n",
    "    result = ensure_sentiment_columns(result)\n",
    "\n",
    "    # 4. Sorting berdasarkan viral_score (optional, bisa diubah sesuai kebutuhan)\n",
    "    result = result.sort_values(by='viral_score', ascending=False)\n",
    "\n",
    "    # 5. Reset index\n",
    "    df_issue = result.reset_index(drop=True).reset_index()\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    return df_issue\n",
    "\n",
    "def get_central_issue(data_chunk):\n",
    "    issue_list = []\n",
    "    for idx, row in data_chunk.iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "    # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "    prompt = f\"\"\"\n",
    "    Kamu adalah Media Social Analyst Expert dengan keahlian khusus dalam pengelompokan tematik.\n",
    "\n",
    "    # TUGAS\n",
    "    Analisis dan kelompokkan list issue sosial media di bawah ini menjadi kelompok-kelompok tematik.\n",
    "\n",
    "    # INSTRUKSI PENTING\n",
    "    1. Setiap issue ID hanya boleh masuk ke dalam SATU kelompok (mutually exclusive).\n",
    "    2. Hindari tumpang tindih issue di antara kelompok-kelompok.\n",
    "    3. Fokus pada tema/topik utama dari setiap issue.\n",
    "    4. Buat nama kelompok yang singkat, jelas, dan mencerminkan tema utama.\n",
    "    5. Berikan deskripsi kelompok yang informatif dan komprehensif.\n",
    "\n",
    "    # LIST ISSUE\n",
    "    ```\n",
    "    {issue_list}\n",
    "    ```\n",
    "\n",
    "    # FORMAT OUTPUT\n",
    "    Kembalikan hasil pengelompokan dalam format JSON yang tepat berikut ini:\n",
    "    ```\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 1\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [1, 5, 10, 15] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 2\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [2, 6, 11, 16] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    # PARAMETER KUALITAS\n",
    "    - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "    - Kelompokkan berdasarkan kemiripan tema/topik dan keywords, bukan sentimen\n",
    "\n",
    "    Berikan hasil pengelompokan dalam format JSON murni tanpa komentar atau penjelasan tambahan.\n",
    "    \"\"\"\n",
    "\n",
    "    centrality = call_gemini(prompt)\n",
    "    return pd.DataFrame(eval(re.findall(r'\\[.*\\]',centrality, flags=re.I|re.S)[0]))\n",
    "\n",
    "def get_topics_overview(keywords):\n",
    "    #mendapatkan raw data\n",
    "    df = pd.DataFrame(get_relevan_data(keywords))\n",
    "\n",
    "    #mendapatkan dataframe aggregate per issue\n",
    "    df_issue = get_df_issue(df)\n",
    "\n",
    "    #chunk df_issue\n",
    "    data_chunk = chunk_dataframe(df_issue, 100)\n",
    "\n",
    "    #mendapatkan central issue berdasarkan viral score tertinggi\n",
    "    df_central = get_central_issue(data_chunk[0])\n",
    "\n",
    "    LIST_UNIFIED_ISSUE = df_central['unified_issue'].to_list()\n",
    "\n",
    "    SISA = data_chunk[0][~data_chunk[0]['index'].isin([j for i in df_central['list_issue_id'] for j in i])]\n",
    "    if not SISA.empty:\n",
    "        data_chunk[1] = pd.concat([data_chunk[1], SISA])\n",
    "\n",
    "\n",
    "\n",
    "    #mendapatkan seluruh unified issue \n",
    "    all_result = []\n",
    "    for DC in tqdm(data_chunk[1:]):\n",
    "        issue_list = []\n",
    "        for idx, row in DC.iterrows():\n",
    "            issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "\n",
    "        df_predict = ''\n",
    "\n",
    "        while True:\n",
    "            if type(df_predict)!=str:\n",
    "                issue_list = [i for i in issue_list if i['id'] not in [j for i in df_predict['list_issue_id'] for j in i]]\n",
    "            # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "            prompt = f\"\"\"\n",
    "                Kamu adalah Media Social Analyst Expert dengan keahlian dalam pengelompokan tematik dan kategorisasi konten sosial media.\n",
    "\n",
    "                # TUGAS UTAMA\n",
    "                Analisis dan kelompokkan list issue baru ke dalam kelompok tematik yang sudah ada (jika relevan) atau buat kelompok baru jika diperlukan.\n",
    "\n",
    "                # ATURAN PENGELOMPOKAN\n",
    "                1. PENTING: Prioritaskan pengelompokan ke dalam kategori yang sudah ada jika terdapat kemiripan tema.\n",
    "                2. Buat kategori baru HANYA jika issue tidak cocok dengan kelompok yang sudah ada.\n",
    "                3. Setiap issue ID HANYA boleh masuk ke SATU kelompok (mutually exclusive).\n",
    "                4. Hindari TUMPANG TINDIH issue di antara kelompok-kelompok.\n",
    "\n",
    "                # KELOMPOK YANG SUDAH ADA\n",
    "                Berikut adalah kelompok tematik yang sudah ada dan HARUS digunakan jika relevan:\n",
    "                ```\n",
    "                {LIST_UNIFIED_ISSUE}\n",
    "                ```\n",
    "\n",
    "                # LIST ISSUE YANG AKAN DIKELOMPOKKAN\n",
    "                ```\n",
    "                {issue_list}\n",
    "                ```\n",
    "\n",
    "                # PETUNJUK PENGELOMPOKAN\n",
    "                - Jika issue memiliki kemiripan SUBSTANSIAL dengan kelompok yang sudah ada → masukkan ke kelompok tersebut\n",
    "                - Jika issue SAMA SEKALI TIDAK TERKAIT dengan kelompok yang ada → buat kelompok baru\n",
    "                - Kemiripan didasarkan pada tema, topik, kata kunci, dan konteks (bukan sentimen)\n",
    "                - Usahakan untuk TIDAK membuat kelompok baru jika masih bisa dimasukkan ke kelompok yang sudah ada\n",
    "\n",
    "                # FORMAT OUTPUT (JSON)\n",
    "                [\n",
    "                  {{\n",
    "                    \"unified_issue\": \"Nama Issue Kelompok (gunakan dari list yang sudah ada atau buat baru)\",\n",
    "                    \"description\": \"Deskripsi singkat dan jelas tentang tema kelompok ini\",\n",
    "                    \"list_issue_id\": [1, 5, 10, 15] // Daftar ID issue yang masuk dalam kelompok ini\n",
    "                  }},\n",
    "                  ...\n",
    "                ]\n",
    "\n",
    "                # KRITERIA KUALITAS\n",
    "                - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "                - Nama kelompok baru harus singkat, jelas, dan deskriptif\n",
    "                - Prioritaskan penggunaan nama kelompok yang sudah ada\n",
    "                - Pastikan seluruh index issue masuk ke setiap kelompok tanpa tersisia\n",
    "\n",
    "                PENTING: Hasilkan HANYA format JSON murni tanpa komentar, penjelasan, atau notasi lain.\"\"\"\n",
    "\n",
    "            predict = call_gemini(prompt)\n",
    "            df_predict = pd.DataFrame(eval(re.findall(r'\\[.*\\]',predict, flags=re.I|re.S)[0]))\n",
    "            all_result.append(df_predict)\n",
    "\n",
    "            LIST_UNIFIED_ISSUE.extend(df_predict['unified_issue'].to_list())\n",
    "\n",
    "            LIST_UNIFIED_ISSUE = list(set(LIST_UNIFIED_ISSUE))\n",
    "\n",
    "\n",
    "            SISA = set([i['id'] for i in issue_list])- set([j for i in df_predict['list_issue_id'] for j in i])\n",
    "\n",
    "            if not SISA:\n",
    "                break\n",
    "\n",
    "    all_result.append(df_central)\n",
    "    topics = pd.concat(all_result)\n",
    "\n",
    "    #merge dengan nama issue berdasarkan id\n",
    "    all_topics = topics.groupby('unified_issue').agg({'list_issue_id':[lambda s: [j for i in s for j in i], lambda s: len([j for i in s for j in i])]}).reset_index()\n",
    "    all_topics.columns = ['unified_issue','list_issue','total_issue']\n",
    "\n",
    "    merge_all_topics = []\n",
    "    for _,i in all_topics.iterrows():\n",
    "        data = df_issue[df_issue['index'].isin(i['list_issue'])]\n",
    "        dt = data[['total_posts','viral_score','reach_score','negative','positive','neutral']].sum().to_dict()\n",
    "        dt.update({'unified_issue':i['unified_issue'], 'list_issue':data['issue'].to_list()})\n",
    "\n",
    "        merge_all_topics.append(dt)\n",
    "\n",
    "    df_all_topics = pd.DataFrame(merge_all_topics).sort_values('viral_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "    df_all_topics['share_of_voice'] = df_all_topics['total_posts']/df_all_topics['total_posts'].sum()*100\n",
    "\n",
    "\n",
    "    #ambil importance post sebagai perwakilan\n",
    "    list_data = []\n",
    "    for _,i in df_all_topics.iterrows():\n",
    "        unified_issue = i['unified_issue']\n",
    "        list_issue = i['list_issue']\n",
    "        list_caption = df[df['issue'].isin(list_issue)].sort_values('viral_score',ascending = False).drop_duplicates('post_caption')[:5]['post_caption'].to_list()\n",
    "\n",
    "        list_data.append({'unified_issue':unified_issue,\n",
    "                                'list_caption':list_caption,\n",
    "                          'sentiment negative':i['negative'],\n",
    "                          'sentiment positive':i['positive']})\n",
    "\n",
    "    list_prediction = []\n",
    "    for unified_issues_list in tqdm(chunk_list(list_data, 40)):\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Kamu adalah Media Social Analyst Expert dengan keahlian dalam analisis kategori dan isu-isu sosial media.\n",
    "\n",
    "        # TUGAS\n",
    "        Buatkan deskripsi yang informatif, akurat, dan komprehensif untuk setiap kategori unified_issue berikut ini.\n",
    "\n",
    "        # PETUNJUK\n",
    "        1. Deskripsi harus menjelaskan dengan jelas apa yang termasuk dalam kategori tersebut\n",
    "        2. Deskripsi harus ringkas namun komprehensif (maksimal 2-3 kalimat)\n",
    "        3. Gunakan bahasa yang netral dan profesional\n",
    "        4. Fokus pada konten dan cakupan dari kategori isu tersebut\n",
    "        5. Jangan menyertakan opini pribadi atau bias dalam deskripsi\n",
    "        6. Berikan juga description berdasarkan sentimentnya\n",
    "\n",
    "        # DATA UNIFIED_ISSUE\n",
    "        ```\n",
    "        {unified_issues_list}\n",
    "        ```\n",
    "\n",
    "        # FORMAT OUTPUT\n",
    "        Berikan hasil dalam format JSON seperti berikut:\n",
    "        ```\n",
    "        [\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue Lainnya\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          ...\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        ### HARD RULE\n",
    "        - Berikan HANYA output JSON tanpa penjelasan atau komentar tambahan.\n",
    "        - Hindari penjelasan \"This category focuses\" , contoh description yang baik seperti ini \"Conversations praised recent innovations in the nickel industry that emphasize sustainability and local economic growth, boosting positive sentiment towards Prabowo by 18%\"\n",
    "        \"\"\"\n",
    "        response_text = call_gemini(prompt)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "        except:\n",
    "            response_text = (re.sub(f\"(?<!\\:\\s)\\\"([\\w\\s\\/\\.\\@\\-]+)\\\"(?![\\,\\:])\",r\"`\\1`\",response_text))\n",
    "            \n",
    "\n",
    "            try:\n",
    "                json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "            except:\n",
    "                json_string = []\n",
    "                for i in re.findall(r'{.*?}', response_text, flags=re.I|re.S):\n",
    "                    try:\n",
    "                        json_string.append(eval(i))\n",
    "                    except:\n",
    "                        pass  \n",
    "                    \n",
    "        \n",
    "        list_prediction.extend(json_string)\n",
    "\n",
    "\n",
    "    topics_result = pd.DataFrame(list_prediction).merge(df_all_topics, on = 'unified_issue')\n",
    "\n",
    "    return topics_result.sort_values('share_of_voice', ascending = False)\n",
    "\n",
    "def set_topics(keywords):\n",
    "\n",
    "    final_result = get_topics_overview(keywords)\n",
    "\n",
    "    result = ingest_to_elasticsearch(\n",
    "        data= [{'topics':final_result.to_dict(orient = 'records'),\n",
    "                'id':id_}],\n",
    "        hosts=\"http://localhost:9200\",\n",
    "        index=\"topics\",\n",
    "        id_field=\"id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7452750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:15:14.463803Z",
     "start_time": "2025-04-20T16:15:14.444600Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "def create_uuid(keyword):\n",
    "    # Gunakan namespace standar (ada juga untuk URL, DNS, dll)\n",
    "    namespace = uuid.NAMESPACE_DNS\n",
    "\n",
    "    return uuid.uuid5(namespace, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290ae186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:28:51.648196Z",
     "start_time": "2025-04-20T16:15:14.474830Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_gibran raka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [03:56<00:00,  8.45s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_partai gerindra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 55/55 [08:51<00:00,  9.67s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.04s/it]\n"
     ]
    }
   ],
   "source": [
    "mysql=About_MySQL( db_host, db_port, db_user, db_password, db_name)\n",
    "\n",
    "query = \"select * from keyword_projects\"\n",
    "data_projects = mysql.to_pull_data(query)\n",
    "\n",
    "for _ , i in data_projects.groupby(['project_name','owner_id']).agg({'relevan_keyword':list}).reset_index().iterrows():\n",
    "    id_ = create_uuid(\"{}_{}\".format(i['owner_id'], i['project_name']))\n",
    "    print(\"{}_{}\".format(i['owner_id'], i['project_name']))\n",
    "    keywords = i['relevan_keyword']\n",
    "    set_topics(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec1c82",
   "metadata": {},
   "source": [
    "## search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3816ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:03.056500Z",
     "start_time": "2025-04-18T06:24:03.034567Z"
    },
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.topics_overview import search_topics\n",
    "#INPUT\n",
    "owner_id = 5\n",
    "project_name = 'gibran rakas'\n",
    "keywords = [\n",
    "            \"pdip\",\n",
    "            \"gerindra\",\n",
    "            \"solo\",\n",
    "            \"wakil presiden terpilih\",\n",
    "            \"wali kota solo\",\n",
    "            \"gibran raka\",\n",
    "            \"pilpres 2024\",\n",
    "            \"kaesang pangarep\",\n",
    "            \"anak presiden jokowi\",\n",
    "            \"prabowo gibran\"\n",
    "          ]\n",
    "\n",
    "hasil = search_topics(  \n",
    "    owner_id = owner_id,\n",
    "    project_name = project_name,\n",
    "    es_host='localhost:9200',\n",
    "    keywords=keywords,\n",
    "    sentiment = ['neutral'],\n",
    "    channels = ['twitter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e748c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T02:46:05.470451Z",
     "start_time": "2025-04-18T02:46:05.433574Z"
    },
    "code_folding": [
     13,
     48,
     104
    ]
   },
   "outputs": [],
   "source": [
    "from utils.es_client import get_elasticsearch_client\n",
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import uuid\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "\n",
    "def create_uuid(keyword):\n",
    "    # Gunakan namespace standar (ada juga untuk URL, DNS, dll)\n",
    "    namespace = uuid.NAMESPACE_DNS\n",
    "\n",
    "    return uuid.uuid5(namespace, keyword)\n",
    "\n",
    "def matching_issue(final_result, df_data):\n",
    "    #final_result : dataframe yg isinya group topics \n",
    "    #df_data :dataframe yang ingin di cari grup nya\n",
    "     \n",
    "    hasil = []\n",
    "    for _,i  in final_result.iterrows():\n",
    "        list_issue = i['list_issue']\n",
    "        dt = df_data[df_data['issue'].isin(list_issue)]\n",
    "\n",
    "\n",
    "        if dt.empty:\n",
    "            continue\n",
    "\n",
    "        row = i.to_dict().copy()\n",
    "\n",
    "        agg_sentiment = dt.groupby('sentiment').size().to_dict()\n",
    "        sentiment_categories = ['positive', 'negative', 'neutral']\n",
    "        for category in sentiment_categories:\n",
    "            agg_sentiment.setdefault(category, 0)\n",
    "\n",
    "        agg_sentiment = {key: value for key, value in agg_sentiment.items() if key in sentiment_categories}\n",
    "\n",
    "\n",
    "        agg_score = dt[['viral_score','reach_score']].sum().to_dict()\n",
    "\n",
    "        row.update(agg_sentiment)\n",
    "        row.update(agg_score)\n",
    "        row.update({'total_posts':dt.shape[0]})\n",
    "        hasil.append(row)\n",
    "\n",
    "    df = pd.DataFrame(hasil).fillna(0)\n",
    "    df['share_of_voice'] = df['total_posts']/df['total_posts'].sum()*100\n",
    "\n",
    "    return df.to_dict(orient = 'records')\n",
    "\n",
    "def search_topics(  \n",
    "    owner_id = None,\n",
    "    project_name = None,\n",
    "    es_host='localhost:9200',\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None):\n",
    "    \n",
    "    print('(((((((((((((((((((((( MASUK ))))))))))))))))))))))')\n",
    "    es = get_elasticsearch_client(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #PROCESS\n",
    "    id_ = create_uuid(\"{}_{}\".format(owner_id, project_name))\n",
    "\n",
    "    query = {\n",
    "        \"source\":[],\n",
    "      \"query\": {\n",
    "        \"match\": {\n",
    "          \"_id\": id_\n",
    "        }\n",
    "      }\n",
    "\n",
    "    }\n",
    "\n",
    "    response = es.search(\n",
    "        index='topics',\n",
    "        body=query\n",
    "    )\n",
    "\n",
    "    data_project = [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "    if data_project:\n",
    "\n",
    "        #project sudah tergenerate\n",
    "        final_result = pd.DataFrame(data_project[0]['topics'])\n",
    "\n",
    "        #pake filter berdasarkan input user\n",
    "        result = get_mentions(\n",
    "            source= [\"issue\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\"],\n",
    "            page_size=10000,\n",
    "            es_host=es_host,    \n",
    "            es_username=es_username,\n",
    "            es_password=es_password,\n",
    "            use_ssl=use_ssl,\n",
    "            verify_certs=verify_certs,\n",
    "            ca_certs=ca_certs,\n",
    "            keywords=keywords,\n",
    "            search_exact_phrases=search_exact_phrases,\n",
    "            case_sensitive=case_sensitive,\n",
    "            sentiment=sentiment,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            date_filter=date_filter,\n",
    "            custom_start_date=custom_start_date,\n",
    "            custom_end_date=custom_end_date,\n",
    "            channels=channels,\n",
    "            importance=importance,\n",
    "            influence_score_min=influence_score_min,\n",
    "            influence_score_max=influence_score_max,\n",
    "            region=region,\n",
    "            language=language,\n",
    "            domain=domain,\n",
    "            sort_type = 'popular'\n",
    "    \n",
    "        )\n",
    "\n",
    "        df_data = pd.DataFrame(result['data'])\n",
    "        \n",
    "        if df_data.empty:\n",
    "            return []\n",
    "        \n",
    "        return matching_issue(final_result, df_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b84d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T02:46:06.298402Z",
     "start_time": "2025-04-18T02:46:06.133593Z"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"es_host\":\"localhost:9200\",\n",
    "  \"keywords\": [\n",
    "    \"gibran\"\n",
    "  ],\n",
    "  \"owner_id\": \"5\",\n",
    "  \"project_name\": \"gibran raka\"\n",
    "}\n",
    "\n",
    "hasil = search_topics(  \n",
    "    owner_id = '5',\n",
    "    project_name = \"gibran raka\",\n",
    "    es_host='localhost:9200',\n",
    "    keywords=[\"gibran\"],\n",
    "    channels = ['reddit'])\n",
    "pd.DataFrame(hasil).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ceef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:21.874560Z",
     "start_time": "2025-04-18T06:24:19.039861Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.topics_overview import search_topics\n",
    "\n",
    "hasil = search_topics(  \n",
    "    owner_id = '5',\n",
    "    project_name = \"gibran raka\",\n",
    "    es_host='localhost:9200',\n",
    "    keywords=[\"prabowo\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec773c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:24.532917Z",
     "start_time": "2025-04-18T06:24:24.481488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(hasil).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47ba52",
   "metadata": {},
   "source": [
    "## if topics belum ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64be9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:12:21.568180Z",
     "start_time": "2025-04-18T06:11:57.022404Z"
    },
    "code_folding": [
     26
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from utils.gemini import call_gemini\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "\n",
    "\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "db_host = \"34.101.146.213\"\n",
    "db_port = 3306\n",
    "db_user = \"arilindra21\"\n",
    "db_password = \"sukabumi030495\"\n",
    "db_name = \"auth_api_db\"\n",
    "       \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class About_MySQL:\n",
    "    def __init__(self, db_host, db_port, db_user, db_password, db_name):\n",
    "        # Membuat URL koneksi dengan format SQLAlchemy\n",
    "        self.database_url = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "        \n",
    "        # Membuat engine SQLAlchemy\n",
    "        self.engine = create_engine(self.database_url)\n",
    "\n",
    "    def to_pull_data(self, query):\n",
    "        # Mengambil data menggunakan query yang diberikan dan mengonversinya ke DataFrame\n",
    "        with self.engine.connect() as connection:\n",
    "            # Menjalankan query dan mengonversi hasilnya ke DataFrame\n",
    "            df = pd.read_sql(text(query), connection)\n",
    "        return df\n",
    "    \n",
    "    def to_push_data(self, dataframe: pd.DataFrame, table_name: str, if_exist: str = 'replace'):\n",
    "        \"\"\"\n",
    "        Menyimpan DataFrame ke tabel MySQL.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: pd.DataFrame yang akan disimpan\n",
    "        - table_name: nama tabel tujuan\n",
    "        - if_exist: 'replace' untuk mengganti tabel, 'append' untuk menambahkan data\n",
    "        \"\"\"\n",
    "        assert if_exist in ['replace', 'append'], \"Parameter 'if_exist' harus 'replace' atau 'append'\"\n",
    "        \n",
    "        dataframe.to_sql(\n",
    "            name=table_name,\n",
    "            con=self.engine,\n",
    "            if_exists=if_exist,\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        print(f\"✅ Data berhasil dipush ke tabel `{table_name}` dengan mode `{if_exist}`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6bd437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:18:04.950071Z",
     "start_time": "2025-04-18T06:18:04.591225Z"
    },
    "code_folding": [
     1,
     50,
     64,
     79,
     121,
     172,
     206,
     258,
     457,
     461
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_to_elasticsearch(data: Union[Dict[str, Any], List[Dict[str, Any]]],\n",
    "                           hosts: Union[str, List[str]] = 'http://localhost:9200',\n",
    "                           index: str = 'my_index',\n",
    "                           bulk_size: int = 1000,\n",
    "                           id_field: Optional[str] = None,\n",
    "                           **es_kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ingest data into Elasticsearch using the official Elasticsearch Python client\n",
    "    \n",
    "    Args:\n",
    "        data: Single document (dict) or list of documents to ingest\n",
    "        hosts: Elasticsearch host URL or list of hosts\n",
    "        index: Name of the index\n",
    "        bulk_size: Number of documents to send in each bulk request\n",
    "        id_field: Field to use as document ID\n",
    "        es_kwargs: Additional keyword arguments for Elasticsearch client\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the bulk operation\n",
    "    \"\"\"\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch(hosts=hosts, **es_kwargs)\n",
    "    \n",
    "    # Convert single document to list if needed\n",
    "    documents = data if isinstance(data, list) else [data]\n",
    "    \n",
    "    # Prepare documents for bulk operation\n",
    "    actions = []\n",
    "    for doc in documents:\n",
    "        action = {\n",
    "            \"_index\": index,\n",
    "            \"_source\": doc\n",
    "        }\n",
    "        \n",
    "        # Add document ID if provided\n",
    "        if id_field and id_field in doc:\n",
    "            action[\"_id\"] = doc[id_field]\n",
    "            \n",
    "        actions.append(action)\n",
    "    \n",
    "    # Execute bulk operation\n",
    "    result = helpers.bulk(es, actions, chunk_size=bulk_size)\n",
    "    \n",
    "    return {\n",
    "        \"success_count\": result[0],\n",
    "        \"error_count\": result[1],\n",
    "        \"total_documents\": len(documents)\n",
    "    }\n",
    "\n",
    "def chunk_list(data: List, chunk_size: int) -> List[List]:\n",
    "    \"\"\"\n",
    "    Membagi list menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi chunk-chunk dari list\n",
    "    \"\"\"\n",
    "    # Membagi list data menjadi chunk sesuai dengan ukuran chunk_size\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Membagi DataFrame menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk (baris)\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi DataFrame chunked\n",
    "    \"\"\"\n",
    "    # Menghitung jumlah chunk yang dibutuhkan\n",
    "    chunked_data = [df.iloc[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "    return chunked_data\n",
    "\n",
    "class ElasticsearchHelper:\n",
    "    def __init__(self, host: str):\n",
    "        self.host = host\n",
    "        self.es = self.connect()\n",
    "\n",
    "    def connect(self) -> Elasticsearch:\n",
    "        \"\"\"Membuat koneksi ke Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            es = Elasticsearch(\n",
    "                self.host,\n",
    "                verify_certs=False  # Hanya untuk development, non-TLS\n",
    "            )\n",
    "            return es\n",
    "        except ElasticsearchException as e:\n",
    "            print(f\"Failed to connect to Elasticsearch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_data(self, index: str, query: dict, size: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tarik data dari Elasticsearch dengan scan helpers untuk data besar\"\"\"\n",
    "        try:\n",
    "            if not self.es:\n",
    "                raise ConnectionError(\"Tidak dapat terkoneksi dengan Elasticsearch.\")\n",
    "            \n",
    "            # Gunakan helpers.scan untuk menarik data besar\n",
    "            scan_response = helpers.scan(\n",
    "                client=self.es,\n",
    "                index=index,\n",
    "                query=query,\n",
    "                size=size,\n",
    "                scroll=\"2m\"  # Menggunakan scroll selama 2 menit untuk mengambil data\n",
    "            )\n",
    "\n",
    "            # Ambil hasil dari scan\n",
    "            all_hits = [doc[\"_source\"] for doc in scan_response]\n",
    "\n",
    "            return all_hits\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error umum: {e}\")\n",
    "            return []\n",
    "        \n",
    "def get_relevan_data(keywords,start_date=None, sampling = False ):\n",
    "    \n",
    "    \n",
    "    current_date = datetime.now()\n",
    "    if not start_date:\n",
    "        # Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "        \n",
    "        date_120_days_ago = current_date - timedelta(days=120)\n",
    "        # Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "        start_date = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    es_helper = ElasticsearchHelper(host=\"http://localhost:9200\")\n",
    "    query = {\n",
    "        \"_source\": [\"issue\", \"post_caption\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\", \"channel\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                                \"query\": keyword,\n",
    "                                \"operator\": \"and\"  # Semua kata dalam keyword harus ada\n",
    "                            }\n",
    "                        }\n",
    "                    } for keyword in keywords\n",
    "                ],\n",
    "              \"must\": [\n",
    "                            {\n",
    "                                \"range\": {\n",
    "                                    \"post_created_at\": {\n",
    "                                        \"gte\": start_date,  # Greater than or equal to 120 hari yang lalu\n",
    "                                        \"lte\": current_date.strftime(\"%Y-%m-%d %H:%M:%S\")  # Less than or equal to sekarang\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"issue\": \"Not Specified\"  # Filter: 'issue' tidak boleh \"Not Specified\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1  # Minimal satu keyword yang cocok\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es_helper.fetch_data(index=\"twitter_data,linkedin_data,news_data,reddit_data,youtube_data\",\n",
    "                                query=query, size=10000)\n",
    "    \n",
    "def get_df_issue(df):\n",
    "    issue_total_posts = df.groupby('issue').size().reset_index(name='total_posts')\n",
    "\n",
    "    # Hitung sum viral_score dan reach_score per issue\n",
    "    issue_scores = df.groupby('issue').agg({\n",
    "        'viral_score': 'sum',\n",
    "        'reach_score': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Hitung total per jenis sentiment per issue\n",
    "    sentiment_counts = pd.crosstab(df['issue'], df['sentiment']).reset_index()\n",
    "\n",
    "    # 2. Gabungkan semua metrik\n",
    "    result = issue_total_posts.merge(issue_scores, on='issue')\n",
    "    result = result.merge(sentiment_counts, on='issue')\n",
    "\n",
    "    # 3. Buat fungsi untuk memastikan semua jenis sentiment ada (termasuk jika nilai 0)\n",
    "    def ensure_sentiment_columns(df, sentiments=['positive', 'negative', 'neutral']):\n",
    "        for sentiment in sentiments:\n",
    "            if sentiment not in df.columns:\n",
    "                df[sentiment] = 0\n",
    "        return df\n",
    "\n",
    "    result = ensure_sentiment_columns(result)\n",
    "\n",
    "    # 4. Sorting berdasarkan viral_score (optional, bisa diubah sesuai kebutuhan)\n",
    "    result = result.sort_values(by='viral_score', ascending=False)\n",
    "\n",
    "    # 5. Reset index\n",
    "    df_issue = result.reset_index(drop=True).reset_index()\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    return df_issue\n",
    "\n",
    "def get_central_issue(data_chunk):\n",
    "    issue_list = []\n",
    "    for idx, row in data_chunk.iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "    # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "    prompt = f\"\"\"\n",
    "    Kamu adalah Media Social Analyst Expert dengan keahlian khusus dalam pengelompokan tematik.\n",
    "\n",
    "    # TUGAS\n",
    "    Analisis dan kelompokkan list issue sosial media di bawah ini menjadi kelompok-kelompok tematik.\n",
    "\n",
    "    # INSTRUKSI PENTING\n",
    "    1. Setiap issue ID hanya boleh masuk ke dalam SATU kelompok (mutually exclusive).\n",
    "    2. Hindari tumpang tindih issue di antara kelompok-kelompok.\n",
    "    3. Fokus pada tema/topik utama dari setiap issue.\n",
    "    4. Buat nama kelompok yang singkat, jelas, dan mencerminkan tema utama.\n",
    "    5. Berikan deskripsi kelompok yang informatif dan komprehensif.\n",
    "\n",
    "    # LIST ISSUE\n",
    "    ```\n",
    "    {issue_list}\n",
    "    ```\n",
    "\n",
    "    # FORMAT OUTPUT\n",
    "    Kembalikan hasil pengelompokan dalam format JSON yang tepat berikut ini:\n",
    "    ```\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 1\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [1, 5, 10, 15] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 2\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [2, 6, 11, 16] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    # PARAMETER KUALITAS\n",
    "    - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "    - Kelompokkan berdasarkan kemiripan tema/topik dan keywords, bukan sentimen\n",
    "\n",
    "    Berikan hasil pengelompokan dalam format JSON murni tanpa komentar atau penjelasan tambahan.\n",
    "    \"\"\"\n",
    "\n",
    "    centrality = call_gemini(prompt)\n",
    "    return pd.DataFrame(eval(re.findall(r'\\[.*\\]',centrality, flags=re.I|re.S)[0]))\n",
    "\n",
    "def get_topics_overview(keywords):\n",
    "    #mendapatkan raw data\n",
    "    df = pd.DataFrame(get_relevan_data(keywords))\n",
    "\n",
    "    #mendapatkan dataframe aggregate per issue\n",
    "    df_issue = get_df_issue(df)\n",
    "\n",
    "    #chunk df_issue\n",
    "    data_chunk = chunk_dataframe(df_issue, 100)\n",
    "\n",
    "    #mendapatkan central issue berdasarkan viral score tertinggi\n",
    "    df_central = get_central_issue(data_chunk[0])\n",
    "\n",
    "    LIST_UNIFIED_ISSUE = df_central['unified_issue'].to_list()\n",
    "\n",
    "    SISA = data_chunk[0][~data_chunk[0]['index'].isin([j for i in df_central['list_issue_id'] for j in i])]\n",
    "    if not SISA.empty:\n",
    "        data_chunk[1] = pd.concat([data_chunk[1], SISA])\n",
    "\n",
    "\n",
    "\n",
    "    #mendapatkan seluruh unified issue \n",
    "    all_result = []\n",
    "    for DC in tqdm(data_chunk[1:]):\n",
    "        issue_list = []\n",
    "        for idx, row in DC.iterrows():\n",
    "            issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "\n",
    "        df_predict = ''\n",
    "\n",
    "        while True:\n",
    "            if type(df_predict)!=str:\n",
    "                issue_list = [i for i in issue_list if i['id'] not in [j for i in df_predict['list_issue_id'] for j in i]]\n",
    "            # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "            prompt = f\"\"\"\n",
    "                Kamu adalah Media Social Analyst Expert dengan keahlian dalam pengelompokan tematik dan kategorisasi konten sosial media.\n",
    "\n",
    "                # TUGAS UTAMA\n",
    "                Analisis dan kelompokkan list issue baru ke dalam kelompok tematik yang sudah ada (jika relevan) atau buat kelompok baru jika diperlukan.\n",
    "\n",
    "                # ATURAN PENGELOMPOKAN\n",
    "                1. PENTING: Prioritaskan pengelompokan ke dalam kategori yang sudah ada jika terdapat kemiripan tema.\n",
    "                2. Buat kategori baru HANYA jika issue tidak cocok dengan kelompok yang sudah ada.\n",
    "                3. Setiap issue ID HANYA boleh masuk ke SATU kelompok (mutually exclusive).\n",
    "                4. Hindari TUMPANG TINDIH issue di antara kelompok-kelompok.\n",
    "\n",
    "                # KELOMPOK YANG SUDAH ADA\n",
    "                Berikut adalah kelompok tematik yang sudah ada dan HARUS digunakan jika relevan:\n",
    "                ```\n",
    "                {LIST_UNIFIED_ISSUE}\n",
    "                ```\n",
    "\n",
    "                # LIST ISSUE YANG AKAN DIKELOMPOKKAN\n",
    "                ```\n",
    "                {issue_list}\n",
    "                ```\n",
    "\n",
    "                # PETUNJUK PENGELOMPOKAN\n",
    "                - Jika issue memiliki kemiripan SUBSTANSIAL dengan kelompok yang sudah ada → masukkan ke kelompok tersebut\n",
    "                - Jika issue SAMA SEKALI TIDAK TERKAIT dengan kelompok yang ada → buat kelompok baru\n",
    "                - Kemiripan didasarkan pada tema, topik, kata kunci, dan konteks (bukan sentimen)\n",
    "                - Usahakan untuk TIDAK membuat kelompok baru jika masih bisa dimasukkan ke kelompok yang sudah ada\n",
    "\n",
    "                # FORMAT OUTPUT (JSON)\n",
    "                [\n",
    "                  {{\n",
    "                    \"unified_issue\": \"Nama Issue Kelompok (gunakan dari list yang sudah ada atau buat baru)\",\n",
    "                    \"description\": \"Deskripsi singkat dan jelas tentang tema kelompok ini\",\n",
    "                    \"list_issue_id\": [1, 5, 10, 15] // Daftar ID issue yang masuk dalam kelompok ini\n",
    "                  }},\n",
    "                  ...\n",
    "                ]\n",
    "\n",
    "                # KRITERIA KUALITAS\n",
    "                - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "                - Nama kelompok baru harus singkat, jelas, dan deskriptif\n",
    "                - Prioritaskan penggunaan nama kelompok yang sudah ada\n",
    "                - Pastikan seluruh index issue masuk ke setiap kelompok tanpa tersisia\n",
    "\n",
    "                PENTING: Hasilkan HANYA format JSON murni tanpa komentar, penjelasan, atau notasi lain.\"\"\"\n",
    "\n",
    "            predict = call_gemini(prompt)\n",
    "            df_predict = pd.DataFrame(eval(re.findall(r'\\[.*\\]',predict, flags=re.I|re.S)[0]))\n",
    "            all_result.append(df_predict)\n",
    "\n",
    "            LIST_UNIFIED_ISSUE.extend(df_predict['unified_issue'].to_list())\n",
    "\n",
    "            LIST_UNIFIED_ISSUE = list(set(LIST_UNIFIED_ISSUE))\n",
    "\n",
    "\n",
    "            SISA = set([i['id'] for i in issue_list])- set([j for i in df_predict['list_issue_id'] for j in i])\n",
    "\n",
    "            if not SISA:\n",
    "                break\n",
    "\n",
    "    all_result.append(df_central)\n",
    "    topics = pd.concat(all_result)\n",
    "\n",
    "    #merge dengan nama issue berdasarkan id\n",
    "    all_topics = topics.groupby('unified_issue').agg({'list_issue_id':[lambda s: [j for i in s for j in i], lambda s: len([j for i in s for j in i])]}).reset_index()\n",
    "    all_topics.columns = ['unified_issue','list_issue','total_issue']\n",
    "\n",
    "    merge_all_topics = []\n",
    "    for _,i in all_topics.iterrows():\n",
    "        data = df_issue[df_issue['index'].isin(i['list_issue'])]\n",
    "        dt = data[['total_posts','viral_score','reach_score','negative','positive','neutral']].sum().to_dict()\n",
    "        dt.update({'unified_issue':i['unified_issue'], 'list_issue':data['issue'].to_list()})\n",
    "\n",
    "        merge_all_topics.append(dt)\n",
    "\n",
    "    df_all_topics = pd.DataFrame(merge_all_topics).sort_values('viral_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "    df_all_topics['share_of_voice'] = df_all_topics['total_posts']/df_all_topics['total_posts'].sum()*100\n",
    "\n",
    "\n",
    "    #ambil importance post sebagai perwakilan\n",
    "    list_data = []\n",
    "    for _,i in df_all_topics.iterrows():\n",
    "        unified_issue = i['unified_issue']\n",
    "        list_issue = i['list_issue']\n",
    "        list_caption = df[df['issue'].isin(list_issue)].sort_values('viral_score',ascending = False).drop_duplicates('post_caption')[:5]['post_caption'].to_list()\n",
    "\n",
    "        list_data.append({'unified_issue':unified_issue,\n",
    "                                'list_caption':list_caption,\n",
    "                          'sentiment negative':i['negative'],\n",
    "                          'sentiment positive':i['positive']})\n",
    "\n",
    "    list_prediction = []\n",
    "    for unified_issues_list in tqdm(chunk_list(list_data, 40)):\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Kamu adalah Media Social Analyst Expert dengan keahlian dalam analisis kategori dan isu-isu sosial media.\n",
    "\n",
    "        # TUGAS\n",
    "        Buatkan deskripsi yang informatif, akurat, dan komprehensif untuk setiap kategori unified_issue berikut ini.\n",
    "\n",
    "        # PETUNJUK\n",
    "        1. Deskripsi harus menjelaskan dengan jelas apa yang termasuk dalam kategori tersebut\n",
    "        2. Deskripsi harus ringkas namun komprehensif (maksimal 2-3 kalimat)\n",
    "        3. Gunakan bahasa yang netral dan profesional\n",
    "        4. Fokus pada konten dan cakupan dari kategori isu tersebut\n",
    "        5. Jangan menyertakan opini pribadi atau bias dalam deskripsi\n",
    "        6. Berikan juga description berdasarkan sentimentnya\n",
    "\n",
    "        # DATA UNIFIED_ISSUE\n",
    "        ```\n",
    "        {unified_issues_list}\n",
    "        ```\n",
    "\n",
    "        # FORMAT OUTPUT\n",
    "        Berikan hasil dalam format JSON seperti berikut:\n",
    "        ```\n",
    "        [\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue Lainnya\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          ...\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        ### HARD RULE\n",
    "        - Berikan HANYA output JSON tanpa penjelasan atau komentar tambahan.\n",
    "        - Hindari penjelasan \"This category focuses\" , contoh description yang baik seperti ini \"Conversations praised recent innovations in the nickel industry that emphasize sustainability and local economic growth, boosting positive sentiment towards Prabowo by 18%\"\n",
    "        \"\"\"\n",
    "        response_text = call_gemini(prompt)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "        except:\n",
    "            response_text = (re.sub(f\"(?<!\\:\\s)\\\"([\\w\\s\\/\\.\\@\\-]+)\\\"(?![\\,\\:])\",r\"`\\1`\",response_text))\n",
    "            \n",
    "\n",
    "            try:\n",
    "                json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "            except:\n",
    "                json_string = []\n",
    "                for i in re.findall(r'{.*?}', response_text, flags=re.I|re.S):\n",
    "                    try:\n",
    "                        json_string.append(eval(i))\n",
    "                    except:\n",
    "                        pass  \n",
    "                    \n",
    "        \n",
    "        list_prediction.extend(json_string)\n",
    "\n",
    "\n",
    "    topics_result = pd.DataFrame(list_prediction).merge(df_all_topics, on = 'unified_issue')\n",
    "\n",
    "    return topics_result.sort_values('share_of_voice', ascending = False)\n",
    "\n",
    "def set_topics(keywords):\n",
    "\n",
    "    final_result = get_topics_overview(keywords)\n",
    "\n",
    "    result = ingest_to_elasticsearch(\n",
    "        data= [{'topics':final_result.to_dict(orient = 'records'),\n",
    "                'id':id_}],\n",
    "        hosts=\"http://localhost:9200\",\n",
    "        index=\"topics\",\n",
    "        id_field=\"id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0fc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:18.630534Z",
     "start_time": "2025-04-18T06:18:24.309333Z"
    },
    "code_folding": [
     30,
     138
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = ['prabowo']\n",
    "# Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "\n",
    "current_date = datetime.now()\n",
    "date_120_days_ago = current_date - timedelta(days=10)\n",
    "# Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "start_date = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#mendapatkan raw data\n",
    "df = pd.DataFrame(get_relevan_data(keywords,start_date))\n",
    "\n",
    "#mendapatkan dataframe aggregate per issue\n",
    "df_issue = get_df_issue(df)\n",
    "\n",
    "#chunk df_issue\n",
    "data_chunk = chunk_dataframe(df_issue, 100)\n",
    "\n",
    "#mendapatkan central issue berdasarkan viral score tertinggi\n",
    "df_central = get_central_issue(data_chunk[0])\n",
    "\n",
    "LIST_UNIFIED_ISSUE = df_central['unified_issue'].to_list()\n",
    "\n",
    "SISA = data_chunk[0][~data_chunk[0]['index'].isin([j for i in df_central['list_issue_id'] for j in i])]\n",
    "if not SISA.empty:\n",
    "    data_chunk[1] = pd.concat([data_chunk[1], SISA])\n",
    "\n",
    "\n",
    "\n",
    "#mendapatkan seluruh unified issue \n",
    "all_result = []\n",
    "for DC in tqdm(data_chunk[1:]):\n",
    "    issue_list = []\n",
    "    for idx, row in DC.iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "\n",
    "    df_predict = ''\n",
    "\n",
    "    while True:\n",
    "        if type(df_predict)!=str:\n",
    "            issue_list = [i for i in issue_list if i['id'] not in [j for i in df_predict['list_issue_id'] for j in i]]\n",
    "        # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "        prompt = f\"\"\"\n",
    "            Kamu adalah Media Social Analyst Expert dengan keahlian dalam pengelompokan tematik dan kategorisasi konten sosial media.\n",
    "\n",
    "            # TUGAS UTAMA\n",
    "            Analisis dan kelompokkan list issue baru ke dalam kelompok tematik yang sudah ada (jika relevan) atau buat kelompok baru jika diperlukan.\n",
    "\n",
    "            # ATURAN PENGELOMPOKAN\n",
    "            1. PENTING: Prioritaskan pengelompokan ke dalam kategori yang sudah ada jika terdapat kemiripan tema.\n",
    "            2. Buat kategori baru HANYA jika issue tidak cocok dengan kelompok yang sudah ada.\n",
    "            3. Setiap issue ID HANYA boleh masuk ke SATU kelompok (mutually exclusive).\n",
    "            4. Hindari TUMPANG TINDIH issue di antara kelompok-kelompok.\n",
    "\n",
    "            # KELOMPOK YANG SUDAH ADA\n",
    "            Berikut adalah kelompok tematik yang sudah ada dan HARUS digunakan jika relevan:\n",
    "            ```\n",
    "            {LIST_UNIFIED_ISSUE}\n",
    "            ```\n",
    "\n",
    "            # LIST ISSUE YANG AKAN DIKELOMPOKKAN\n",
    "            ```\n",
    "            {issue_list}\n",
    "            ```\n",
    "\n",
    "            # PETUNJUK PENGELOMPOKAN\n",
    "            - Jika issue memiliki kemiripan SUBSTANSIAL dengan kelompok yang sudah ada → masukkan ke kelompok tersebut\n",
    "            - Jika issue SAMA SEKALI TIDAK TERKAIT dengan kelompok yang ada → buat kelompok baru\n",
    "            - Kemiripan didasarkan pada tema, topik, kata kunci, dan konteks (bukan sentimen)\n",
    "            - Usahakan untuk TIDAK membuat kelompok baru jika masih bisa dimasukkan ke kelompok yang sudah ada\n",
    "\n",
    "            # FORMAT OUTPUT (JSON)\n",
    "            [\n",
    "              {{\n",
    "                \"unified_issue\": \"Nama Issue Kelompok (gunakan dari list yang sudah ada atau buat baru)\",\n",
    "                \"description\": \"Deskripsi singkat dan jelas tentang tema kelompok ini\",\n",
    "                \"list_issue_id\": [1, 5, 10, 15] // Daftar ID issue yang masuk dalam kelompok ini\n",
    "              }},\n",
    "              ...\n",
    "            ]\n",
    "\n",
    "            # KRITERIA KUALITAS\n",
    "            - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "            - Nama kelompok baru harus singkat, jelas, dan deskriptif\n",
    "            - Prioritaskan penggunaan nama kelompok yang sudah ada\n",
    "            - Pastikan seluruh index issue masuk ke setiap kelompok tanpa tersisia\n",
    "\n",
    "            PENTING: Hasilkan HANYA format JSON murni tanpa komentar, penjelasan, atau notasi lain.\"\"\"\n",
    "\n",
    "        predict = call_gemini(prompt)\n",
    "        df_predict = pd.DataFrame(eval(re.findall(r'\\[.*\\]',predict, flags=re.I|re.S)[0]))\n",
    "        all_result.append(df_predict)\n",
    "\n",
    "        LIST_UNIFIED_ISSUE.extend(df_predict['unified_issue'].to_list())\n",
    "\n",
    "        LIST_UNIFIED_ISSUE = list(set(LIST_UNIFIED_ISSUE))\n",
    "\n",
    "\n",
    "        SISA = set([i['id'] for i in issue_list])- set([j for i in df_predict['list_issue_id'] for j in i])\n",
    "\n",
    "        if not SISA:\n",
    "            break\n",
    "\n",
    "all_result.append(df_central)\n",
    "topics = pd.concat(all_result)\n",
    "\n",
    "#merge dengan nama issue berdasarkan id\n",
    "all_topics = topics.groupby('unified_issue').agg({'list_issue_id':[lambda s: [j for i in s for j in i], lambda s: len([j for i in s for j in i])]}).reset_index()\n",
    "all_topics.columns = ['unified_issue','list_issue','total_issue']\n",
    "\n",
    "merge_all_topics = []\n",
    "for _,i in all_topics.iterrows():\n",
    "    data = df_issue[df_issue['index'].isin(i['list_issue'])]\n",
    "    dt = data[['total_posts','viral_score','reach_score','negative','positive','neutral']].sum().to_dict()\n",
    "    dt.update({'unified_issue':i['unified_issue'], 'list_issue':data['issue'].to_list()})\n",
    "\n",
    "    merge_all_topics.append(dt)\n",
    "\n",
    "df_all_topics = pd.DataFrame(merge_all_topics).sort_values('viral_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "df_all_topics['share_of_voice'] = df_all_topics['total_posts']/df_all_topics['total_posts'].sum()*100\n",
    "\n",
    "\n",
    "#ambil importance post sebagai perwakilan\n",
    "list_data = []\n",
    "for _,i in df_all_topics.iterrows():\n",
    "    unified_issue = i['unified_issue']\n",
    "    list_issue = i['list_issue']\n",
    "    list_caption = df[df['issue'].isin(list_issue)].sort_values('viral_score',ascending = False).drop_duplicates('post_caption')[:5]['post_caption'].to_list()\n",
    "\n",
    "    list_data.append({'unified_issue':unified_issue,\n",
    "                            'list_caption':list_caption,\n",
    "                      'sentiment negative':i['negative'],\n",
    "                      'sentiment positive':i['positive']})\n",
    "\n",
    "list_prediction = []\n",
    "for unified_issues_list in tqdm(chunk_list(list_data, 40)):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Kamu adalah Media Social Analyst Expert dengan keahlian dalam analisis kategori dan isu-isu sosial media.\n",
    "\n",
    "    # TUGAS\n",
    "    Buatkan deskripsi yang informatif, akurat, dan komprehensif untuk setiap kategori unified_issue berikut ini.\n",
    "\n",
    "    # PETUNJUK\n",
    "    1. Deskripsi harus menjelaskan dengan jelas apa yang termasuk dalam kategori tersebut\n",
    "    2. Deskripsi harus ringkas namun komprehensif (maksimal 2-3 kalimat)\n",
    "    3. Gunakan bahasa yang netral dan profesional\n",
    "    4. Fokus pada konten dan cakupan dari kategori isu tersebut\n",
    "    5. Jangan menyertakan opini pribadi atau bias dalam deskripsi\n",
    "    6. Berikan juga description berdasarkan sentimentnya\n",
    "\n",
    "    # DATA UNIFIED_ISSUE\n",
    "    ```\n",
    "    {unified_issues_list}\n",
    "    ```\n",
    "\n",
    "    # FORMAT OUTPUT\n",
    "    Berikan hasil dalam format JSON seperti berikut:\n",
    "    ```\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Unified Issue\",\n",
    "        \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Unified Issue Lainnya\",\n",
    "        \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    ### HARD RULE\n",
    "    - Berikan HANYA output JSON tanpa penjelasan atau komentar tambahan.\n",
    "    - Hindari penjelasan \"This category focuses\" , contoh description yang baik seperti ini \"Conversations praised recent innovations in the nickel industry that emphasize sustainability and local economic growth, boosting positive sentiment towards Prabowo by 18%\"\n",
    "    \"\"\"\n",
    "    response_text = call_gemini(prompt)\n",
    "\n",
    "\n",
    "    try:\n",
    "        json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "    except:\n",
    "        response_text = (re.sub(f\"(?<!\\:\\s)\\\"([\\w\\s\\/\\.\\@\\-]+)\\\"(?![\\,\\:])\",r\"`\\1`\",response_text))\n",
    "\n",
    "\n",
    "        try:\n",
    "            json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "        except:\n",
    "            json_string = []\n",
    "            for i in re.findall(r'{.*?}', response_text, flags=re.I|re.S):\n",
    "                try:\n",
    "                    json_string.append(eval(i))\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "\n",
    "    list_prediction.extend(json_string)\n",
    "\n",
    "\n",
    "topics_result = pd.DataFrame(list_prediction).merge(df_all_topics, on = 'unified_issue')\n",
    "topics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d7b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:25:39.218232Z",
     "start_time": "2025-04-18T06:25:39.146000Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da09d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:38.438252Z",
     "start_time": "2025-04-18T06:24:38.425667Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(hasil).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d11152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccef6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:27:51.272549Z",
     "start_time": "2025-04-18T06:27:51.260547Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_relevan_data(keywords,start_date=None, size = 10000 ):\n",
    "    current_date = datetime.now()\n",
    "    if not start_date:\n",
    "        # Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "        \n",
    "        date_120_days_ago = current_date - timedelta(days=120)\n",
    "        # Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "        start_date = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    es_helper = ElasticsearchHelper(host=\"http://localhost:9200\")\n",
    "    query = {\n",
    "        \"_source\": [\"issue\", \"post_caption\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\", \"channel\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                                \"query\": keyword,\n",
    "                                \"operator\": \"and\"  # Semua kata dalam keyword harus ada\n",
    "                            }\n",
    "                        }\n",
    "                    } for keyword in keywords\n",
    "                ],\n",
    "              \"must\": [\n",
    "                            {\n",
    "                                \"range\": {\n",
    "                                    \"post_created_at\": {\n",
    "                                        \"gte\": start_date,  # Greater than or equal to 120 hari yang lalu\n",
    "                                        \"lte\": current_date.strftime(\"%Y-%m-%d %H:%M:%S\")  # Less than or equal to sekarang\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"issue\": \"Not Specified\"  # Filter: 'issue' tidak boleh \"Not Specified\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1  # Minimal satu keyword yang cocok\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es_helper.fetch_data(index=\"twitter_data,linkedin_data,news_data,reddit_data,youtube_data\",\n",
    "                                query=query, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93191139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:28:17.993980Z",
     "start_time": "2025-04-18T06:28:14.674935Z"
    }
   },
   "outputs": [],
   "source": [
    "data = get_relevan_data(['ruu tni'],size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb90ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:28:34.445211Z",
     "start_time": "2025-04-18T06:28:34.434698Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e194c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa35e610",
   "metadata": {},
   "source": [
    "# KOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8861555f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T04:30:36.727403Z",
     "start_time": "2025-04-18T04:30:36.546549Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.kol_overview import search_kol\n",
    "\n",
    "hasil = search_kol(  \n",
    "    owner_id = '5',\n",
    "    project_name = \"gibran raka\",\n",
    "    es_host='localhost:9200',\n",
    "    keywords=[\"pdip\"],sentiment=['negative'])\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(hasil).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a7dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:48:35.694679Z",
     "start_time": "2025-04-18T05:48:29.806913Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
