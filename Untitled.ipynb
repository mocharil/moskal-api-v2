{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aecf78a",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff89b76",
   "metadata": {},
   "source": [
    "Topics dan KOL belum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cbd40",
   "metadata": {},
   "source": [
    "## keyword trends V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81741b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:16:58.774503Z",
     "start_time": "2025-04-17T11:16:58.746274Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from utils.keyword_trends import get_keyword_trends\n",
    "result = get_keyword_trends(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    \n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6439c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:27:50.138012Z",
     "start_time": "2025-04-17T05:27:50.111428Z"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3a0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c08cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e82c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73cb8ba9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## context of discussion V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8260d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:28:36.279645Z",
     "start_time": "2025-04-17T05:28:36.235043Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.context_of_disccusion import get_context_of_discussion\n",
    "result = get_context_of_discussion(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b92b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:28:39.122675Z",
     "start_time": "2025-04-17T05:28:39.107211Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b341e9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "024831ff",
   "metadata": {},
   "source": [
    "## list of mentions Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f6800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:24:46.142578Z",
     "start_time": "2025-04-17T11:24:46.090992Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "\n",
    "# Contoh penggunaan dasar\n",
    "result = get_mentions(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    source = [\"post_created_at\",\"issue\"],\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],\n",
    "    page=1,\n",
    "    page_size=3000,\n",
    "    sort_type=\"relevant\",  # Sort berdasarkan viral_score\n",
    "    sort_order=\"desc\",\n",
    ")\n",
    "\n",
    "result['pagination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d75ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T11:24:48.067773Z",
     "start_time": "2025-04-17T11:24:48.059738Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829e66b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# analysis DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e75235",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## overview X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94126ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:40:20.199454Z",
     "start_time": "2025-04-17T10:40:20.084438Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from utils.analysis_overview import get_social_media_matrix\n",
    "matrix = get_social_media_matrix(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\",'dan'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-04-01',\n",
    "    end_date='2025-04-10',)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c00d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1208a174",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## mentions by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e757553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:36.121428Z",
     "start_time": "2025-04-17T08:45:36.058923Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_category_analytics.py - Script untuk mendapatkan analisis kategori dan sentimen\n",
    "\n",
    "Script ini mengambil data tentang mentions berdasarkan kategori dan sentiment berdasarkan\n",
    "kategori dari Elasticsearch untuk visualisasi.\n",
    "\"\"\"\n",
    "from utils.analysis_sentiment_mentions import get_category_analytics\n",
    "\n",
    "data= get_category_analytics(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e76c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:37.702530Z",
     "start_time": "2025-04-17T08:45:37.685295Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30cdd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:15.528427Z",
     "start_time": "2025-04-17T08:45:15.515683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint({\n",
    "    \"mentions_by_category\":mentions_by_category,\n",
    "    'sentiemnt_by_category':sentiment_by_category,\n",
    "    'sentiment_breakdown':sentiment_breakdown\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401578ff",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## presence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897d36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:47:36.652282Z",
     "start_time": "2025-04-17T08:47:36.555309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.presence_score import get_presence_score, format_presence_score_data\n",
    "presence_data = get_presence_score(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])\n",
    "\n",
    "# Format data untuk tampilan yang lebih baik\n",
    "formatted_data = format_presence_score_data(presence_data)\n",
    "\n",
    "\n",
    "formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b336a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:38:06.490527Z",
     "start_time": "2025-04-17T04:38:06.429218Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4892b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:37:45.705193Z",
     "start_time": "2025-04-17T04:37:45.700253Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "presence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a49fd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:15:11.174708Z",
     "start_time": "2025-04-16T05:15:11.140661Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## most share of voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727b6b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:22:34.500572Z",
     "start_time": "2025-04-17T07:22:34.488736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.share_of_voice import get_share_of_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df367653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:43:57.180454Z",
     "start_time": "2025-04-17T04:43:57.160373Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47995ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:22:35.422668Z",
     "start_time": "2025-04-17T07:22:35.387444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_share_of_voice(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0fdcb7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## most followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf773e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:43:49.408511Z",
     "start_time": "2025-04-17T07:43:49.389139Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.most_followers import get_most_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc2869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:45:48.044686Z",
     "start_time": "2025-04-17T07:45:48.022524Z"
    },
    "code_folding": [
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "most_followers.py - Script untuk mendapatkan akun dengan followers/koneksi terbanyak\n",
    "\n",
    "Script ini menganalisis data dari Elasticsearch untuk menentukan akun\n",
    "dengan jumlah followers/koneksi terbanyak yang membicarakan suatu topik,\n",
    "dengan dukungan pagination.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Literal, Optional, Union\n",
    "\n",
    "# Import utilitas dari paket utils\n",
    "from utils.es_client import get_elasticsearch_client\n",
    "from utils.es_query_builder import get_date_range\n",
    "\n",
    "def get_most_followers(\n",
    "    es_host='localhost:9200',\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None,\n",
    "    limit=10,\n",
    "    page=1,\n",
    "    page_size=10,\n",
    "    include_total_count=True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Mendapatkan daftar akun dengan jumlah followers terbanyak\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    es_host : str\n",
    "        Host Elasticsearch\n",
    "    es_username : str, optional\n",
    "        Username Elasticsearch\n",
    "    es_password : str, optional\n",
    "        Password Elasticsearch\n",
    "    use_ssl : bool, optional\n",
    "        Gunakan SSL untuk koneksi\n",
    "    verify_certs : bool, optional\n",
    "        Verifikasi sertifikat SSL\n",
    "    ca_certs : str, optional\n",
    "        Path ke sertifikat CA\n",
    "    keywords : list, optional\n",
    "        Daftar keyword untuk filter\n",
    "    search_exact_phrases : bool, optional\n",
    "        Jika True, gunakan match_phrase untuk pencarian keyword, jika False gunakan match AND\n",
    "    case_sensitive : bool, optional\n",
    "        Jika True, pencarian keyword bersifat case-sensitive, jika False tidak memperhatikan huruf besar/kecil\n",
    "    sentiment : list, optional\n",
    "        Daftar sentiment ['positive', 'negative', 'neutral']\n",
    "    start_date : str, optional\n",
    "        Tanggal awal format YYYY-MM-DD\n",
    "    end_date : str, optional\n",
    "        Tanggal akhir format YYYY-MM-DD\n",
    "    date_filter : str, optional\n",
    "        Filter tanggal untuk digunakan jika start_date dan end_date tidak disediakan\n",
    "    custom_start_date : str, optional\n",
    "        Tanggal awal kustom jika date_filter adalah \"custom\"\n",
    "    custom_end_date : str, optional\n",
    "        Tanggal akhir kustom jika date_filter adalah \"custom\"\n",
    "    channels : list, optional\n",
    "        Daftar channel ['twitter', 'news', 'instagram', dll]\n",
    "    importance : str, optional\n",
    "        'important mentions' atau 'all mentions'\n",
    "    influence_score_min : float, optional\n",
    "        Skor pengaruh minimum (0-100)\n",
    "    influence_score_max : float, optional\n",
    "        Skor pengaruh maksimum (0-100)\n",
    "    region : list, optional\n",
    "        Daftar region\n",
    "    language : list, optional\n",
    "        Daftar bahasa\n",
    "    domain : list, optional\n",
    "        Daftar domain untuk filter\n",
    "    limit : int, optional\n",
    "        Jumlah total akun yang akan dianalisis (untuk keseluruhan dataset)\n",
    "    page : int, optional\n",
    "        Halaman yang akan diambil (untuk pagination)\n",
    "    page_size : int, optional\n",
    "        Jumlah item per halaman (untuk pagination)\n",
    "    include_total_count : bool, optional\n",
    "        Sertakan jumlah total akun di hasil\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict\n",
    "        Dictionary berisi daftar akun dengan jumlah followers terbanyak dengan dukungan pagination\n",
    "    \"\"\"\n",
    "    # Buat koneksi Elasticsearch\n",
    "    es = get_elasticsearch_client(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs\n",
    "    )\n",
    "    \n",
    "    if not es:\n",
    "        return {\n",
    "            \"data\": [],\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": 0,\n",
    "                \"total_items\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Definisikan semua channel yang mungkin\n",
    "    default_channels = [\"twitter\", \"linkedin\", \"reddit\", \"youtube\", \"news\"]\n",
    "    \n",
    "    # Filter channels jika disediakan\n",
    "    if channels:\n",
    "        selected_channels = [ch for ch in channels if ch in default_channels]\n",
    "    else:\n",
    "        selected_channels = default_channels\n",
    "    \n",
    "    # Mapping channel ke index Elasticsearch\n",
    "    channel_to_index = {\n",
    "        \"twitter\": \"twitter_data\",\n",
    "        \"instagram\": \"instagram_data\",\n",
    "        \"linkedin\": \"linkedin_data\",\n",
    "        \"reddit\": \"reddit_data\",\n",
    "        \"youtube\": \"youtube_data\",\n",
    "        \"tiktok\": \"tiktok_data\",\n",
    "        \"news\": \"news_data\",\n",
    "        \"blogs\": \"blogs_data\",\n",
    "        \"facebook\": \"facebook_data\",\n",
    "        \"podcasts\": \"podcasts_data\",\n",
    "        \"videos\": \"videos_data\",\n",
    "        \"web\": \"web_data\"\n",
    "    }\n",
    "    \n",
    "    # Dapatkan indeks yang akan di-query\n",
    "    indices = [channel_to_index[ch] for ch in selected_channels if ch in channel_to_index]\n",
    "    \n",
    "    if not indices:\n",
    "        print(\"Error: No valid indices\")\n",
    "        return {\n",
    "            \"data\": [],\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": 0,\n",
    "                \"total_items\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Dapatkan rentang tanggal jika tidak disediakan\n",
    "    if not start_date or not end_date:\n",
    "        start_date, end_date = get_date_range(\n",
    "            date_filter=date_filter,\n",
    "            custom_start_date=custom_start_date,\n",
    "            custom_end_date=custom_end_date\n",
    "        )\n",
    "    \n",
    "    # Bangun query untuk mendapatkan akun dengan followers terbanyak\n",
    "    must_conditions = [\n",
    "        {\n",
    "            \"range\": {\n",
    "                \"post_created_at\": {\n",
    "                    \"gte\": start_date,\n",
    "                    \"lte\": end_date\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Tambahkan filter keywords jika ada\n",
    "    if keywords:\n",
    "        # Konversi keywords ke list jika belum\n",
    "        keyword_list = keywords if isinstance(keywords, list) else [keywords]\n",
    "        keyword_should_conditions = []\n",
    "        \n",
    "        # Tentukan field yang akan digunakan berdasarkan case_sensitive\n",
    "        caption_field = \"post_caption.keyword\" if case_sensitive else \"post_caption\"\n",
    "        issue_field = \"issue.keyword\" if case_sensitive else \"issue\"\n",
    "        \n",
    "        if search_exact_phrases:\n",
    "            # Gunakan match_phrase untuk exact matching\n",
    "            for kw in keyword_list:\n",
    "                keyword_should_conditions.append({\"match_phrase\": {caption_field: kw}})\n",
    "                keyword_should_conditions.append({\"match_phrase\": {issue_field: kw}})\n",
    "        else:\n",
    "            # Gunakan match dengan operator AND\n",
    "            for kw in keyword_list:\n",
    "                keyword_should_conditions.append({\"match\": {caption_field: {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "                keyword_should_conditions.append({\"match\": {issue_field: {\"query\": kw, \"operator\": \"AND\"}}})\n",
    "        \n",
    "        keyword_condition = {\n",
    "            \"bool\": {\n",
    "                \"should\": keyword_should_conditions,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        must_conditions.append(keyword_condition)\n",
    "    \n",
    "    # Bangun filter untuk query\n",
    "    filter_conditions = []\n",
    "    \n",
    "    # Filter untuk sentiment\n",
    "    if sentiment:\n",
    "        sentiment_condition = {\n",
    "            \"terms\": {\n",
    "                \"sentiment\": sentiment if isinstance(sentiment, list) else [sentiment]\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(sentiment_condition)\n",
    "    \n",
    "    # Filter untuk importance\n",
    "    if importance == \"important mentions\":\n",
    "        filter_conditions.append({\n",
    "            \"range\": {\n",
    "                \"influence_score\": {\n",
    "                    \"gt\": 50\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    # Filter untuk influence score\n",
    "    if influence_score_min is not None or influence_score_max is not None:\n",
    "        influence_condition = {\"range\": {\"influence_score\": {}}}\n",
    "        if influence_score_min is not None:\n",
    "            influence_condition[\"range\"][\"influence_score\"][\"gte\"] = influence_score_min\n",
    "        if influence_score_max is not None:\n",
    "            influence_condition[\"range\"][\"influence_score\"][\"lte\"] = influence_score_max\n",
    "        filter_conditions.append(influence_condition)\n",
    "        \n",
    "    # Filter untuk region menggunakan wildcard\n",
    "    if region:\n",
    "        region_conditions = []\n",
    "        region_list = region if isinstance(region, list) else [region]\n",
    "        \n",
    "        for r in region_list:\n",
    "            region_conditions.append({\"wildcard\": {\"region\": f\"*{r}*\"}})\n",
    "        \n",
    "        region_filter = {\n",
    "            \"bool\": {\n",
    "                \"should\": region_conditions,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(region_filter)\n",
    "        \n",
    "    # Filter untuk language menggunakan wildcard\n",
    "    if language:\n",
    "        language_conditions = []\n",
    "        language_list = language if isinstance(language, list) else [language]\n",
    "        \n",
    "        for l in language_list:\n",
    "            language_conditions.append({\"wildcard\": {\"language\": f\"*{l}*\"}})\n",
    "        \n",
    "        language_filter = {\n",
    "            \"bool\": {\n",
    "                \"should\": language_conditions,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(language_filter)\n",
    "        \n",
    "    # Filter untuk domain\n",
    "    if domain:\n",
    "        domain_condition = {\n",
    "            \"bool\": {\n",
    "                \"should\": [{\"wildcard\": {\"link_post\": f\"*{d}*\"}} for d in (domain if isinstance(domain, list) else [domain])],\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "        filter_conditions.append(domain_condition)\n",
    "    \n",
    "    # Gabungkan semua kondisi ke dalam query utama\n",
    "    query = {\n",
    "        \"size\": 0,  # Kita hanya perlu agregasi\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": must_conditions\n",
    "            }\n",
    "        },\n",
    "        \"aggs\": {\n",
    "            \"by_channel\": {\n",
    "                \"terms\": {\n",
    "                    \"field\": \"channel\",\n",
    "                    \"size\": len(selected_channels)\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"by_username\": {\n",
    "                        \"terms\": {\n",
    "                            \"field\": \"username\",\n",
    "                            \"size\": limit\n",
    "                        },\n",
    "                        \"aggs\": {\n",
    "                            \"subscribers\": {\n",
    "                                \"max\": {\n",
    "                                    \"field\": \"subscriber\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"followers\": {\n",
    "                                \"max\": {\n",
    "                                    \"field\": \"user_followers\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"connections\": {\n",
    "                                \"max\": {\n",
    "                                    \"field\": \"user_connections\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"influence_score\": {\n",
    "                                \"avg\": {\n",
    "                                    \"field\": \"user_influence_score\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"total_reach\": {\n",
    "                                \"sum\": {\n",
    "                                    \"field\": \"reach_score\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"total_mentions\": {\n",
    "                \"value_count\": {\n",
    "                    \"field\": \"link_post\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Tambahkan filter jika ada\n",
    "    if filter_conditions:\n",
    "        query[\"query\"][\"bool\"][\"filter\"] = filter_conditions\n",
    "    \n",
    "    \n",
    "    print(query)\n",
    "    try:\n",
    "        # Jalankan query\n",
    "        response = es.search(\n",
    "            index=\",\".join(indices),\n",
    "            body=query\n",
    "        )\n",
    "        \n",
    "        # Proses hasil untuk mendapatkan akun dengan followers terbanyak\n",
    "        channel_buckets = response[\"aggregations\"][\"by_channel\"][\"buckets\"]\n",
    "        total_mentions = response[\"aggregations\"][\"total_mentions\"][\"value\"]\n",
    "        \n",
    "        # Kumpulkan data dari semua channel\n",
    "        followers_data = []\n",
    "        \n",
    "        for channel_bucket in channel_buckets:\n",
    "            channel = channel_bucket[\"key\"]\n",
    "            username_buckets = channel_bucket[\"by_username\"][\"buckets\"]\n",
    "            \n",
    "            for username_bucket in username_buckets:\n",
    "                username = username_bucket[\"key\"]\n",
    "                mentions = username_bucket[\"doc_count\"]\n",
    "                subscribers = username_bucket[\"subscribers\"][\"value\"]\n",
    "                followers = username_bucket[\"followers\"][\"value\"]\n",
    "                connections = username_bucket[\"connections\"][\"value\"]\n",
    "                influence_score = username_bucket[\"influence_score\"][\"value\"] or 0\n",
    "                reach = username_bucket[\"total_reach\"][\"value\"]\n",
    "                \n",
    "                followers_data.append({\n",
    "                    \"channel\": channel,\n",
    "                    \"username\": username,\n",
    "                    \"followers\": followers or connections or subscribers or 0,\n",
    "                    \"influence_score\": influence_score,\n",
    "                    \"total_mentions\": mentions,\n",
    "                    \"total_reach\": reach\n",
    "                })\n",
    "        \n",
    "        \n",
    "        print(followers_data)\n",
    "        # Sortir berdasarkan jumlah followers\n",
    "        followers_data.sort(key=lambda x: x[\"followers\"], reverse=True)\n",
    "        \n",
    "        # Pagination\n",
    "        total_items = len(followers_data)\n",
    "        total_pages = (total_items + page_size - 1) // page_size  # ceiling division\n",
    "        \n",
    "        start_index = (page - 1) * page_size\n",
    "        end_index = min(start_index + page_size, total_items)\n",
    "        \n",
    "        paginated_data = followers_data[start_index:end_index]\n",
    "        \n",
    "        # Format username\n",
    "        for item in paginated_data:\n",
    "            if item[\"channel\"] == \"twitter\" and not item[\"username\"].startswith(\"@\"):\n",
    "                item[\"username\"] = f\"@{item['username']}\"\n",
    "        \n",
    "        # Buat hasil dengan informasi pagination\n",
    "        result = {\n",
    "            \"data\": paginated_data,\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": total_pages,\n",
    "                \"total_items\": total_items\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Tambahkan daftar channel yang digunakan\n",
    "        result[\"channels\"] = selected_channels\n",
    "        \n",
    "        # Tambahkan total mentions keseluruhan jika diminta\n",
    "        if include_total_count:\n",
    "            result[\"total_mentions\"] = total_mentions\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Elasticsearch: {e}\")\n",
    "        return {\n",
    "            \"data\": [],\n",
    "            \"pagination\": {\n",
    "                \"page\": page,\n",
    "                \"page_size\": page_size,\n",
    "                \"total_pages\": 0,\n",
    "                \"total_items\": 0\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d832d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:45:48.589233Z",
     "start_time": "2025-04-17T07:45:48.522116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_most_followers(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-21',\n",
    "    end_date='2025-04-30',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128d8ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c6d1a2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## trending hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d25ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:53:19.070408Z",
     "start_time": "2025-04-17T08:53:18.992762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.trending_hashtags import get_trending_hashtags\n",
    "results = get_trending_hashtags(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "   page=1,\n",
    "    page_size=5)\n",
    "\n",
    "# Display pagination info\n",
    "print(f\"Showing page {results['pagination']['page']} of {results['pagination']['total_pages']}\")\n",
    "print(f\"Total hashtags found: {results['pagination']['total_items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3894dc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## trending links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db731b2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:52:25.282384Z",
     "start_time": "2025-04-16T06:52:24.884484Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.trending_links import get_trending_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906598f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:56:03.231977Z",
     "start_time": "2025-04-17T04:56:03.175531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfffcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:56:03.616692Z",
     "start_time": "2025-04-17T04:56:03.518361Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_trending_links(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfc973",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139be06",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078c7575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:50:21.228378Z",
     "start_time": "2025-04-16T06:50:21.216129Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## popular emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21661eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:16:34.651513Z",
     "start_time": "2025-04-16T07:16:34.498486Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.popular_emojis import get_popular_emojis\n",
    "\n",
    "# Get top emojis from Twitter content\n",
    "emojis = get_popular_emojis(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"ruu tni\"],\n",
    "    channels=[\"twitter\",\"youtube\",'news','reddit'],\n",
    "    page=1,\n",
    "    page_size=50\n",
    ")\n",
    "\n",
    "emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2df5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:58:49.239498Z",
     "start_time": "2025-04-17T04:58:49.214744Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac1b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T04:58:58.310464Z",
     "start_time": "2025-04-17T04:58:58.066456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_popular_emojis(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'],    page=1,\n",
    "    page_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f89ff4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5c103",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5913947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:45:45.468884Z",
     "start_time": "2025-04-16T07:45:45.346504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.analysis_overview import get_social_media_matrix\n",
    "matrix = get_social_media_matrix(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"prabowo danantara\", \"dan\"],\n",
    "    start_date=\"2025-04-01\",\n",
    "    end_date=\"2025-04-02\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1483718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T07:45:48.478448Z",
     "start_time": "2025-04-16T07:45:48.458423Z"
    },
    "hidden": true
   },
   "source": [
    "## stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7061b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:43:57.433068Z",
     "start_time": "2025-04-16T09:43:57.350271Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.summary_stats import get_stats_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2b433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:03:36.568355Z",
     "start_time": "2025-04-17T05:03:36.523711Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_stats_summary(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dbea7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a65f8694",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fba336",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sama seperti sebelumnya, tapi yg dimasukin adalah list issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a50f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## sentiment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91506f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:03:57.948660Z",
     "start_time": "2025-04-17T10:03:57.780940Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "\n",
    "# Contoh penggunaan dasar\n",
    "post_positive = get_mentions(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    language = ['Indo'],\n",
    "    page=1,\n",
    "    page_size=50,\n",
    "    sentiment = ['positive'],\n",
    "    source = [\"post_caption\"],\n",
    "    sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "    sort_order=\"desc\",\n",
    ")\n",
    "\n",
    "post_negative = get_mentions(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    language = ['Indo'],\n",
    "    page=1,\n",
    "    page_size=50,\n",
    "    sentiment = ['negative'],\n",
    "    source = [\"post_caption\"],\n",
    "    sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "    sort_order=\"desc\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c9a591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:15:54.284202Z",
     "start_time": "2025-04-17T10:15:54.273169Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.list_of_mentions import get_mentions\n",
    "from utils.gemini import call_gemini\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def get_topics_sentiment_analysis(\n",
    "    es_host='localhost:9200',\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Mengambil analisis topik berdasarkan sentimen (positif dan negatif) \n",
    "    menggunakan model Gemini untuk menganalisis konten.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary berisi rangkuman topik positif dan negatif:\n",
    "        {\n",
    "            'positive_topics': \"...\",\n",
    "            'negative_topics': \"...\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Mendapatkan post positif\n",
    "    post_positive = get_mentions(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs,\n",
    "        keywords=keywords,\n",
    "        search_exact_phrases=search_exact_phrases,\n",
    "        case_sensitive=case_sensitive,\n",
    "        sentiment=['positive'],\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        date_filter=date_filter,\n",
    "        custom_start_date=custom_start_date,\n",
    "        custom_end_date=custom_end_date,\n",
    "        channels=channels,\n",
    "        importance=importance,\n",
    "        influence_score_min=influence_score_min,\n",
    "        influence_score_max=influence_score_max,\n",
    "        region=region,\n",
    "        language=language,\n",
    "        domain=domain,\n",
    "        sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "        sort_order=\"desc\",\n",
    "    page=1,\n",
    "    page_size=50\n",
    "    )\n",
    "\n",
    "    # Mendapatkan post negatif\n",
    "    post_negative = get_mentions(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs,\n",
    "        keywords=keywords,\n",
    "        search_exact_phrases=search_exact_phrases,\n",
    "        case_sensitive=case_sensitive,\n",
    "        sentiment=['negative'],\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        date_filter=date_filter,\n",
    "        custom_start_date=custom_start_date,\n",
    "        custom_end_date=custom_end_date,\n",
    "        channels=channels,\n",
    "        importance=importance,\n",
    "        influence_score_min=influence_score_min,\n",
    "        influence_score_max=influence_score_max,\n",
    "        region=region,\n",
    "        language=language,\n",
    "        domain=domain,\n",
    "        sort_type=\"viral_score\",  # Sort berdasarkan viral_score\n",
    "        sort_order=\"desc\",\n",
    "        page=1,\n",
    "    page_size=50\n",
    "    )\n",
    "\n",
    "    # Menyusun prompt untuk Gemini\n",
    "    prompt = f\"\"\"You are a Social Media Analyst Expert. Your task is to analyze and summarize the content based on the list of social media posts provided below. The posts are divided into two categories based on sentiment:\n",
    "\n",
    "    POSITIVE POSTS\n",
    "    {post_positive['data']}\n",
    "\n",
    "    NEGATIVE POSTS\n",
    "    {post_negative['data']}\n",
    "\n",
    "    OUTPUT (in JSON format):\n",
    "    {{\n",
    "      \"positive_topics\": \"<Provide a concise summary (2–3 sentences) that captures the key topics or themes discussed in the positive posts.>\",\n",
    "      \"negative_topics\": \"<Provide a concise summary (2–3 sentences) that captures the key topics or concerns raised in the negative posts.>\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Memanggil Gemini API\n",
    "    prediction = call_gemini(prompt)\n",
    "    \n",
    "    try:\n",
    "        # Mencoba parse JSON dari respons\n",
    "        json_result = re.findall(r'\\{.*\\}', prediction, flags=re.I|re.S)[0]\n",
    "        return json.loads(json_result)\n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        # Menangani error parsing\n",
    "        return {\n",
    "            \"positive_topics\": \"Error analyzing positive topics.\",\n",
    "            \"negative_topics\": \"Error analyzing negative topics.\",\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb268e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:16:05.886141Z",
     "start_time": "2025-04-17T10:16:01.293707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_topics_sentiment_analysis( es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    language = ['Indo']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3603a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c12875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:59:57.087681Z",
     "start_time": "2025-04-16T09:59:57.046298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.keyword_trends import get_keyword_trends\n",
    "get_keyword_trends(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b1fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:59:59.492179Z",
     "start_time": "2025-04-16T09:59:59.474909Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ed2a3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## channel shares and sentiment overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed698cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:04:42.292798Z",
     "start_time": "2025-04-17T05:04:42.242243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_category_analytics.py - Script untuk mendapatkan analisis kategori dan sentimen\n",
    "\n",
    "Script ini mengambil data tentang mentions berdasarkan kategori dan sentiment berdasarkan\n",
    "kategori dari Elasticsearch untuk visualisasi.\n",
    "\"\"\"\n",
    "from utils.analysis_sentiment_mentions import get_category_analytics\n",
    "\n",
    "channel_shares, _ ,sentiment_breakdown = get_category_analytics(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"prabowo danantara\", \"gibran\"],\n",
    "    start_date=\"2025-03-18\",\n",
    "    end_date=\"2025-04-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634db486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T05:04:42.822090Z",
     "start_time": "2025-04-17T05:04:42.816568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "channel_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58c385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:02:40.488684Z",
     "start_time": "2025-04-16T10:02:40.473506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentiment_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51923a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## emotion, intent and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070e77f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:56:51.701972Z",
     "start_time": "2025-04-17T08:56:51.387559Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.intent_emotions_region import get_intents_emotions_region_share\n",
    "analysis = get_intents_emotions_region_share(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[\"gibran\"],\n",
    "    date_filter=\"last 100 days\"\n",
    ")\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abf9a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:36:28.154966Z",
     "start_time": "2025-04-17T07:36:28.097335Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_intents_emotions_region_share(\n",
    "    es_host=\"http://localhost:9200\",\n",
    "    keywords=[ \"prabowo\"],\n",
    "    channels=[\"twitter\",'youtube'],\n",
    "    search_exact_phrases = False,\n",
    "    case_sensitive = False,\n",
    "    date_filter = 'yesterday',\n",
    "    start_date='2025-01-01',\n",
    "    end_date='2025-04-01',\n",
    "    influence_score_max = 10,\n",
    "    region = ['Maluku'],\n",
    "    language = ['Indo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5908ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa8ad5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d72c68",
   "metadata": {},
   "source": [
    "# TOPICCCCCCCCCCCCCCCCCCCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58bda1",
   "metadata": {},
   "source": [
    "## scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdbb53d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:15:14.233323Z",
     "start_time": "2025-04-20T16:14:43.363761Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from utils.gemini import call_gemini\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "\n",
    "# Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "current_date = datetime.now()\n",
    "date_120_days_ago = current_date - timedelta(days=30)\n",
    "# Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "date_120_days_ago_str = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "db_host = \"34.101.146.213\"\n",
    "db_port = 3306\n",
    "db_user = \"arilindra21\"\n",
    "db_password = \"sukabumi030495\"\n",
    "db_name = \"auth_api_db\"\n",
    "       \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class About_MySQL:\n",
    "    def __init__(self, db_host, db_port, db_user, db_password, db_name):\n",
    "        # Membuat URL koneksi dengan format SQLAlchemy\n",
    "        self.database_url = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "        \n",
    "        # Membuat engine SQLAlchemy\n",
    "        self.engine = create_engine(self.database_url)\n",
    "\n",
    "    def to_pull_data(self, query):\n",
    "        # Mengambil data menggunakan query yang diberikan dan mengonversinya ke DataFrame\n",
    "        with self.engine.connect() as connection:\n",
    "            # Menjalankan query dan mengonversi hasilnya ke DataFrame\n",
    "            df = pd.read_sql(text(query), connection)\n",
    "        return df\n",
    "    \n",
    "    def to_push_data(self, dataframe: pd.DataFrame, table_name: str, if_exist: str = 'replace'):\n",
    "        \"\"\"\n",
    "        Menyimpan DataFrame ke tabel MySQL.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: pd.DataFrame yang akan disimpan\n",
    "        - table_name: nama tabel tujuan\n",
    "        - if_exist: 'replace' untuk mengganti tabel, 'append' untuk menambahkan data\n",
    "        \"\"\"\n",
    "        assert if_exist in ['replace', 'append'], \"Parameter 'if_exist' harus 'replace' atau 'append'\"\n",
    "        \n",
    "        dataframe.to_sql(\n",
    "            name=table_name,\n",
    "            con=self.engine,\n",
    "            if_exists=if_exist,\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        print(f\"✅ Data berhasil dipush ke tabel `{table_name}` dengan mode `{if_exist}`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a688268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:15:14.372681Z",
     "start_time": "2025-04-20T16:15:14.245421Z"
    },
    "code_folding": [
     1,
     50,
     64,
     79,
     121,
     162,
     196,
     248,
     447
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_to_elasticsearch(data: Union[Dict[str, Any], List[Dict[str, Any]]],\n",
    "                           hosts: Union[str, List[str]] = 'http://localhost:9200',\n",
    "                           index: str = 'my_index',\n",
    "                           bulk_size: int = 1000,\n",
    "                           id_field: Optional[str] = None,\n",
    "                           **es_kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ingest data into Elasticsearch using the official Elasticsearch Python client\n",
    "    \n",
    "    Args:\n",
    "        data: Single document (dict) or list of documents to ingest\n",
    "        hosts: Elasticsearch host URL or list of hosts\n",
    "        index: Name of the index\n",
    "        bulk_size: Number of documents to send in each bulk request\n",
    "        id_field: Field to use as document ID\n",
    "        es_kwargs: Additional keyword arguments for Elasticsearch client\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the bulk operation\n",
    "    \"\"\"\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch(hosts=hosts, **es_kwargs)\n",
    "    \n",
    "    # Convert single document to list if needed\n",
    "    documents = data if isinstance(data, list) else [data]\n",
    "    \n",
    "    # Prepare documents for bulk operation\n",
    "    actions = []\n",
    "    for doc in documents:\n",
    "        action = {\n",
    "            \"_index\": index,\n",
    "            \"_source\": doc\n",
    "        }\n",
    "        \n",
    "        # Add document ID if provided\n",
    "        if id_field and id_field in doc:\n",
    "            action[\"_id\"] = doc[id_field]\n",
    "            \n",
    "        actions.append(action)\n",
    "    \n",
    "    # Execute bulk operation\n",
    "    result = helpers.bulk(es, actions, chunk_size=bulk_size)\n",
    "    \n",
    "    return {\n",
    "        \"success_count\": result[0],\n",
    "        \"error_count\": result[1],\n",
    "        \"total_documents\": len(documents)\n",
    "    }\n",
    "\n",
    "def chunk_list(data: List, chunk_size: int) -> List[List]:\n",
    "    \"\"\"\n",
    "    Membagi list menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi chunk-chunk dari list\n",
    "    \"\"\"\n",
    "    # Membagi list data menjadi chunk sesuai dengan ukuran chunk_size\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Membagi DataFrame menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk (baris)\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi DataFrame chunked\n",
    "    \"\"\"\n",
    "    # Menghitung jumlah chunk yang dibutuhkan\n",
    "    chunked_data = [df.iloc[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "    return chunked_data\n",
    "\n",
    "class ElasticsearchHelper:\n",
    "    def __init__(self, host: str):\n",
    "        self.host = host\n",
    "        self.es = self.connect()\n",
    "\n",
    "    def connect(self) -> Elasticsearch:\n",
    "        \"\"\"Membuat koneksi ke Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            es = Elasticsearch(\n",
    "                self.host,\n",
    "                verify_certs=False  # Hanya untuk development, non-TLS\n",
    "            )\n",
    "            return es\n",
    "        except ElasticsearchException as e:\n",
    "            print(f\"Failed to connect to Elasticsearch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_data(self, index: str, query: dict, size: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tarik data dari Elasticsearch dengan scan helpers untuk data besar\"\"\"\n",
    "        try:\n",
    "            if not self.es:\n",
    "                raise ConnectionError(\"Tidak dapat terkoneksi dengan Elasticsearch.\")\n",
    "            \n",
    "            # Gunakan helpers.scan untuk menarik data besar\n",
    "            scan_response = helpers.scan(\n",
    "                client=self.es,\n",
    "                index=index,\n",
    "                query=query,\n",
    "                size=size,\n",
    "                scroll=\"2m\"  # Menggunakan scroll selama 2 menit untuk mengambil data\n",
    "            )\n",
    "\n",
    "            # Ambil hasil dari scan\n",
    "            all_hits = [doc[\"_source\"] for doc in scan_response]\n",
    "\n",
    "            return all_hits\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error umum: {e}\")\n",
    "            return []\n",
    "        \n",
    "def get_relevan_data(keywords):\n",
    "    es_helper = ElasticsearchHelper(host=\"http://localhost:9200\")\n",
    "    query = {\n",
    "        \"_source\": [\"issue\", \"post_caption\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\", \"channel\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                                \"query\": keyword,\n",
    "                                \"operator\": \"and\"  # Semua kata dalam keyword harus ada\n",
    "                            }\n",
    "                        }\n",
    "                    } for keyword in keywords\n",
    "                ],\n",
    "              \"must\": [\n",
    "                            {\n",
    "                                \"range\": {\n",
    "                                    \"post_created_at\": {\n",
    "                                        \"gte\": date_120_days_ago_str,  # Greater than or equal to 120 hari yang lalu\n",
    "                                        \"lte\": current_date.strftime(\"%Y-%m-%d %H:%M:%S\")  # Less than or equal to sekarang\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"issue\": \"Not Specified\"  # Filter: 'issue' tidak boleh \"Not Specified\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1  # Minimal satu keyword yang cocok\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es_helper.fetch_data(index=\"twitter_data,linkedin_data,news_data,reddit_data,youtube_data\",\n",
    "                                query=query, size=10000)\n",
    "    \n",
    "def get_df_issue(df):\n",
    "    issue_total_posts = df.groupby('issue').size().reset_index(name='total_posts')\n",
    "\n",
    "    # Hitung sum viral_score dan reach_score per issue\n",
    "    issue_scores = df.groupby('issue').agg({\n",
    "        'viral_score': 'sum',\n",
    "        'reach_score': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Hitung total per jenis sentiment per issue\n",
    "    sentiment_counts = pd.crosstab(df['issue'], df['sentiment']).reset_index()\n",
    "\n",
    "    # 2. Gabungkan semua metrik\n",
    "    result = issue_total_posts.merge(issue_scores, on='issue')\n",
    "    result = result.merge(sentiment_counts, on='issue')\n",
    "\n",
    "    # 3. Buat fungsi untuk memastikan semua jenis sentiment ada (termasuk jika nilai 0)\n",
    "    def ensure_sentiment_columns(df, sentiments=['positive', 'negative', 'neutral']):\n",
    "        for sentiment in sentiments:\n",
    "            if sentiment not in df.columns:\n",
    "                df[sentiment] = 0\n",
    "        return df\n",
    "\n",
    "    result = ensure_sentiment_columns(result)\n",
    "\n",
    "    # 4. Sorting berdasarkan viral_score (optional, bisa diubah sesuai kebutuhan)\n",
    "    result = result.sort_values(by='viral_score', ascending=False)\n",
    "\n",
    "    # 5. Reset index\n",
    "    df_issue = result.reset_index(drop=True).reset_index()\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    return df_issue\n",
    "\n",
    "def get_central_issue(data_chunk):\n",
    "    issue_list = []\n",
    "    for idx, row in data_chunk.iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "    # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "    prompt = f\"\"\"\n",
    "    Kamu adalah Media Social Analyst Expert dengan keahlian khusus dalam pengelompokan tematik.\n",
    "\n",
    "    # TUGAS\n",
    "    Analisis dan kelompokkan list issue sosial media di bawah ini menjadi kelompok-kelompok tematik.\n",
    "\n",
    "    # INSTRUKSI PENTING\n",
    "    1. Setiap issue ID hanya boleh masuk ke dalam SATU kelompok (mutually exclusive).\n",
    "    2. Hindari tumpang tindih issue di antara kelompok-kelompok.\n",
    "    3. Fokus pada tema/topik utama dari setiap issue.\n",
    "    4. Buat nama kelompok yang singkat, jelas, dan mencerminkan tema utama.\n",
    "    5. Berikan deskripsi kelompok yang informatif dan komprehensif.\n",
    "\n",
    "    # LIST ISSUE\n",
    "    ```\n",
    "    {issue_list}\n",
    "    ```\n",
    "\n",
    "    # FORMAT OUTPUT\n",
    "    Kembalikan hasil pengelompokan dalam format JSON yang tepat berikut ini:\n",
    "    ```\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 1\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [1, 5, 10, 15] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 2\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [2, 6, 11, 16] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    # PARAMETER KUALITAS\n",
    "    - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "    - Kelompokkan berdasarkan kemiripan tema/topik dan keywords, bukan sentimen\n",
    "\n",
    "    Berikan hasil pengelompokan dalam format JSON murni tanpa komentar atau penjelasan tambahan.\n",
    "    \"\"\"\n",
    "\n",
    "    centrality = call_gemini(prompt)\n",
    "    return pd.DataFrame(eval(re.findall(r'\\[.*\\]',centrality, flags=re.I|re.S)[0]))\n",
    "\n",
    "def get_topics_overview(keywords):\n",
    "    #mendapatkan raw data\n",
    "    df = pd.DataFrame(get_relevan_data(keywords))\n",
    "\n",
    "    #mendapatkan dataframe aggregate per issue\n",
    "    df_issue = get_df_issue(df)\n",
    "\n",
    "    #chunk df_issue\n",
    "    data_chunk = chunk_dataframe(df_issue, 100)\n",
    "\n",
    "    #mendapatkan central issue berdasarkan viral score tertinggi\n",
    "    df_central = get_central_issue(data_chunk[0])\n",
    "\n",
    "    LIST_UNIFIED_ISSUE = df_central['unified_issue'].to_list()\n",
    "\n",
    "    SISA = data_chunk[0][~data_chunk[0]['index'].isin([j for i in df_central['list_issue_id'] for j in i])]\n",
    "    if not SISA.empty:\n",
    "        data_chunk[1] = pd.concat([data_chunk[1], SISA])\n",
    "\n",
    "\n",
    "\n",
    "    #mendapatkan seluruh unified issue \n",
    "    all_result = []\n",
    "    for DC in tqdm(data_chunk[1:]):\n",
    "        issue_list = []\n",
    "        for idx, row in DC.iterrows():\n",
    "            issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "\n",
    "        df_predict = ''\n",
    "\n",
    "        while True:\n",
    "            if type(df_predict)!=str:\n",
    "                issue_list = [i for i in issue_list if i['id'] not in [j for i in df_predict['list_issue_id'] for j in i]]\n",
    "            # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "            prompt = f\"\"\"\n",
    "                Kamu adalah Media Social Analyst Expert dengan keahlian dalam pengelompokan tematik dan kategorisasi konten sosial media.\n",
    "\n",
    "                # TUGAS UTAMA\n",
    "                Analisis dan kelompokkan list issue baru ke dalam kelompok tematik yang sudah ada (jika relevan) atau buat kelompok baru jika diperlukan.\n",
    "\n",
    "                # ATURAN PENGELOMPOKAN\n",
    "                1. PENTING: Prioritaskan pengelompokan ke dalam kategori yang sudah ada jika terdapat kemiripan tema.\n",
    "                2. Buat kategori baru HANYA jika issue tidak cocok dengan kelompok yang sudah ada.\n",
    "                3. Setiap issue ID HANYA boleh masuk ke SATU kelompok (mutually exclusive).\n",
    "                4. Hindari TUMPANG TINDIH issue di antara kelompok-kelompok.\n",
    "\n",
    "                # KELOMPOK YANG SUDAH ADA\n",
    "                Berikut adalah kelompok tematik yang sudah ada dan HARUS digunakan jika relevan:\n",
    "                ```\n",
    "                {LIST_UNIFIED_ISSUE}\n",
    "                ```\n",
    "\n",
    "                # LIST ISSUE YANG AKAN DIKELOMPOKKAN\n",
    "                ```\n",
    "                {issue_list}\n",
    "                ```\n",
    "\n",
    "                # PETUNJUK PENGELOMPOKAN\n",
    "                - Jika issue memiliki kemiripan SUBSTANSIAL dengan kelompok yang sudah ada → masukkan ke kelompok tersebut\n",
    "                - Jika issue SAMA SEKALI TIDAK TERKAIT dengan kelompok yang ada → buat kelompok baru\n",
    "                - Kemiripan didasarkan pada tema, topik, kata kunci, dan konteks (bukan sentimen)\n",
    "                - Usahakan untuk TIDAK membuat kelompok baru jika masih bisa dimasukkan ke kelompok yang sudah ada\n",
    "\n",
    "                # FORMAT OUTPUT (JSON)\n",
    "                [\n",
    "                  {{\n",
    "                    \"unified_issue\": \"Nama Issue Kelompok (gunakan dari list yang sudah ada atau buat baru)\",\n",
    "                    \"description\": \"Deskripsi singkat dan jelas tentang tema kelompok ini\",\n",
    "                    \"list_issue_id\": [1, 5, 10, 15] // Daftar ID issue yang masuk dalam kelompok ini\n",
    "                  }},\n",
    "                  ...\n",
    "                ]\n",
    "\n",
    "                # KRITERIA KUALITAS\n",
    "                - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "                - Nama kelompok baru harus singkat, jelas, dan deskriptif\n",
    "                - Prioritaskan penggunaan nama kelompok yang sudah ada\n",
    "                - Pastikan seluruh index issue masuk ke setiap kelompok tanpa tersisia\n",
    "\n",
    "                PENTING: Hasilkan HANYA format JSON murni tanpa komentar, penjelasan, atau notasi lain.\"\"\"\n",
    "\n",
    "            predict = call_gemini(prompt)\n",
    "            df_predict = pd.DataFrame(eval(re.findall(r'\\[.*\\]',predict, flags=re.I|re.S)[0]))\n",
    "            all_result.append(df_predict)\n",
    "\n",
    "            LIST_UNIFIED_ISSUE.extend(df_predict['unified_issue'].to_list())\n",
    "\n",
    "            LIST_UNIFIED_ISSUE = list(set(LIST_UNIFIED_ISSUE))\n",
    "\n",
    "\n",
    "            SISA = set([i['id'] for i in issue_list])- set([j for i in df_predict['list_issue_id'] for j in i])\n",
    "\n",
    "            if not SISA:\n",
    "                break\n",
    "\n",
    "    all_result.append(df_central)\n",
    "    topics = pd.concat(all_result)\n",
    "\n",
    "    #merge dengan nama issue berdasarkan id\n",
    "    all_topics = topics.groupby('unified_issue').agg({'list_issue_id':[lambda s: [j for i in s for j in i], lambda s: len([j for i in s for j in i])]}).reset_index()\n",
    "    all_topics.columns = ['unified_issue','list_issue','total_issue']\n",
    "\n",
    "    merge_all_topics = []\n",
    "    for _,i in all_topics.iterrows():\n",
    "        data = df_issue[df_issue['index'].isin(i['list_issue'])]\n",
    "        dt = data[['total_posts','viral_score','reach_score','negative','positive','neutral']].sum().to_dict()\n",
    "        dt.update({'unified_issue':i['unified_issue'], 'list_issue':data['issue'].to_list()})\n",
    "\n",
    "        merge_all_topics.append(dt)\n",
    "\n",
    "    df_all_topics = pd.DataFrame(merge_all_topics).sort_values('viral_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "    df_all_topics['share_of_voice'] = df_all_topics['total_posts']/df_all_topics['total_posts'].sum()*100\n",
    "\n",
    "\n",
    "    #ambil importance post sebagai perwakilan\n",
    "    list_data = []\n",
    "    for _,i in df_all_topics.iterrows():\n",
    "        unified_issue = i['unified_issue']\n",
    "        list_issue = i['list_issue']\n",
    "        list_caption = df[df['issue'].isin(list_issue)].sort_values('viral_score',ascending = False).drop_duplicates('post_caption')[:5]['post_caption'].to_list()\n",
    "\n",
    "        list_data.append({'unified_issue':unified_issue,\n",
    "                                'list_caption':list_caption,\n",
    "                          'sentiment negative':i['negative'],\n",
    "                          'sentiment positive':i['positive']})\n",
    "\n",
    "    list_prediction = []\n",
    "    for unified_issues_list in tqdm(chunk_list(list_data, 40)):\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Kamu adalah Media Social Analyst Expert dengan keahlian dalam analisis kategori dan isu-isu sosial media.\n",
    "\n",
    "        # TUGAS\n",
    "        Buatkan deskripsi yang informatif, akurat, dan komprehensif untuk setiap kategori unified_issue berikut ini.\n",
    "\n",
    "        # PETUNJUK\n",
    "        1. Deskripsi harus menjelaskan dengan jelas apa yang termasuk dalam kategori tersebut\n",
    "        2. Deskripsi harus ringkas namun komprehensif (maksimal 2-3 kalimat)\n",
    "        3. Gunakan bahasa yang netral dan profesional\n",
    "        4. Fokus pada konten dan cakupan dari kategori isu tersebut\n",
    "        5. Jangan menyertakan opini pribadi atau bias dalam deskripsi\n",
    "        6. Berikan juga description berdasarkan sentimentnya\n",
    "\n",
    "        # DATA UNIFIED_ISSUE\n",
    "        ```\n",
    "        {unified_issues_list}\n",
    "        ```\n",
    "\n",
    "        # FORMAT OUTPUT\n",
    "        Berikan hasil dalam format JSON seperti berikut:\n",
    "        ```\n",
    "        [\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue Lainnya\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          ...\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        ### HARD RULE\n",
    "        - Berikan HANYA output JSON tanpa penjelasan atau komentar tambahan.\n",
    "        - Hindari penjelasan \"This category focuses\" , contoh description yang baik seperti ini \"Conversations praised recent innovations in the nickel industry that emphasize sustainability and local economic growth, boosting positive sentiment towards Prabowo by 18%\"\n",
    "        \"\"\"\n",
    "        response_text = call_gemini(prompt)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "        except:\n",
    "            response_text = (re.sub(f\"(?<!\\:\\s)\\\"([\\w\\s\\/\\.\\@\\-]+)\\\"(?![\\,\\:])\",r\"`\\1`\",response_text))\n",
    "            \n",
    "\n",
    "            try:\n",
    "                json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "            except:\n",
    "                json_string = []\n",
    "                for i in re.findall(r'{.*?}', response_text, flags=re.I|re.S):\n",
    "                    try:\n",
    "                        json_string.append(eval(i))\n",
    "                    except:\n",
    "                        pass  \n",
    "                    \n",
    "        \n",
    "        list_prediction.extend(json_string)\n",
    "\n",
    "\n",
    "    topics_result = pd.DataFrame(list_prediction).merge(df_all_topics, on = 'unified_issue')\n",
    "\n",
    "    return topics_result.sort_values('share_of_voice', ascending = False)\n",
    "\n",
    "def set_topics(keywords):\n",
    "\n",
    "    final_result = get_topics_overview(keywords)\n",
    "\n",
    "    result = ingest_to_elasticsearch(\n",
    "        data= [{'topics':final_result.to_dict(orient = 'records'),\n",
    "                'id':id_}],\n",
    "        hosts=\"http://localhost:9200\",\n",
    "        index=\"topics\",\n",
    "        id_field=\"id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7452750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:15:14.463803Z",
     "start_time": "2025-04-20T16:15:14.444600Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "def create_uuid(keyword):\n",
    "    # Gunakan namespace standar (ada juga untuk URL, DNS, dll)\n",
    "    namespace = uuid.NAMESPACE_DNS\n",
    "\n",
    "    return uuid.uuid5(namespace, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290ae186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T16:28:51.648196Z",
     "start_time": "2025-04-20T16:15:14.474830Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_gibran raka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [03:56<00:00,  8.45s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_partai gerindra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 55/55 [08:51<00:00,  9.67s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.04s/it]\n"
     ]
    }
   ],
   "source": [
    "mysql=About_MySQL( db_host, db_port, db_user, db_password, db_name)\n",
    "\n",
    "query = \"select * from keyword_projects\"\n",
    "data_projects = mysql.to_pull_data(query)\n",
    "\n",
    "for _ , i in data_projects.groupby(['project_name','owner_id']).agg({'relevan_keyword':list}).reset_index().iterrows():\n",
    "    id_ = create_uuid(\"{}_{}\".format(i['owner_id'], i['project_name']))\n",
    "    print(\"{}_{}\".format(i['owner_id'], i['project_name']))\n",
    "    keywords = i['relevan_keyword']\n",
    "    set_topics(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec1c82",
   "metadata": {},
   "source": [
    "## search topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3816ae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:03.056500Z",
     "start_time": "2025-04-18T06:24:03.034567Z"
    },
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.topics_overview import search_topics\n",
    "#INPUT\n",
    "owner_id = 5\n",
    "project_name = 'gibran rakas'\n",
    "keywords = [\n",
    "            \"pdip\",\n",
    "            \"gerindra\",\n",
    "            \"solo\",\n",
    "            \"wakil presiden terpilih\",\n",
    "            \"wali kota solo\",\n",
    "            \"gibran raka\",\n",
    "            \"pilpres 2024\",\n",
    "            \"kaesang pangarep\",\n",
    "            \"anak presiden jokowi\",\n",
    "            \"prabowo gibran\"\n",
    "          ]\n",
    "\n",
    "hasil = search_topics(  \n",
    "    owner_id = owner_id,\n",
    "    project_name = project_name,\n",
    "    es_host='localhost:9200',\n",
    "    keywords=keywords,\n",
    "    sentiment = ['neutral'],\n",
    "    channels = ['twitter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e748c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T02:46:05.470451Z",
     "start_time": "2025-04-18T02:46:05.433574Z"
    },
    "code_folding": [
     13,
     48,
     104
    ]
   },
   "outputs": [],
   "source": [
    "from utils.es_client import get_elasticsearch_client\n",
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import uuid\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "\n",
    "def create_uuid(keyword):\n",
    "    # Gunakan namespace standar (ada juga untuk URL, DNS, dll)\n",
    "    namespace = uuid.NAMESPACE_DNS\n",
    "\n",
    "    return uuid.uuid5(namespace, keyword)\n",
    "\n",
    "def matching_issue(final_result, df_data):\n",
    "    #final_result : dataframe yg isinya group topics \n",
    "    #df_data :dataframe yang ingin di cari grup nya\n",
    "     \n",
    "    hasil = []\n",
    "    for _,i  in final_result.iterrows():\n",
    "        list_issue = i['list_issue']\n",
    "        dt = df_data[df_data['issue'].isin(list_issue)]\n",
    "\n",
    "\n",
    "        if dt.empty:\n",
    "            continue\n",
    "\n",
    "        row = i.to_dict().copy()\n",
    "\n",
    "        agg_sentiment = dt.groupby('sentiment').size().to_dict()\n",
    "        sentiment_categories = ['positive', 'negative', 'neutral']\n",
    "        for category in sentiment_categories:\n",
    "            agg_sentiment.setdefault(category, 0)\n",
    "\n",
    "        agg_sentiment = {key: value for key, value in agg_sentiment.items() if key in sentiment_categories}\n",
    "\n",
    "\n",
    "        agg_score = dt[['viral_score','reach_score']].sum().to_dict()\n",
    "\n",
    "        row.update(agg_sentiment)\n",
    "        row.update(agg_score)\n",
    "        row.update({'total_posts':dt.shape[0]})\n",
    "        hasil.append(row)\n",
    "\n",
    "    df = pd.DataFrame(hasil).fillna(0)\n",
    "    df['share_of_voice'] = df['total_posts']/df['total_posts'].sum()*100\n",
    "\n",
    "    return df.to_dict(orient = 'records')\n",
    "\n",
    "def search_topics(  \n",
    "    owner_id = None,\n",
    "    project_name = None,\n",
    "    es_host='localhost:9200',\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None):\n",
    "    \n",
    "    print('(((((((((((((((((((((( MASUK ))))))))))))))))))))))')\n",
    "    es = get_elasticsearch_client(\n",
    "        es_host=es_host,\n",
    "        es_username=es_username,\n",
    "        es_password=es_password,\n",
    "        use_ssl=use_ssl,\n",
    "        verify_certs=verify_certs,\n",
    "        ca_certs=ca_certs\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #PROCESS\n",
    "    id_ = create_uuid(\"{}_{}\".format(owner_id, project_name))\n",
    "\n",
    "    query = {\n",
    "        \"source\":[],\n",
    "      \"query\": {\n",
    "        \"match\": {\n",
    "          \"_id\": id_\n",
    "        }\n",
    "      }\n",
    "\n",
    "    }\n",
    "\n",
    "    response = es.search(\n",
    "        index='topics',\n",
    "        body=query\n",
    "    )\n",
    "\n",
    "    data_project = [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "    if data_project:\n",
    "\n",
    "        #project sudah tergenerate\n",
    "        final_result = pd.DataFrame(data_project[0]['topics'])\n",
    "\n",
    "        #pake filter berdasarkan input user\n",
    "        result = get_mentions(\n",
    "            source= [\"issue\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\"],\n",
    "            page_size=10000,\n",
    "            es_host=es_host,    \n",
    "            es_username=es_username,\n",
    "            es_password=es_password,\n",
    "            use_ssl=use_ssl,\n",
    "            verify_certs=verify_certs,\n",
    "            ca_certs=ca_certs,\n",
    "            keywords=keywords,\n",
    "            search_exact_phrases=search_exact_phrases,\n",
    "            case_sensitive=case_sensitive,\n",
    "            sentiment=sentiment,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            date_filter=date_filter,\n",
    "            custom_start_date=custom_start_date,\n",
    "            custom_end_date=custom_end_date,\n",
    "            channels=channels,\n",
    "            importance=importance,\n",
    "            influence_score_min=influence_score_min,\n",
    "            influence_score_max=influence_score_max,\n",
    "            region=region,\n",
    "            language=language,\n",
    "            domain=domain,\n",
    "            sort_type = 'popular'\n",
    "    \n",
    "        )\n",
    "\n",
    "        df_data = pd.DataFrame(result['data'])\n",
    "        \n",
    "        if df_data.empty:\n",
    "            return []\n",
    "        \n",
    "        return matching_issue(final_result, df_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b84d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T02:46:06.298402Z",
     "start_time": "2025-04-18T02:46:06.133593Z"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "\"es_host\":\"localhost:9200\",\n",
    "  \"keywords\": [\n",
    "    \"gibran\"\n",
    "  ],\n",
    "  \"owner_id\": \"5\",\n",
    "  \"project_name\": \"gibran raka\"\n",
    "}\n",
    "\n",
    "hasil = search_topics(  \n",
    "    owner_id = '5',\n",
    "    project_name = \"gibran raka\",\n",
    "    es_host='localhost:9200',\n",
    "    keywords=[\"gibran\"],\n",
    "    channels = ['reddit'])\n",
    "pd.DataFrame(hasil).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ceef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:21.874560Z",
     "start_time": "2025-04-18T06:24:19.039861Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.topics_overview import search_topics\n",
    "\n",
    "hasil = search_topics(  \n",
    "    owner_id = '5',\n",
    "    project_name = \"gibran raka\",\n",
    "    es_host='localhost:9200',\n",
    "    keywords=[\"prabowo\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec773c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:24.532917Z",
     "start_time": "2025-04-18T06:24:24.481488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(hasil).fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47ba52",
   "metadata": {},
   "source": [
    "## if topics belum ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64be9e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:12:21.568180Z",
     "start_time": "2025-04-18T06:11:57.022404Z"
    },
    "code_folding": [
     26
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from utils.gemini import call_gemini\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Union, Any, Optional\n",
    "\n",
    "\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "db_host = \"34.101.146.213\"\n",
    "db_port = 3306\n",
    "db_user = \"arilindra21\"\n",
    "db_password = \"sukabumi030495\"\n",
    "db_name = \"auth_api_db\"\n",
    "       \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class About_MySQL:\n",
    "    def __init__(self, db_host, db_port, db_user, db_password, db_name):\n",
    "        # Membuat URL koneksi dengan format SQLAlchemy\n",
    "        self.database_url = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "        \n",
    "        # Membuat engine SQLAlchemy\n",
    "        self.engine = create_engine(self.database_url)\n",
    "\n",
    "    def to_pull_data(self, query):\n",
    "        # Mengambil data menggunakan query yang diberikan dan mengonversinya ke DataFrame\n",
    "        with self.engine.connect() as connection:\n",
    "            # Menjalankan query dan mengonversi hasilnya ke DataFrame\n",
    "            df = pd.read_sql(text(query), connection)\n",
    "        return df\n",
    "    \n",
    "    def to_push_data(self, dataframe: pd.DataFrame, table_name: str, if_exist: str = 'replace'):\n",
    "        \"\"\"\n",
    "        Menyimpan DataFrame ke tabel MySQL.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: pd.DataFrame yang akan disimpan\n",
    "        - table_name: nama tabel tujuan\n",
    "        - if_exist: 'replace' untuk mengganti tabel, 'append' untuk menambahkan data\n",
    "        \"\"\"\n",
    "        assert if_exist in ['replace', 'append'], \"Parameter 'if_exist' harus 'replace' atau 'append'\"\n",
    "        \n",
    "        dataframe.to_sql(\n",
    "            name=table_name,\n",
    "            con=self.engine,\n",
    "            if_exists=if_exist,\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        print(f\"✅ Data berhasil dipush ke tabel `{table_name}` dengan mode `{if_exist}`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6bd437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:18:04.950071Z",
     "start_time": "2025-04-18T06:18:04.591225Z"
    },
    "code_folding": [
     1,
     50,
     64,
     79,
     121,
     172,
     206,
     258,
     457,
     461
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_to_elasticsearch(data: Union[Dict[str, Any], List[Dict[str, Any]]],\n",
    "                           hosts: Union[str, List[str]] = 'http://localhost:9200',\n",
    "                           index: str = 'my_index',\n",
    "                           bulk_size: int = 1000,\n",
    "                           id_field: Optional[str] = None,\n",
    "                           **es_kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ingest data into Elasticsearch using the official Elasticsearch Python client\n",
    "    \n",
    "    Args:\n",
    "        data: Single document (dict) or list of documents to ingest\n",
    "        hosts: Elasticsearch host URL or list of hosts\n",
    "        index: Name of the index\n",
    "        bulk_size: Number of documents to send in each bulk request\n",
    "        id_field: Field to use as document ID\n",
    "        es_kwargs: Additional keyword arguments for Elasticsearch client\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with statistics about the bulk operation\n",
    "    \"\"\"\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch(hosts=hosts, **es_kwargs)\n",
    "    \n",
    "    # Convert single document to list if needed\n",
    "    documents = data if isinstance(data, list) else [data]\n",
    "    \n",
    "    # Prepare documents for bulk operation\n",
    "    actions = []\n",
    "    for doc in documents:\n",
    "        action = {\n",
    "            \"_index\": index,\n",
    "            \"_source\": doc\n",
    "        }\n",
    "        \n",
    "        # Add document ID if provided\n",
    "        if id_field and id_field in doc:\n",
    "            action[\"_id\"] = doc[id_field]\n",
    "            \n",
    "        actions.append(action)\n",
    "    \n",
    "    # Execute bulk operation\n",
    "    result = helpers.bulk(es, actions, chunk_size=bulk_size)\n",
    "    \n",
    "    return {\n",
    "        \"success_count\": result[0],\n",
    "        \"error_count\": result[1],\n",
    "        \"total_documents\": len(documents)\n",
    "    }\n",
    "\n",
    "def chunk_list(data: List, chunk_size: int) -> List[List]:\n",
    "    \"\"\"\n",
    "    Membagi list menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi chunk-chunk dari list\n",
    "    \"\"\"\n",
    "    # Membagi list data menjadi chunk sesuai dengan ukuran chunk_size\n",
    "    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def chunk_dataframe(df: pd.DataFrame, chunk_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Membagi DataFrame menjadi beberapa chunk berdasarkan ukuran yang diberikan.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame yang akan dibagi\n",
    "    - chunk_size: Ukuran setiap chunk (baris)\n",
    "\n",
    "    Returns:\n",
    "    - List yang berisi DataFrame chunked\n",
    "    \"\"\"\n",
    "    # Menghitung jumlah chunk yang dibutuhkan\n",
    "    chunked_data = [df.iloc[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "    return chunked_data\n",
    "\n",
    "class ElasticsearchHelper:\n",
    "    def __init__(self, host: str):\n",
    "        self.host = host\n",
    "        self.es = self.connect()\n",
    "\n",
    "    def connect(self) -> Elasticsearch:\n",
    "        \"\"\"Membuat koneksi ke Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            es = Elasticsearch(\n",
    "                self.host,\n",
    "                verify_certs=False  # Hanya untuk development, non-TLS\n",
    "            )\n",
    "            return es\n",
    "        except ElasticsearchException as e:\n",
    "            print(f\"Failed to connect to Elasticsearch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_data(self, index: str, query: dict, size: int = 1000) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tarik data dari Elasticsearch dengan scan helpers untuk data besar\"\"\"\n",
    "        try:\n",
    "            if not self.es:\n",
    "                raise ConnectionError(\"Tidak dapat terkoneksi dengan Elasticsearch.\")\n",
    "            \n",
    "            # Gunakan helpers.scan untuk menarik data besar\n",
    "            scan_response = helpers.scan(\n",
    "                client=self.es,\n",
    "                index=index,\n",
    "                query=query,\n",
    "                size=size,\n",
    "                scroll=\"2m\"  # Menggunakan scroll selama 2 menit untuk mengambil data\n",
    "            )\n",
    "\n",
    "            # Ambil hasil dari scan\n",
    "            all_hits = [doc[\"_source\"] for doc in scan_response]\n",
    "\n",
    "            return all_hits\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error umum: {e}\")\n",
    "            return []\n",
    "        \n",
    "def get_relevan_data(keywords,start_date=None, sampling = False ):\n",
    "    \n",
    "    \n",
    "    current_date = datetime.now()\n",
    "    if not start_date:\n",
    "        # Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "        \n",
    "        date_120_days_ago = current_date - timedelta(days=120)\n",
    "        # Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "        start_date = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    es_helper = ElasticsearchHelper(host=\"http://localhost:9200\")\n",
    "    query = {\n",
    "        \"_source\": [\"issue\", \"post_caption\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\", \"channel\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                                \"query\": keyword,\n",
    "                                \"operator\": \"and\"  # Semua kata dalam keyword harus ada\n",
    "                            }\n",
    "                        }\n",
    "                    } for keyword in keywords\n",
    "                ],\n",
    "              \"must\": [\n",
    "                            {\n",
    "                                \"range\": {\n",
    "                                    \"post_created_at\": {\n",
    "                                        \"gte\": start_date,  # Greater than or equal to 120 hari yang lalu\n",
    "                                        \"lte\": current_date.strftime(\"%Y-%m-%d %H:%M:%S\")  # Less than or equal to sekarang\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"issue\": \"Not Specified\"  # Filter: 'issue' tidak boleh \"Not Specified\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1  # Minimal satu keyword yang cocok\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es_helper.fetch_data(index=\"twitter_data,linkedin_data,news_data,reddit_data,youtube_data\",\n",
    "                                query=query, size=10000)\n",
    "    \n",
    "def get_df_issue(df):\n",
    "    issue_total_posts = df.groupby('issue').size().reset_index(name='total_posts')\n",
    "\n",
    "    # Hitung sum viral_score dan reach_score per issue\n",
    "    issue_scores = df.groupby('issue').agg({\n",
    "        'viral_score': 'sum',\n",
    "        'reach_score': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Hitung total per jenis sentiment per issue\n",
    "    sentiment_counts = pd.crosstab(df['issue'], df['sentiment']).reset_index()\n",
    "\n",
    "    # 2. Gabungkan semua metrik\n",
    "    result = issue_total_posts.merge(issue_scores, on='issue')\n",
    "    result = result.merge(sentiment_counts, on='issue')\n",
    "\n",
    "    # 3. Buat fungsi untuk memastikan semua jenis sentiment ada (termasuk jika nilai 0)\n",
    "    def ensure_sentiment_columns(df, sentiments=['positive', 'negative', 'neutral']):\n",
    "        for sentiment in sentiments:\n",
    "            if sentiment not in df.columns:\n",
    "                df[sentiment] = 0\n",
    "        return df\n",
    "\n",
    "    result = ensure_sentiment_columns(result)\n",
    "\n",
    "    # 4. Sorting berdasarkan viral_score (optional, bisa diubah sesuai kebutuhan)\n",
    "    result = result.sort_values(by='viral_score', ascending=False)\n",
    "\n",
    "    # 5. Reset index\n",
    "    df_issue = result.reset_index(drop=True).reset_index()\n",
    "\n",
    "    # Tampilkan hasil\n",
    "    return df_issue\n",
    "\n",
    "def get_central_issue(data_chunk):\n",
    "    issue_list = []\n",
    "    for idx, row in data_chunk.iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "    # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "    prompt = f\"\"\"\n",
    "    Kamu adalah Media Social Analyst Expert dengan keahlian khusus dalam pengelompokan tematik.\n",
    "\n",
    "    # TUGAS\n",
    "    Analisis dan kelompokkan list issue sosial media di bawah ini menjadi kelompok-kelompok tematik.\n",
    "\n",
    "    # INSTRUKSI PENTING\n",
    "    1. Setiap issue ID hanya boleh masuk ke dalam SATU kelompok (mutually exclusive).\n",
    "    2. Hindari tumpang tindih issue di antara kelompok-kelompok.\n",
    "    3. Fokus pada tema/topik utama dari setiap issue.\n",
    "    4. Buat nama kelompok yang singkat, jelas, dan mencerminkan tema utama.\n",
    "    5. Berikan deskripsi kelompok yang informatif dan komprehensif.\n",
    "\n",
    "    # LIST ISSUE\n",
    "    ```\n",
    "    {issue_list}\n",
    "    ```\n",
    "\n",
    "    # FORMAT OUTPUT\n",
    "    Kembalikan hasil pengelompokan dalam format JSON yang tepat berikut ini:\n",
    "    ```\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 1\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [1, 5, 10, 15] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Issue Kelompok 2\",\n",
    "        \"description\": \"Deskripsi ringkas tentang tema kelompok ini\",\n",
    "        \"list_issue_id\": [2, 6, 11, 16] // Daftar ID yang masuk dalam kelompok ini\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    # PARAMETER KUALITAS\n",
    "    - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "    - Kelompokkan berdasarkan kemiripan tema/topik dan keywords, bukan sentimen\n",
    "\n",
    "    Berikan hasil pengelompokan dalam format JSON murni tanpa komentar atau penjelasan tambahan.\n",
    "    \"\"\"\n",
    "\n",
    "    centrality = call_gemini(prompt)\n",
    "    return pd.DataFrame(eval(re.findall(r'\\[.*\\]',centrality, flags=re.I|re.S)[0]))\n",
    "\n",
    "def get_topics_overview(keywords):\n",
    "    #mendapatkan raw data\n",
    "    df = pd.DataFrame(get_relevan_data(keywords))\n",
    "\n",
    "    #mendapatkan dataframe aggregate per issue\n",
    "    df_issue = get_df_issue(df)\n",
    "\n",
    "    #chunk df_issue\n",
    "    data_chunk = chunk_dataframe(df_issue, 100)\n",
    "\n",
    "    #mendapatkan central issue berdasarkan viral score tertinggi\n",
    "    df_central = get_central_issue(data_chunk[0])\n",
    "\n",
    "    LIST_UNIFIED_ISSUE = df_central['unified_issue'].to_list()\n",
    "\n",
    "    SISA = data_chunk[0][~data_chunk[0]['index'].isin([j for i in df_central['list_issue_id'] for j in i])]\n",
    "    if not SISA.empty:\n",
    "        data_chunk[1] = pd.concat([data_chunk[1], SISA])\n",
    "\n",
    "\n",
    "\n",
    "    #mendapatkan seluruh unified issue \n",
    "    all_result = []\n",
    "    for DC in tqdm(data_chunk[1:]):\n",
    "        issue_list = []\n",
    "        for idx, row in DC.iterrows():\n",
    "            issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "\n",
    "        df_predict = ''\n",
    "\n",
    "        while True:\n",
    "            if type(df_predict)!=str:\n",
    "                issue_list = [i for i in issue_list if i['id'] not in [j for i in df_predict['list_issue_id'] for j in i]]\n",
    "            # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "            prompt = f\"\"\"\n",
    "                Kamu adalah Media Social Analyst Expert dengan keahlian dalam pengelompokan tematik dan kategorisasi konten sosial media.\n",
    "\n",
    "                # TUGAS UTAMA\n",
    "                Analisis dan kelompokkan list issue baru ke dalam kelompok tematik yang sudah ada (jika relevan) atau buat kelompok baru jika diperlukan.\n",
    "\n",
    "                # ATURAN PENGELOMPOKAN\n",
    "                1. PENTING: Prioritaskan pengelompokan ke dalam kategori yang sudah ada jika terdapat kemiripan tema.\n",
    "                2. Buat kategori baru HANYA jika issue tidak cocok dengan kelompok yang sudah ada.\n",
    "                3. Setiap issue ID HANYA boleh masuk ke SATU kelompok (mutually exclusive).\n",
    "                4. Hindari TUMPANG TINDIH issue di antara kelompok-kelompok.\n",
    "\n",
    "                # KELOMPOK YANG SUDAH ADA\n",
    "                Berikut adalah kelompok tematik yang sudah ada dan HARUS digunakan jika relevan:\n",
    "                ```\n",
    "                {LIST_UNIFIED_ISSUE}\n",
    "                ```\n",
    "\n",
    "                # LIST ISSUE YANG AKAN DIKELOMPOKKAN\n",
    "                ```\n",
    "                {issue_list}\n",
    "                ```\n",
    "\n",
    "                # PETUNJUK PENGELOMPOKAN\n",
    "                - Jika issue memiliki kemiripan SUBSTANSIAL dengan kelompok yang sudah ada → masukkan ke kelompok tersebut\n",
    "                - Jika issue SAMA SEKALI TIDAK TERKAIT dengan kelompok yang ada → buat kelompok baru\n",
    "                - Kemiripan didasarkan pada tema, topik, kata kunci, dan konteks (bukan sentimen)\n",
    "                - Usahakan untuk TIDAK membuat kelompok baru jika masih bisa dimasukkan ke kelompok yang sudah ada\n",
    "\n",
    "                # FORMAT OUTPUT (JSON)\n",
    "                [\n",
    "                  {{\n",
    "                    \"unified_issue\": \"Nama Issue Kelompok (gunakan dari list yang sudah ada atau buat baru)\",\n",
    "                    \"description\": \"Deskripsi singkat dan jelas tentang tema kelompok ini\",\n",
    "                    \"list_issue_id\": [1, 5, 10, 15] // Daftar ID issue yang masuk dalam kelompok ini\n",
    "                  }},\n",
    "                  ...\n",
    "                ]\n",
    "\n",
    "                # KRITERIA KUALITAS\n",
    "                - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "                - Nama kelompok baru harus singkat, jelas, dan deskriptif\n",
    "                - Prioritaskan penggunaan nama kelompok yang sudah ada\n",
    "                - Pastikan seluruh index issue masuk ke setiap kelompok tanpa tersisia\n",
    "\n",
    "                PENTING: Hasilkan HANYA format JSON murni tanpa komentar, penjelasan, atau notasi lain.\"\"\"\n",
    "\n",
    "            predict = call_gemini(prompt)\n",
    "            df_predict = pd.DataFrame(eval(re.findall(r'\\[.*\\]',predict, flags=re.I|re.S)[0]))\n",
    "            all_result.append(df_predict)\n",
    "\n",
    "            LIST_UNIFIED_ISSUE.extend(df_predict['unified_issue'].to_list())\n",
    "\n",
    "            LIST_UNIFIED_ISSUE = list(set(LIST_UNIFIED_ISSUE))\n",
    "\n",
    "\n",
    "            SISA = set([i['id'] for i in issue_list])- set([j for i in df_predict['list_issue_id'] for j in i])\n",
    "\n",
    "            if not SISA:\n",
    "                break\n",
    "\n",
    "    all_result.append(df_central)\n",
    "    topics = pd.concat(all_result)\n",
    "\n",
    "    #merge dengan nama issue berdasarkan id\n",
    "    all_topics = topics.groupby('unified_issue').agg({'list_issue_id':[lambda s: [j for i in s for j in i], lambda s: len([j for i in s for j in i])]}).reset_index()\n",
    "    all_topics.columns = ['unified_issue','list_issue','total_issue']\n",
    "\n",
    "    merge_all_topics = []\n",
    "    for _,i in all_topics.iterrows():\n",
    "        data = df_issue[df_issue['index'].isin(i['list_issue'])]\n",
    "        dt = data[['total_posts','viral_score','reach_score','negative','positive','neutral']].sum().to_dict()\n",
    "        dt.update({'unified_issue':i['unified_issue'], 'list_issue':data['issue'].to_list()})\n",
    "\n",
    "        merge_all_topics.append(dt)\n",
    "\n",
    "    df_all_topics = pd.DataFrame(merge_all_topics).sort_values('viral_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "    df_all_topics['share_of_voice'] = df_all_topics['total_posts']/df_all_topics['total_posts'].sum()*100\n",
    "\n",
    "\n",
    "    #ambil importance post sebagai perwakilan\n",
    "    list_data = []\n",
    "    for _,i in df_all_topics.iterrows():\n",
    "        unified_issue = i['unified_issue']\n",
    "        list_issue = i['list_issue']\n",
    "        list_caption = df[df['issue'].isin(list_issue)].sort_values('viral_score',ascending = False).drop_duplicates('post_caption')[:5]['post_caption'].to_list()\n",
    "\n",
    "        list_data.append({'unified_issue':unified_issue,\n",
    "                                'list_caption':list_caption,\n",
    "                          'sentiment negative':i['negative'],\n",
    "                          'sentiment positive':i['positive']})\n",
    "\n",
    "    list_prediction = []\n",
    "    for unified_issues_list in tqdm(chunk_list(list_data, 40)):\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Kamu adalah Media Social Analyst Expert dengan keahlian dalam analisis kategori dan isu-isu sosial media.\n",
    "\n",
    "        # TUGAS\n",
    "        Buatkan deskripsi yang informatif, akurat, dan komprehensif untuk setiap kategori unified_issue berikut ini.\n",
    "\n",
    "        # PETUNJUK\n",
    "        1. Deskripsi harus menjelaskan dengan jelas apa yang termasuk dalam kategori tersebut\n",
    "        2. Deskripsi harus ringkas namun komprehensif (maksimal 2-3 kalimat)\n",
    "        3. Gunakan bahasa yang netral dan profesional\n",
    "        4. Fokus pada konten dan cakupan dari kategori isu tersebut\n",
    "        5. Jangan menyertakan opini pribadi atau bias dalam deskripsi\n",
    "        6. Berikan juga description berdasarkan sentimentnya\n",
    "\n",
    "        # DATA UNIFIED_ISSUE\n",
    "        ```\n",
    "        {unified_issues_list}\n",
    "        ```\n",
    "\n",
    "        # FORMAT OUTPUT\n",
    "        Berikan hasil dalam format JSON seperti berikut:\n",
    "        ```\n",
    "        [\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          {{\n",
    "            \"unified_issue\": \"Nama Unified Issue Lainnya\",\n",
    "            \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "          }},\n",
    "          ...\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        ### HARD RULE\n",
    "        - Berikan HANYA output JSON tanpa penjelasan atau komentar tambahan.\n",
    "        - Hindari penjelasan \"This category focuses\" , contoh description yang baik seperti ini \"Conversations praised recent innovations in the nickel industry that emphasize sustainability and local economic growth, boosting positive sentiment towards Prabowo by 18%\"\n",
    "        \"\"\"\n",
    "        response_text = call_gemini(prompt)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "        except:\n",
    "            response_text = (re.sub(f\"(?<!\\:\\s)\\\"([\\w\\s\\/\\.\\@\\-]+)\\\"(?![\\,\\:])\",r\"`\\1`\",response_text))\n",
    "            \n",
    "\n",
    "            try:\n",
    "                json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "            except:\n",
    "                json_string = []\n",
    "                for i in re.findall(r'{.*?}', response_text, flags=re.I|re.S):\n",
    "                    try:\n",
    "                        json_string.append(eval(i))\n",
    "                    except:\n",
    "                        pass  \n",
    "                    \n",
    "        \n",
    "        list_prediction.extend(json_string)\n",
    "\n",
    "\n",
    "    topics_result = pd.DataFrame(list_prediction).merge(df_all_topics, on = 'unified_issue')\n",
    "\n",
    "    return topics_result.sort_values('share_of_voice', ascending = False)\n",
    "\n",
    "def set_topics(keywords):\n",
    "\n",
    "    final_result = get_topics_overview(keywords)\n",
    "\n",
    "    result = ingest_to_elasticsearch(\n",
    "        data= [{'topics':final_result.to_dict(orient = 'records'),\n",
    "                'id':id_}],\n",
    "        hosts=\"http://localhost:9200\",\n",
    "        index=\"topics\",\n",
    "        id_field=\"id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0fc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:21:18.630534Z",
     "start_time": "2025-04-18T06:18:24.309333Z"
    },
    "code_folding": [
     30,
     138
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = ['prabowo']\n",
    "# Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "\n",
    "current_date = datetime.now()\n",
    "date_120_days_ago = current_date - timedelta(days=10)\n",
    "# Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "start_date = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#mendapatkan raw data\n",
    "df = pd.DataFrame(get_relevan_data(keywords,start_date))\n",
    "\n",
    "#mendapatkan dataframe aggregate per issue\n",
    "df_issue = get_df_issue(df)\n",
    "\n",
    "#chunk df_issue\n",
    "data_chunk = chunk_dataframe(df_issue, 100)\n",
    "\n",
    "#mendapatkan central issue berdasarkan viral score tertinggi\n",
    "df_central = get_central_issue(data_chunk[0])\n",
    "\n",
    "LIST_UNIFIED_ISSUE = df_central['unified_issue'].to_list()\n",
    "\n",
    "SISA = data_chunk[0][~data_chunk[0]['index'].isin([j for i in df_central['list_issue_id'] for j in i])]\n",
    "if not SISA.empty:\n",
    "    data_chunk[1] = pd.concat([data_chunk[1], SISA])\n",
    "\n",
    "\n",
    "\n",
    "#mendapatkan seluruh unified issue \n",
    "all_result = []\n",
    "for DC in tqdm(data_chunk[1:]):\n",
    "    issue_list = []\n",
    "    for idx, row in DC.iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "\n",
    "    df_predict = ''\n",
    "\n",
    "    while True:\n",
    "        if type(df_predict)!=str:\n",
    "            issue_list = [i for i in issue_list if i['id'] not in [j for i in df_predict['list_issue_id'] for j in i]]\n",
    "        # Buat prompt yang lebih terstruktur dan eksplisit\n",
    "        prompt = f\"\"\"\n",
    "            Kamu adalah Media Social Analyst Expert dengan keahlian dalam pengelompokan tematik dan kategorisasi konten sosial media.\n",
    "\n",
    "            # TUGAS UTAMA\n",
    "            Analisis dan kelompokkan list issue baru ke dalam kelompok tematik yang sudah ada (jika relevan) atau buat kelompok baru jika diperlukan.\n",
    "\n",
    "            # ATURAN PENGELOMPOKAN\n",
    "            1. PENTING: Prioritaskan pengelompokan ke dalam kategori yang sudah ada jika terdapat kemiripan tema.\n",
    "            2. Buat kategori baru HANYA jika issue tidak cocok dengan kelompok yang sudah ada.\n",
    "            3. Setiap issue ID HANYA boleh masuk ke SATU kelompok (mutually exclusive).\n",
    "            4. Hindari TUMPANG TINDIH issue di antara kelompok-kelompok.\n",
    "\n",
    "            # KELOMPOK YANG SUDAH ADA\n",
    "            Berikut adalah kelompok tematik yang sudah ada dan HARUS digunakan jika relevan:\n",
    "            ```\n",
    "            {LIST_UNIFIED_ISSUE}\n",
    "            ```\n",
    "\n",
    "            # LIST ISSUE YANG AKAN DIKELOMPOKKAN\n",
    "            ```\n",
    "            {issue_list}\n",
    "            ```\n",
    "\n",
    "            # PETUNJUK PENGELOMPOKAN\n",
    "            - Jika issue memiliki kemiripan SUBSTANSIAL dengan kelompok yang sudah ada → masukkan ke kelompok tersebut\n",
    "            - Jika issue SAMA SEKALI TIDAK TERKAIT dengan kelompok yang ada → buat kelompok baru\n",
    "            - Kemiripan didasarkan pada tema, topik, kata kunci, dan konteks (bukan sentimen)\n",
    "            - Usahakan untuk TIDAK membuat kelompok baru jika masih bisa dimasukkan ke kelompok yang sudah ada\n",
    "\n",
    "            # FORMAT OUTPUT (JSON)\n",
    "            [\n",
    "              {{\n",
    "                \"unified_issue\": \"Nama Issue Kelompok (gunakan dari list yang sudah ada atau buat baru)\",\n",
    "                \"description\": \"Deskripsi singkat dan jelas tentang tema kelompok ini\",\n",
    "                \"list_issue_id\": [1, 5, 10, 15] // Daftar ID issue yang masuk dalam kelompok ini\n",
    "              }},\n",
    "              ...\n",
    "            ]\n",
    "\n",
    "            # KRITERIA KUALITAS\n",
    "            - Setiap kelompok sebaiknya memiliki minimal 2 issue\n",
    "            - Nama kelompok baru harus singkat, jelas, dan deskriptif\n",
    "            - Prioritaskan penggunaan nama kelompok yang sudah ada\n",
    "            - Pastikan seluruh index issue masuk ke setiap kelompok tanpa tersisia\n",
    "\n",
    "            PENTING: Hasilkan HANYA format JSON murni tanpa komentar, penjelasan, atau notasi lain.\"\"\"\n",
    "\n",
    "        predict = call_gemini(prompt)\n",
    "        df_predict = pd.DataFrame(eval(re.findall(r'\\[.*\\]',predict, flags=re.I|re.S)[0]))\n",
    "        all_result.append(df_predict)\n",
    "\n",
    "        LIST_UNIFIED_ISSUE.extend(df_predict['unified_issue'].to_list())\n",
    "\n",
    "        LIST_UNIFIED_ISSUE = list(set(LIST_UNIFIED_ISSUE))\n",
    "\n",
    "\n",
    "        SISA = set([i['id'] for i in issue_list])- set([j for i in df_predict['list_issue_id'] for j in i])\n",
    "\n",
    "        if not SISA:\n",
    "            break\n",
    "\n",
    "all_result.append(df_central)\n",
    "topics = pd.concat(all_result)\n",
    "\n",
    "#merge dengan nama issue berdasarkan id\n",
    "all_topics = topics.groupby('unified_issue').agg({'list_issue_id':[lambda s: [j for i in s for j in i], lambda s: len([j for i in s for j in i])]}).reset_index()\n",
    "all_topics.columns = ['unified_issue','list_issue','total_issue']\n",
    "\n",
    "merge_all_topics = []\n",
    "for _,i in all_topics.iterrows():\n",
    "    data = df_issue[df_issue['index'].isin(i['list_issue'])]\n",
    "    dt = data[['total_posts','viral_score','reach_score','negative','positive','neutral']].sum().to_dict()\n",
    "    dt.update({'unified_issue':i['unified_issue'], 'list_issue':data['issue'].to_list()})\n",
    "\n",
    "    merge_all_topics.append(dt)\n",
    "\n",
    "df_all_topics = pd.DataFrame(merge_all_topics).sort_values('viral_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "df_all_topics['share_of_voice'] = df_all_topics['total_posts']/df_all_topics['total_posts'].sum()*100\n",
    "\n",
    "\n",
    "#ambil importance post sebagai perwakilan\n",
    "list_data = []\n",
    "for _,i in df_all_topics.iterrows():\n",
    "    unified_issue = i['unified_issue']\n",
    "    list_issue = i['list_issue']\n",
    "    list_caption = df[df['issue'].isin(list_issue)].sort_values('viral_score',ascending = False).drop_duplicates('post_caption')[:5]['post_caption'].to_list()\n",
    "\n",
    "    list_data.append({'unified_issue':unified_issue,\n",
    "                            'list_caption':list_caption,\n",
    "                      'sentiment negative':i['negative'],\n",
    "                      'sentiment positive':i['positive']})\n",
    "\n",
    "list_prediction = []\n",
    "for unified_issues_list in tqdm(chunk_list(list_data, 40)):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Kamu adalah Media Social Analyst Expert dengan keahlian dalam analisis kategori dan isu-isu sosial media.\n",
    "\n",
    "    # TUGAS\n",
    "    Buatkan deskripsi yang informatif, akurat, dan komprehensif untuk setiap kategori unified_issue berikut ini.\n",
    "\n",
    "    # PETUNJUK\n",
    "    1. Deskripsi harus menjelaskan dengan jelas apa yang termasuk dalam kategori tersebut\n",
    "    2. Deskripsi harus ringkas namun komprehensif (maksimal 2-3 kalimat)\n",
    "    3. Gunakan bahasa yang netral dan profesional\n",
    "    4. Fokus pada konten dan cakupan dari kategori isu tersebut\n",
    "    5. Jangan menyertakan opini pribadi atau bias dalam deskripsi\n",
    "    6. Berikan juga description berdasarkan sentimentnya\n",
    "\n",
    "    # DATA UNIFIED_ISSUE\n",
    "    ```\n",
    "    {unified_issues_list}\n",
    "    ```\n",
    "\n",
    "    # FORMAT OUTPUT\n",
    "    Berikan hasil dalam format JSON seperti berikut:\n",
    "    ```\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Unified Issue\",\n",
    "        \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama Unified Issue Lainnya\",\n",
    "        \"description\": \"Deskripsi komprehensif tentang unified issue ini\"\n",
    "      }},\n",
    "      ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    ### HARD RULE\n",
    "    - Berikan HANYA output JSON tanpa penjelasan atau komentar tambahan.\n",
    "    - Hindari penjelasan \"This category focuses\" , contoh description yang baik seperti ini \"Conversations praised recent innovations in the nickel industry that emphasize sustainability and local economic growth, boosting positive sentiment towards Prabowo by 18%\"\n",
    "    \"\"\"\n",
    "    response_text = call_gemini(prompt)\n",
    "\n",
    "\n",
    "    try:\n",
    "        json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "    except:\n",
    "        response_text = (re.sub(f\"(?<!\\:\\s)\\\"([\\w\\s\\/\\.\\@\\-]+)\\\"(?![\\,\\:])\",r\"`\\1`\",response_text))\n",
    "\n",
    "\n",
    "        try:\n",
    "            json_string = eval(re.findall(r'\\[.*\\]', response_text, flags = re.I|re.S)[0])\n",
    "        except:\n",
    "            json_string = []\n",
    "            for i in re.findall(r'{.*?}', response_text, flags=re.I|re.S):\n",
    "                try:\n",
    "                    json_string.append(eval(i))\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "\n",
    "    list_prediction.extend(json_string)\n",
    "\n",
    "\n",
    "topics_result = pd.DataFrame(list_prediction).merge(df_all_topics, on = 'unified_issue')\n",
    "topics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d7b77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:25:39.218232Z",
     "start_time": "2025-04-18T06:25:39.146000Z"
    }
   },
   "outputs": [],
   "source": [
    "topics_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da09d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:24:38.438252Z",
     "start_time": "2025-04-18T06:24:38.425667Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(hasil).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d11152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccef6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:27:51.272549Z",
     "start_time": "2025-04-18T06:27:51.260547Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_relevan_data(keywords,start_date=None, size = 10000 ):\n",
    "    current_date = datetime.now()\n",
    "    if not start_date:\n",
    "        # Hitung tanggal 120 hari ke belakang dari sekarang\n",
    "        \n",
    "        date_120_days_ago = current_date - timedelta(days=120)\n",
    "        # Format tanggal ke format ISO 8601 yang kompatibel dengan Elasticsearch\n",
    "        start_date = date_120_days_ago.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    es_helper = ElasticsearchHelper(host=\"http://localhost:9200\")\n",
    "    query = {\n",
    "        \"_source\": [\"issue\", \"post_caption\", \"reach_score\", \"viral_score\", \"sentiment\", \"link_post\", \"channel\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                                \"query\": keyword,\n",
    "                                \"operator\": \"and\"  # Semua kata dalam keyword harus ada\n",
    "                            }\n",
    "                        }\n",
    "                    } for keyword in keywords\n",
    "                ],\n",
    "              \"must\": [\n",
    "                            {\n",
    "                                \"range\": {\n",
    "                                    \"post_created_at\": {\n",
    "                                        \"gte\": start_date,  # Greater than or equal to 120 hari yang lalu\n",
    "                                        \"lte\": current_date.strftime(\"%Y-%m-%d %H:%M:%S\")  # Less than or equal to sekarang\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                \"must_not\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"issue\": \"Not Specified\"  # Filter: 'issue' tidak boleh \"Not Specified\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1  # Minimal satu keyword yang cocok\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return es_helper.fetch_data(index=\"twitter_data,linkedin_data,news_data,reddit_data,youtube_data\",\n",
    "                                query=query, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93191139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:28:17.993980Z",
     "start_time": "2025-04-18T06:28:14.674935Z"
    }
   },
   "outputs": [],
   "source": [
    "data = get_relevan_data(['ruu tni'],size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb90ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:28:34.445211Z",
     "start_time": "2025-04-18T06:28:34.434698Z"
    }
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e194c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa35e610",
   "metadata": {},
   "source": [
    "# KOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8861555f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:34:04.155382Z",
     "start_time": "2025-05-11T13:33:52.468936Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to http://34.101.178.71:9200\n"
     ]
    }
   ],
   "source": [
    "from utils.es_client import get_elasticsearch_client\n",
    "from utils.list_of_mentions import get_mentions\n",
    "import pandas as pd\n",
    "import uuid, numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Buat koneksi ke Elasticsearch\n",
    "es = get_elasticsearch_client()\n",
    "\n",
    "def create_link_user(df):\n",
    "    if df['channel'] == 'twitter':\n",
    "        return f\"\"\"https://x.com/{df['username'].strip('@ ')}\"\"\"\n",
    "    if df['channel'] == 'instagram':\n",
    "        return f\"\"\"https://www.instagram.com/{df['username'].strip('@ ')}\"\"\"\n",
    "    if df['channel'] == 'tiktok':\n",
    "        return f\"\"\"https://www.tiktok.com/@{df['username'].strip('@ ')}\"\"\"\n",
    "    if df['channel']=='linkedin':\n",
    "        return f\"\"\"https://www.linkedin.com/in/{df['username'].strip('@ ')}\"\"\"\n",
    "    if df['channel']=='reddit':\n",
    "        return f\"\"\"https://www.reddit.com/{df['username'].strip('@ ')}\"\"\"\n",
    "    \n",
    "    \n",
    "    return df['username']\n",
    "    \n",
    "def add_negative_driver_flag(df):\n",
    "    \"\"\"\n",
    "    Add is_negative_driver column that is True when sentiment_negative \n",
    "    is the dominant sentiment\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sentiment_positive, sentiment_negative, and sentiment_neutral columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added is_negative_driver column\n",
    "    \"\"\"\n",
    "    # Ensure all sentiment columns exist\n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        col_name = f'sentiment_{sentiment}'\n",
    "        if col_name not in df.columns:\n",
    "            df[col_name] = 0\n",
    "    \n",
    "    # Create a new column that checks if negative sentiment is the highest\n",
    "    df['is_negative_driver'] = False\n",
    "    \n",
    "    # Compare sentiment counts and set flag if negative is highest\n",
    "    condition = ((df['sentiment_negative'] > df['sentiment_positive']) & \n",
    "                 (df['sentiment_negative'] > df['sentiment_neutral']))\n",
    "    \n",
    "    df['is_negative_driver'] = condition\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_uuid(keyword):\n",
    "    # Gunakan namespace standar (ada juga untuk URL, DNS, dll)\n",
    "    namespace = uuid.NAMESPACE_DNS\n",
    "\n",
    "    return uuid.uuid5(namespace, keyword)\n",
    "\n",
    "def search_kol(   owner_id = None,\n",
    "    project_name = None,\n",
    "    es_host=None,\n",
    "    es_username=None,\n",
    "    es_password=None,\n",
    "    use_ssl=False,\n",
    "    verify_certs=False,\n",
    "    ca_certs=None,\n",
    "    keywords=None,\n",
    "    search_exact_phrases=False,\n",
    "    case_sensitive=False,\n",
    "    sentiment=None,\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    date_filter=\"last 30 days\",\n",
    "    custom_start_date=None,\n",
    "    custom_end_date=None,\n",
    "    channels=None,\n",
    "    importance=\"all mentions\",\n",
    "    influence_score_min=None,\n",
    "    influence_score_max=None,\n",
    "    region=None,\n",
    "    language=None,\n",
    "    domain=None):\n",
    "\n",
    "    print('get all data mentions')\n",
    "\n",
    "    if not sentiment:\n",
    "        sentiment = ['positive','negative','neutral']\n",
    "\n",
    "    result = get_mentions(\n",
    "            source= [\"issue\",\"user_connections\",\"user_followers\",\"user_influence_score\",\n",
    "                     'user_image_url',\"engagement_rate\",\n",
    "                 \"influence_score\",\"reach_score\", \"viral_score\",\n",
    "                 \"sentiment\", \"link_post\",\"user_category\",\"username\",'channel'],\n",
    "            page_size=10000,\n",
    "            es_host=es_host,    \n",
    "            es_username=es_username,\n",
    "            es_password=es_password,\n",
    "            use_ssl=use_ssl,\n",
    "            verify_certs=verify_certs,\n",
    "            ca_certs=ca_certs,\n",
    "            keywords=keywords,\n",
    "            search_exact_phrases=search_exact_phrases,\n",
    "            case_sensitive=case_sensitive,\n",
    "            sentiment=sentiment,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            date_filter=date_filter,\n",
    "            custom_start_date=custom_start_date,\n",
    "            custom_end_date=custom_end_date,\n",
    "            channels=channels,\n",
    "            importance=importance,\n",
    "            influence_score_min=influence_score_min,\n",
    "            influence_score_max=influence_score_max,\n",
    "            region=region,\n",
    "            language=language,\n",
    "            domain=domain,\n",
    "            sort_type = 'popular'\n",
    "        )\n",
    "    \n",
    "\n",
    "    if not result['data']:\n",
    "        return []\n",
    "    else:\n",
    "        kol = pd.DataFrame(result['data'])\n",
    "        print(kol.shape)\n",
    " \n",
    "        print('set metrics')\n",
    "        for i in set(['user_influence_score','user_followers']) - set(kol):\n",
    "            kol[i] = 0\n",
    "\n",
    "        if 'user_category' not in kol:\n",
    "            kol['user_category'] = 'News Account'\n",
    "\n",
    "        kol[['user_influence_score','user_followers']] = kol[['user_influence_score','user_followers']].fillna(0)\n",
    "        kol['user_category'] = kol['user_category'].fillna('')\n",
    "\n",
    "        kol['link_user'] = kol.apply(lambda s: create_link_user(s), axis=1)        \n",
    "\n",
    "        for i in set(['user_connections','user_followers']) - set(kol):\n",
    "            kol[i] = 0\n",
    "\n",
    "        kol['user_followers'] = kol['user_connections']+kol['user_followers']\n",
    "\n",
    "        # Your groupby with sentiment pivot\n",
    "        agg_kol = kol.groupby(['link_user']).agg({\n",
    "            'link_post': 'size',\n",
    "            'viral_score': 'sum',\n",
    "            'reach_score': 'sum',\n",
    "            'channel': 'max',\n",
    "            'username': 'max',\n",
    "            'user_image_url':'max',\n",
    "            'user_followers':'max',\n",
    "            \"engagement_rate\":'sum',\n",
    "            'issue': lambda s: list(set(s)),\n",
    "            'user_category': 'max',\n",
    "            'user_influence_score': lambda s: max(s)*100\n",
    "        })\n",
    "\n",
    "        # Get sentiment counts per link_user using crosstab\n",
    "        sentiment_counts = pd.crosstab(kol['link_user'], kol['sentiment'])\n",
    "\n",
    "        # Rename columns to add 'sentiment_' prefix\n",
    "        sentiment_counts = sentiment_counts.add_prefix('sentiment_')\n",
    "\n",
    "        # Join the sentiment counts with the main result\n",
    "        final_kol = agg_kol.join(sentiment_counts)\n",
    "\n",
    "        # If any sentiment category is missing, add it with zeros\n",
    "        for sentiment in ['positive', 'negative', 'neutral']:\n",
    "            col_name = f'sentiment_{sentiment}'\n",
    "            if col_name not in final_kol.columns:\n",
    "                final_kol[col_name] = 0        \n",
    "\n",
    "\n",
    "        # Apply the function to final_kol\n",
    "        final_kol = add_negative_driver_flag(final_kol).reset_index()\n",
    "\n",
    "        list_issue = [j for i in final_kol['issue'] for j in i]\n",
    "\n",
    "        print('check topic map')\n",
    "        #check issue mana yang sudah termap dan mana yg belum\n",
    "        query_body = {\n",
    "            \"_source\":[\"unified_issue\",\"list_issue\"],\n",
    "            \"query\":{\n",
    "\n",
    "                \"bool\": {\n",
    "\n",
    "                    \"must\":[\n",
    "\n",
    "                        {        \"match\": {\"project_name.keyword\":project_name}   },\n",
    "                        {        \"terms\": {\"list_issue.keyword\":list_issue}  },\n",
    "\n",
    "\n",
    "                    ]\n",
    "\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = es.search(\n",
    "            index=\"topic_cluster\",\n",
    "            body=query_body,\n",
    "            size = 10000\n",
    "        )\n",
    "        top_map = [i['_source'] for i in response['hits']['hits']]\n",
    "        dict_issue = {}\n",
    "        if top_map:\n",
    "            print('get map')\n",
    "            df_map = pd.DataFrame(top_map)\n",
    "            df_map = df_map.explode('list_issue').drop_duplicates('list_issue')\n",
    "            \n",
    "            for _,i in df_map.iterrows():\n",
    "                dict_issue.update({i['list_issue']:i['unified_issue']})\n",
    "\n",
    "        final_kol['unified_issue'] = final_kol['issue'].transform(lambda s: list(set([dict_issue.get(i,i) for i in s]))[:5])\n",
    "        final_kol['user_category'] = final_kol.apply(lambda s: 'News Account' if s['channel']=='news' else s['user_category'], axis=1)\n",
    "        final_kol.drop('issue', axis=1, inplace=True)\n",
    "        final_kol['most_viral'] = (final_kol['viral_score'] + final_kol['reach_score']) * \\\n",
    "                  np.log(final_kol['user_followers'] + 1.1) * np.log(final_kol['link_post'] + 1.1)* \\\n",
    "                  (final_kol['user_influence_score'] + 1.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        final_kol[\"share_of_voice\"] = final_kol[\"link_post\"]/final_kol[\"link_post\"].sum()*100\n",
    "        \n",
    "        most_negative_kol = final_kol.sort_values(['is_negative_driver','sentiment_negative','most_viral'], ascending = False)[:100]\n",
    "        most_viral_kol = final_kol.sort_values('most_viral', ascending = False)[:100]\n",
    "        \n",
    "        final_kol = pd.concat([most_negative_kol,most_viral_kol]).drop_duplicates('link_user')\n",
    "\n",
    "\n",
    "        return final_kol.sort_values(['is_negative_driver','sentiment_negative','most_viral'], ascending = False)[:150].to_dict(orient = 'records')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693a7dc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:35:01.742372Z",
     "start_time": "2025-05-11T13:35:01.449165Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all data mentions\n",
      "Successfully connected to http://34.101.178.71:9200\n",
      "(100, 13)\n",
      "set metrics\n",
      "check topic map\n",
      "get map\n"
     ]
    }
   ],
   "source": [
    "hasil = search_kol(keywords = ['danantara'],\n",
    "                   project_name='danantara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef7b3a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:35:12.059404Z",
     "start_time": "2025-05-11T13:35:12.034145Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_user</th>\n",
       "      <th>link_post</th>\n",
       "      <th>viral_score</th>\n",
       "      <th>reach_score</th>\n",
       "      <th>channel</th>\n",
       "      <th>username</th>\n",
       "      <th>user_image_url</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>engagement_rate</th>\n",
       "      <th>user_category</th>\n",
       "      <th>user_influence_score</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>is_negative_driver</th>\n",
       "      <th>unified_issue</th>\n",
       "      <th>most_viral</th>\n",
       "      <th>share_of_voice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/konde.co</td>\n",
       "      <td>1</td>\n",
       "      <td>0.955120</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>konde.co</td>\n",
       "      <td>https://scontent-cgk1-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>News Account</td>\n",
       "      <td>43.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Miscellaneous News and Events]</td>\n",
       "      <td>373.929434</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.tiktok.com/@iskandarfaisal07</td>\n",
       "      <td>1</td>\n",
       "      <td>0.645968</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@iskandarfaisal07</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.313253</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Underground Movement of Erroneous Regime]</td>\n",
       "      <td>3.161671</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.tiktok.com/@haidarakbar_</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555577</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@haidarakbar_</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.133948</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Journalists kicked out during President's spe...</td>\n",
       "      <td>3.154640</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.tiktok.com/@ahmadfannyr</td>\n",
       "      <td>1</td>\n",
       "      <td>0.309560</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@ahmadfannyr</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.982414</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Reporter asked not to cover Pak President's s...</td>\n",
       "      <td>3.135503</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.tiktok.com/@arisvaraa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201219</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@arisvaraa</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.706397</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Former corrupt fugitive after 15 years]</td>\n",
       "      <td>3.127076</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>@tvOneNews</td>\n",
       "      <td>1</td>\n",
       "      <td>3.113684</td>\n",
       "      <td>0.070243</td>\n",
       "      <td>youtube</td>\n",
       "      <td>@tvOneNews</td>\n",
       "      <td>https://i.ytimg.com/vi/jC8kdWThglM/hqdefault.j...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.820513</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Breaking news from tvOne]</td>\n",
       "      <td>0.247664</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>@OfficialSINDOnews</td>\n",
       "      <td>1</td>\n",
       "      <td>2.406460</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>youtube</td>\n",
       "      <td>@OfficialSINDOnews</td>\n",
       "      <td>https://i.ytimg.com/an/GfXjFgIUUTUUlzdG6BenXA/...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Minister Airlangga discusses food security]</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://www.tiktok.com/@satriakalteng</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053457</td>\n",
       "      <td>2.322917</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@satriakalteng</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[Danantara, a magnet for foreign investors]</td>\n",
       "      <td>0.184848</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>https://www.tiktok.com/@andarai917</td>\n",
       "      <td>1</td>\n",
       "      <td>0.329089</td>\n",
       "      <td>1.995000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@andarai917</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.666667</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[Government and State-Owned Enterprises (SOEs)...</td>\n",
       "      <td>0.180781</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>@IDXChannel</td>\n",
       "      <td>1</td>\n",
       "      <td>1.485334</td>\n",
       "      <td>0.836437</td>\n",
       "      <td>youtube</td>\n",
       "      <td>@IDXChannel</td>\n",
       "      <td>https://i.ytimg.com/vi/iTNU6ytE9zA/hqdefault.j...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.169811</td>\n",
       "      <td></td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Airlangga Hartarto discusses economic growth]</td>\n",
       "      <td>0.180600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   link_user  link_post  viral_score  \\\n",
       "0         https://www.instagram.com/konde.co          1     0.955120   \n",
       "1   https://www.tiktok.com/@iskandarfaisal07          1     0.645968   \n",
       "2       https://www.tiktok.com/@haidarakbar_          1     0.555577   \n",
       "3        https://www.tiktok.com/@ahmadfannyr          1     0.309560   \n",
       "4          https://www.tiktok.com/@arisvaraa          1     0.201219   \n",
       "..                                       ...        ...          ...   \n",
       "66                                @tvOneNews          1     3.113684   \n",
       "67                        @OfficialSINDOnews          1     2.406460   \n",
       "68     https://www.tiktok.com/@satriakalteng          1     0.053457   \n",
       "69        https://www.tiktok.com/@andarai917          1     0.329089   \n",
       "70                               @IDXChannel          1     1.485334   \n",
       "\n",
       "    reach_score    channel            username  \\\n",
       "0      0.185000  instagram            konde.co   \n",
       "1     40.000000     tiktok   @iskandarfaisal07   \n",
       "2     40.000000     tiktok       @haidarakbar_   \n",
       "3     40.000000     tiktok        @ahmadfannyr   \n",
       "4     40.000000     tiktok          @arisvaraa   \n",
       "..          ...        ...                 ...   \n",
       "66     0.070243    youtube          @tvOneNews   \n",
       "67     0.004013    youtube  @OfficialSINDOnews   \n",
       "68     2.322917     tiktok      @satriakalteng   \n",
       "69     1.995000     tiktok         @andarai917   \n",
       "70     0.836437    youtube         @IDXChannel   \n",
       "\n",
       "                                       user_image_url  user_followers  \\\n",
       "0   https://scontent-cgk1-2.cdninstagram.com/v/t51...         19500.0   \n",
       "1   https://p16-sign-va.tiktokcdn.com/tos-maliva-a...             0.0   \n",
       "2   https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...             0.0   \n",
       "3   https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...             0.0   \n",
       "4   https://p16-sign-va.tiktokcdn.com/tos-maliva-a...             0.0   \n",
       "..                                                ...             ...   \n",
       "66  https://i.ytimg.com/vi/jC8kdWThglM/hqdefault.j...             0.0   \n",
       "67  https://i.ytimg.com/an/GfXjFgIUUTUUlzdG6BenXA/...             0.0   \n",
       "68  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...             0.0   \n",
       "69  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...             0.0   \n",
       "70  https://i.ytimg.com/vi/iTNU6ytE9zA/hqdefault.j...             0.0   \n",
       "\n",
       "    engagement_rate user_category  user_influence_score  sentiment_negative  \\\n",
       "0        100.000000  News Account                 43.65                   1   \n",
       "1        128.313253                                0.00                   1   \n",
       "2        135.133948                                0.00                   1   \n",
       "3        121.982414                                0.00                   1   \n",
       "4        116.706397                                0.00                   1   \n",
       "..              ...           ...                   ...                 ...   \n",
       "66       212.820513                                0.00                   0   \n",
       "67       175.000000                                0.00                   0   \n",
       "68       100.000000                                0.00                   0   \n",
       "69       116.666667                                0.00                   0   \n",
       "70       147.169811                                0.00                   0   \n",
       "\n",
       "    sentiment_neutral  sentiment_positive  is_negative_driver  \\\n",
       "0                   0                   0                True   \n",
       "1                   0                   0                True   \n",
       "2                   0                   0                True   \n",
       "3                   0                   0                True   \n",
       "4                   0                   0                True   \n",
       "..                ...                 ...                 ...   \n",
       "66                  1                   0               False   \n",
       "67                  1                   0               False   \n",
       "68                  0                   1               False   \n",
       "69                  0                   1               False   \n",
       "70                  1                   0               False   \n",
       "\n",
       "                                        unified_issue  most_viral  \\\n",
       "0                     [Miscellaneous News and Events]  373.929434   \n",
       "1          [Underground Movement of Erroneous Regime]    3.161671   \n",
       "2   [Journalists kicked out during President's spe...    3.154640   \n",
       "3   [Reporter asked not to cover Pak President's s...    3.135503   \n",
       "4            [Former corrupt fugitive after 15 years]    3.127076   \n",
       "..                                                ...         ...   \n",
       "66                         [Breaking news from tvOne]    0.247664   \n",
       "67       [Minister Airlangga discusses food security]    0.187500   \n",
       "68        [Danantara, a magnet for foreign investors]    0.184848   \n",
       "69  [Government and State-Owned Enterprises (SOEs)...    0.180781   \n",
       "70     [Airlangga Hartarto discusses economic growth]    0.180600   \n",
       "\n",
       "    share_of_voice  \n",
       "0              1.0  \n",
       "1              1.0  \n",
       "2              1.0  \n",
       "3              1.0  \n",
       "4              1.0  \n",
       "..             ...  \n",
       "66             1.0  \n",
       "67             1.0  \n",
       "68             1.0  \n",
       "69             1.0  \n",
       "70             1.0  \n",
       "\n",
       "[71 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hasil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db21cf8",
   "metadata": {},
   "source": [
    "# kol -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0de6857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:03:25.346707Z",
     "start_time": "2025-05-11T14:03:20.423898Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get all data mentions\n",
      "Successfully connected to http://34.101.178.71:9200\n",
      "(2258, 20)\n",
      "set metrics\n",
      "check topic map\n",
      "get map\n"
     ]
    }
   ],
   "source": [
    "print('get all data mentions')\n",
    "\n",
    "project_name = 'danantara'\n",
    "sentiment = ['positive','negative','neutral']\n",
    "\n",
    "result = get_mentions(\n",
    "        source= [\"issue\",\"user_connections\",\"user_followers\",\"user_influence_score\",\n",
    "                 'user_image_url',\"engagement_rate\",\n",
    "                 \"influence_score\",\"reach_score\", \"viral_score\",\n",
    "                 \"sentiment\", \"link_post\",\"user_category\",\"username\",'channel',\n",
    "                \"votes\",\"likes\",'comments','shares','retweets','reports','replies',\n",
    "                 'views','favorites'\n",
    "                ],\n",
    "        page_size=10000,\n",
    "        keywords=['danantara','prabowo'],\n",
    "\n",
    "        sort_type = 'popular'\n",
    "    )\n",
    "\n",
    "\n",
    "kol = pd.DataFrame(result['data'])\n",
    "print(kol.shape)\n",
    "\n",
    "print('set metrics')\n",
    "for i in set(['user_influence_score','user_followers']) - set(kol):\n",
    "    kol[i] = 0\n",
    "\n",
    "    \n",
    "#-----------------------\n",
    "    \n",
    "def rule_base_user_category(string, category):\n",
    "    if pd.isna(string):\n",
    "        return ''\n",
    "    \n",
    "    if 'news' in string.lower():\n",
    "        return 'News Account'\n",
    "\n",
    "    return category\n",
    "    \n",
    "    \n",
    "kol['user_category'] = kol.apply(lambda s: 'News Account' if s['channel'] == 'news' else rule_base_user_category(s['username'], s['user_category']), axis=1)\n",
    "\n",
    "\n",
    "from utils.influence_score import get_influence_score\n",
    "kol['user_influence_score'] = kol.apply(lambda s: get_influence_score(s), axis=1)\n",
    "\n",
    "#-----------------------                                \n",
    "                                 \n",
    "                                 \n",
    "\n",
    "kol[['user_influence_score','user_followers']] = kol[['user_influence_score','user_followers']].fillna(0)\n",
    "kol['user_category'] = kol['user_category'].fillna('')\n",
    "\n",
    "kol['link_user'] = kol.apply(lambda s: create_link_user(s), axis=1)        \n",
    "\n",
    "for i in set(['user_connections','user_followers']) - set(kol):\n",
    "    kol[i] = 0\n",
    "\n",
    "kol['user_followers'] = kol['user_connections']+kol['user_followers']\n",
    "\n",
    "# Your groupby with sentiment pivot\n",
    "agg_kol = kol.groupby(['link_user']).agg({\n",
    "    'link_post': 'size',\n",
    "    'viral_score': 'sum',\n",
    "    'reach_score': 'sum',\n",
    "    'channel': 'max',\n",
    "    'username': 'max',\n",
    "    'user_image_url':'max',\n",
    "    'user_followers':'max',\n",
    "    \"engagement_rate\":'sum',\n",
    "    'issue': lambda s: list(set(s)),\n",
    "    'user_category': 'max',\n",
    "    'user_influence_score': 'mean'\n",
    "})\n",
    "\n",
    "# Get sentiment counts per link_user using crosstab\n",
    "sentiment_counts = pd.crosstab(kol['link_user'], kol['sentiment'])\n",
    "\n",
    "# Rename columns to add 'sentiment_' prefix\n",
    "sentiment_counts = sentiment_counts.add_prefix('sentiment_')\n",
    "\n",
    "# Join the sentiment counts with the main result\n",
    "final_kol = agg_kol.join(sentiment_counts)\n",
    "\n",
    "# If any sentiment category is missing, add it with zeros\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    col_name = f'sentiment_{sentiment}'\n",
    "    if col_name not in final_kol.columns:\n",
    "        final_kol[col_name] = 0        \n",
    "\n",
    "\n",
    "# Apply the function to final_kol\n",
    "final_kol = add_negative_driver_flag(final_kol).reset_index()\n",
    "\n",
    "list_issue = [j for i in final_kol['issue'] for j in i]\n",
    "\n",
    "print('check topic map')\n",
    "#check issue mana yang sudah termap dan mana yg belum\n",
    "query_body = {\n",
    "    \"_source\":[\"unified_issue\",\"list_issue\"],\n",
    "    \"query\":{\n",
    "\n",
    "        \"bool\": {\n",
    "\n",
    "            \"must\":[\n",
    "\n",
    "                {        \"match\": {\"project_name.keyword\":project_name}   },\n",
    "                {        \"terms\": {\"list_issue.keyword\":list_issue}  },\n",
    "\n",
    "\n",
    "            ]\n",
    "\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(\n",
    "    index=\"topic_cluster\",\n",
    "    body=query_body,\n",
    "    size = 10000\n",
    ")\n",
    "top_map = [i['_source'] for i in response['hits']['hits']]\n",
    "dict_issue = {}\n",
    "if top_map:\n",
    "    print('get map')\n",
    "    df_map = pd.DataFrame(top_map)\n",
    "    df_map = df_map.explode('list_issue').drop_duplicates('list_issue')\n",
    "\n",
    "    for _,i in df_map.iterrows():\n",
    "        dict_issue.update({i['list_issue']:i['unified_issue']})\n",
    "\n",
    "final_kol['unified_issue'] = final_kol['issue'].transform(lambda s: list(set([dict_issue.get(i,i) for i in s]))[:5])\n",
    "final_kol['user_category'] = final_kol.apply(lambda s: 'News Account' if s['channel']=='news' else s['user_category'], axis=1)\n",
    "final_kol.drop('issue', axis=1, inplace=True)\n",
    "final_kol['most_viral'] = final_kol['user_influence_score']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "final_kol[\"share_of_voice\"] = (final_kol[\"link_post\"]/final_kol[\"link_post\"].sum())*100\n",
    "\n",
    "most_negative_kol = final_kol.sort_values(['is_negative_driver','sentiment_negative','most_viral'], ascending = False)[:100]\n",
    "most_viral_kol = final_kol.sort_values('most_viral', ascending = False)[:100]\n",
    "\n",
    "final_kol = pd.concat([most_negative_kol,most_viral_kol]).drop_duplicates('link_user')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b82c438",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:03:41.654379Z",
     "start_time": "2025-05-11T14:03:41.638529Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_user</th>\n",
       "      <th>link_post</th>\n",
       "      <th>viral_score</th>\n",
       "      <th>reach_score</th>\n",
       "      <th>channel</th>\n",
       "      <th>username</th>\n",
       "      <th>user_image_url</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>engagement_rate</th>\n",
       "      <th>user_category</th>\n",
       "      <th>user_influence_score</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_sad</th>\n",
       "      <th>is_negative_driver</th>\n",
       "      <th>unified_issue</th>\n",
       "      <th>most_viral</th>\n",
       "      <th>share_of_voice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.instagram.com/berita_sumbar24jam</td>\n",
       "      <td>5</td>\n",
       "      <td>1.546796</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>berita_sumbar24jam</td>\n",
       "      <td>https://scontent-cgk2-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>546.857821</td>\n",
       "      <td>News Account</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Party on public road causing disturbance, See...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.221435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>https://www.instagram.com/btvidofficial</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.760000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>btvidofficial</td>\n",
       "      <td>https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>38500.0</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>News Account</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[National Harvest and food stock security]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>https://www.instagram.com/kabarterdepan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.195000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>kabarterdepan</td>\n",
       "      <td>https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>95.454545</td>\n",
       "      <td>News Account</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Prabowo on global situation, Gaza, Palestine]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>https://www.tiktok.com/@jejouw</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529764</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@jejouw</td>\n",
       "      <td>https://p16-sign-useast2a.tiktokcdn.com/tos-us...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.643012</td>\n",
       "      <td></td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Interview with Boby Kertanegara]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>https://www.instagram.com/virdindach_</td>\n",
       "      <td>1</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>15.330000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>virdindach_</td>\n",
       "      <td>https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>45100.0</td>\n",
       "      <td>16833.333333</td>\n",
       "      <td>Influencer</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Leaders reflect society's political awareness]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>https://www.tiktok.com/@rizal_real22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149363</td>\n",
       "      <td>8.115000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@rizal_real22</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>625.0</td>\n",
       "      <td>130.000000</td>\n",
       "      <td></td>\n",
       "      <td>2.420000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Rich Indonesians moving assets abroad]</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>https://www.tiktok.com/@faktacom</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095542</td>\n",
       "      <td>14.204286</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@faktacom</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>285400.0</td>\n",
       "      <td>132.258065</td>\n",
       "      <td></td>\n",
       "      <td>2.340000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[LG Energy Solution cancels investment]</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>https://www.tiktok.com/@chybtg</td>\n",
       "      <td>1</td>\n",
       "      <td>0.195610</td>\n",
       "      <td>9.640714</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@chybtg</td>\n",
       "      <td>https://p16-common-sign-sg.tiktokcdn-us.com/to...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.111111</td>\n",
       "      <td></td>\n",
       "      <td>2.330000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Forests diminish, indigenous people suffer]</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>0.044287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>https://www.tiktok.com/@savegaza.savepale</td>\n",
       "      <td>6</td>\n",
       "      <td>6.857835</td>\n",
       "      <td>114.336667</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@savegaza.savepale</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>7414.0</td>\n",
       "      <td>926.632322</td>\n",
       "      <td>Human</td>\n",
       "      <td>2.028333</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Prabowo and import issues, China cuts salarie...</td>\n",
       "      <td>2.028333</td>\n",
       "      <td>0.265722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>https://www.tiktok.com/@seruanhl</td>\n",
       "      <td>2</td>\n",
       "      <td>0.397234</td>\n",
       "      <td>5.480000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@seruanhl</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>79300.0</td>\n",
       "      <td>209.829060</td>\n",
       "      <td></td>\n",
       "      <td>1.255000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Highlighting increasing layoffs, Responding t...</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>0.088574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        link_user  link_post  viral_score  \\\n",
       "75   https://www.instagram.com/berita_sumbar24jam          5     1.546796   \n",
       "78        https://www.instagram.com/btvidofficial          1     0.000000   \n",
       "106       https://www.instagram.com/kabarterdepan          1     0.000000   \n",
       "532                https://www.tiktok.com/@jejouw          1     0.529764   \n",
       "144         https://www.instagram.com/virdindach_          1   503.000000   \n",
       "..                                            ...        ...          ...   \n",
       "811          https://www.tiktok.com/@rizal_real22          1     0.149363   \n",
       "406              https://www.tiktok.com/@faktacom          1     0.095542   \n",
       "335                https://www.tiktok.com/@chybtg          1     0.195610   \n",
       "845     https://www.tiktok.com/@savegaza.savepale          6     6.857835   \n",
       "852              https://www.tiktok.com/@seruanhl          2     0.397234   \n",
       "\n",
       "     reach_score    channel            username  \\\n",
       "75    200.000000  instagram  berita_sumbar24jam   \n",
       "78      9.760000  instagram       btvidofficial   \n",
       "106     6.195000  instagram       kabarterdepan   \n",
       "532    40.000000     tiktok             @jejouw   \n",
       "144    15.330000  instagram         virdindach_   \n",
       "..           ...        ...                 ...   \n",
       "811     8.115000     tiktok       @rizal_real22   \n",
       "406    14.204286     tiktok           @faktacom   \n",
       "335     9.640714     tiktok             @chybtg   \n",
       "845   114.336667     tiktok  @savegaza.savepale   \n",
       "852     5.480000     tiktok           @seruanhl   \n",
       "\n",
       "                                        user_image_url  user_followers  \\\n",
       "75   https://scontent-cgk2-1.cdninstagram.com/v/t51...         10000.0   \n",
       "78   https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....         38500.0   \n",
       "106  https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....        100000.0   \n",
       "532  https://p16-sign-useast2a.tiktokcdn.com/tos-us...             0.0   \n",
       "144  https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....         45100.0   \n",
       "..                                                 ...             ...   \n",
       "811  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...           625.0   \n",
       "406  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...        285400.0   \n",
       "335  https://p16-common-sign-sg.tiktokcdn-us.com/to...             0.0   \n",
       "845  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...          7414.0   \n",
       "852  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...         79300.0   \n",
       "\n",
       "     engagement_rate user_category  user_influence_score  sentiment_negative  \\\n",
       "75        546.857821  News Account             10.000000                   4   \n",
       "78         94.117647  News Account             10.000000                   0   \n",
       "106        95.454545  News Account             10.000000                   0   \n",
       "532       118.643012                           10.000000                   0   \n",
       "144     16833.333333    Influencer             10.000000                   0   \n",
       "..               ...           ...                   ...                 ...   \n",
       "811       130.000000                            2.420000                   1   \n",
       "406       132.258065                            2.340000                   1   \n",
       "335       111.111111                            2.330000                   1   \n",
       "845       926.632322         Human              2.028333                   4   \n",
       "852       209.829060                            1.255000                   2   \n",
       "\n",
       "     sentiment_neutral  sentiment_positive  sentiment_sad  is_negative_driver  \\\n",
       "75                   1                   0              0                True   \n",
       "78                   0                   1              0               False   \n",
       "106                  1                   0              0               False   \n",
       "532                  1                   0              0               False   \n",
       "144                  1                   0              0               False   \n",
       "..                 ...                 ...            ...                 ...   \n",
       "811                  0                   0              0                True   \n",
       "406                  0                   0              0                True   \n",
       "335                  0                   0              0                True   \n",
       "845                  2                   0              0                True   \n",
       "852                  0                   0              0                True   \n",
       "\n",
       "                                         unified_issue  most_viral  \\\n",
       "75   [Party on public road causing disturbance, See...   10.000000   \n",
       "78          [National Harvest and food stock security]   10.000000   \n",
       "106     [Prabowo on global situation, Gaza, Palestine]   10.000000   \n",
       "532                  [Interview with Boby Kertanegara]   10.000000   \n",
       "144    [Leaders reflect society's political awareness]   10.000000   \n",
       "..                                                 ...         ...   \n",
       "811            [Rich Indonesians moving assets abroad]    2.420000   \n",
       "406            [LG Energy Solution cancels investment]    2.340000   \n",
       "335       [Forests diminish, indigenous people suffer]    2.330000   \n",
       "845  [Prabowo and import issues, China cuts salarie...    2.028333   \n",
       "852  [Highlighting increasing layoffs, Responding t...    1.255000   \n",
       "\n",
       "     share_of_voice  \n",
       "75         0.221435  \n",
       "78         0.044287  \n",
       "106        0.044287  \n",
       "532        0.044287  \n",
       "144        0.044287  \n",
       "..              ...  \n",
       "811        0.044287  \n",
       "406        0.044287  \n",
       "335        0.044287  \n",
       "845        0.265722  \n",
       "852        0.088574  \n",
       "\n",
       "[183 rows x 19 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kol.sort_values('most_viral', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fca49753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T14:04:47.624857Z",
     "start_time": "2025-05-11T14:04:47.599553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_user</th>\n",
       "      <th>link_post</th>\n",
       "      <th>viral_score</th>\n",
       "      <th>reach_score</th>\n",
       "      <th>channel</th>\n",
       "      <th>username</th>\n",
       "      <th>user_image_url</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>engagement_rate</th>\n",
       "      <th>user_category</th>\n",
       "      <th>user_influence_score</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_sad</th>\n",
       "      <th>is_negative_driver</th>\n",
       "      <th>unified_issue</th>\n",
       "      <th>most_viral</th>\n",
       "      <th>share_of_voice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>https://www.instagram.com/berita_sumbar24jam</td>\n",
       "      <td>5</td>\n",
       "      <td>1.546796</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>berita_sumbar24jam</td>\n",
       "      <td>https://scontent-cgk2-1.cdninstagram.com/v/t51...</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>546.857821</td>\n",
       "      <td>News Account</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Party on public road causing disturbance, See...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.221435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>https://www.tiktok.com/@horror_nusantara</td>\n",
       "      <td>5</td>\n",
       "      <td>2.702433</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@horror_nusantara</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>80300.0</td>\n",
       "      <td>648.667176</td>\n",
       "      <td>Human</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Is Indonesia under threat?, China's impact on...</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>0.221435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>https://www.tiktok.com/@jeff_mahadika</td>\n",
       "      <td>6</td>\n",
       "      <td>1.803201</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@jeff_mahadika</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>81100.0</td>\n",
       "      <td>722.834845</td>\n",
       "      <td></td>\n",
       "      <td>5.721667</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Back 2 Allah, This is the source, Political c...</td>\n",
       "      <td>5.721667</td>\n",
       "      <td>0.265722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>https://www.tiktok.com/@wotos88</td>\n",
       "      <td>4</td>\n",
       "      <td>1.815973</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@wotos88</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>11800.0</td>\n",
       "      <td>498.752988</td>\n",
       "      <td></td>\n",
       "      <td>4.185000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[TNI Commander Insubordination?, Jokowi's bad ...</td>\n",
       "      <td>4.185000</td>\n",
       "      <td>0.177148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>https://www.tiktok.com/@savegaza.savepale</td>\n",
       "      <td>6</td>\n",
       "      <td>6.857835</td>\n",
       "      <td>114.336667</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@savegaza.savepale</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>7414.0</td>\n",
       "      <td>926.632322</td>\n",
       "      <td>Human</td>\n",
       "      <td>2.028333</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Prabowo and import issues, China cuts salarie...</td>\n",
       "      <td>2.028333</td>\n",
       "      <td>0.265722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>https://www.tiktok.com/@warganet.id_</td>\n",
       "      <td>3</td>\n",
       "      <td>1.430748</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@warganet.id_</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>206400.0</td>\n",
       "      <td>390.224329</td>\n",
       "      <td></td>\n",
       "      <td>5.816667</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Rejection of Prabowo-Gibran loyalists, Guru G...</td>\n",
       "      <td>5.816667</td>\n",
       "      <td>0.132861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>https://www.instagram.com/platform.news</td>\n",
       "      <td>2</td>\n",
       "      <td>155.637427</td>\n",
       "      <td>44.890000</td>\n",
       "      <td>instagram</td>\n",
       "      <td>platform.news</td>\n",
       "      <td>https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....</td>\n",
       "      <td>70100.0</td>\n",
       "      <td>5354.385965</td>\n",
       "      <td>News Account</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Demand to arrest Aguan CS, Bahlil's private j...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.088574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>https://www.tiktok.com/@jabarpetarung</td>\n",
       "      <td>4</td>\n",
       "      <td>0.944191</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@jabarpetarung</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>229100.0</td>\n",
       "      <td>464.099194</td>\n",
       "      <td></td>\n",
       "      <td>7.880000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Annoying Governor, KDM's promise to his mothe...</td>\n",
       "      <td>7.880000</td>\n",
       "      <td>0.177148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@tribunjogjaofc</td>\n",
       "      <td>2</td>\n",
       "      <td>6.846607</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>youtube</td>\n",
       "      <td>@tribunjogjaofc</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>455.136848</td>\n",
       "      <td></td>\n",
       "      <td>6.595000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Demands to remove Gibran, Prabowo reduces rol...</td>\n",
       "      <td>6.595000</td>\n",
       "      <td>0.088574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>https://www.tiktok.com/@sempak.mulyono.007</td>\n",
       "      <td>2</td>\n",
       "      <td>2.066953</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>@sempak.mulyono.007</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>17400.0</td>\n",
       "      <td>302.558215</td>\n",
       "      <td>Buzzer</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>[Oust Gibran, arrest Jokowi, Cleanse the palac...</td>\n",
       "      <td>5.405000</td>\n",
       "      <td>0.088574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        link_user  link_post  viral_score  \\\n",
       "75   https://www.instagram.com/berita_sumbar24jam          5     1.546796   \n",
       "462      https://www.tiktok.com/@horror_nusantara          5     2.702433   \n",
       "529         https://www.tiktok.com/@jeff_mahadika          6     1.803201   \n",
       "975               https://www.tiktok.com/@wotos88          4     1.815973   \n",
       "845     https://www.tiktok.com/@savegaza.savepale          6     6.857835   \n",
       "965          https://www.tiktok.com/@warganet.id_          3     1.430748   \n",
       "125       https://www.instagram.com/platform.news          2   155.637427   \n",
       "517         https://www.tiktok.com/@jabarpetarung          4     0.944191   \n",
       "59                                @tribunjogjaofc          2     6.846607   \n",
       "848    https://www.tiktok.com/@sempak.mulyono.007          2     2.066953   \n",
       "\n",
       "     reach_score    channel             username  \\\n",
       "75    200.000000  instagram   berita_sumbar24jam   \n",
       "462   200.000000     tiktok    @horror_nusantara   \n",
       "529   240.000000     tiktok       @jeff_mahadika   \n",
       "975   160.000000     tiktok             @wotos88   \n",
       "845   114.336667     tiktok   @savegaza.savepale   \n",
       "965   120.000000     tiktok        @warganet.id_   \n",
       "125    44.890000  instagram        platform.news   \n",
       "517   160.000000     tiktok       @jabarpetarung   \n",
       "59     80.000000    youtube      @tribunjogjaofc   \n",
       "848    80.000000     tiktok  @sempak.mulyono.007   \n",
       "\n",
       "                                        user_image_url  user_followers  \\\n",
       "75   https://scontent-cgk2-1.cdninstagram.com/v/t51...         10000.0   \n",
       "462  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...         80300.0   \n",
       "529  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...         81100.0   \n",
       "975  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...         11800.0   \n",
       "845  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...          7414.0   \n",
       "965  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...        206400.0   \n",
       "125  https://instagram.fbdo9-1.fna.fbcdn.net/v/t51....         70100.0   \n",
       "517  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...        229100.0   \n",
       "59                                                                 0.0   \n",
       "848  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...         17400.0   \n",
       "\n",
       "     engagement_rate user_category  user_influence_score  sentiment_negative  \\\n",
       "75        546.857821  News Account             10.000000                   4   \n",
       "462       648.667176         Human              6.260000                   4   \n",
       "529       722.834845                            5.721667                   4   \n",
       "975       498.752988                            4.185000                   4   \n",
       "845       926.632322         Human              2.028333                   4   \n",
       "965       390.224329                            5.816667                   3   \n",
       "125      5354.385965  News Account             10.000000                   2   \n",
       "517       464.099194                            7.880000                   2   \n",
       "59        455.136848                            6.595000                   2   \n",
       "848       302.558215        Buzzer              5.405000                   2   \n",
       "\n",
       "     sentiment_neutral  sentiment_positive  sentiment_sad  is_negative_driver  \\\n",
       "75                   1                   0              0                True   \n",
       "462                  1                   0              0                True   \n",
       "529                  2                   0              0                True   \n",
       "975                  0                   0              0                True   \n",
       "845                  2                   0              0                True   \n",
       "965                  0                   0              0                True   \n",
       "125                  0                   0              0                True   \n",
       "517                  1                   1              0                True   \n",
       "59                   0                   0              0                True   \n",
       "848                  0                   0              0                True   \n",
       "\n",
       "                                         unified_issue  most_viral  \\\n",
       "75   [Party on public road causing disturbance, See...   10.000000   \n",
       "462  [Is Indonesia under threat?, China's impact on...    6.260000   \n",
       "529  [Back 2 Allah, This is the source, Political c...    5.721667   \n",
       "975  [TNI Commander Insubordination?, Jokowi's bad ...    4.185000   \n",
       "845  [Prabowo and import issues, China cuts salarie...    2.028333   \n",
       "965  [Rejection of Prabowo-Gibran loyalists, Guru G...    5.816667   \n",
       "125  [Demand to arrest Aguan CS, Bahlil's private j...   10.000000   \n",
       "517  [Annoying Governor, KDM's promise to his mothe...    7.880000   \n",
       "59   [Demands to remove Gibran, Prabowo reduces rol...    6.595000   \n",
       "848  [Oust Gibran, arrest Jokowi, Cleanse the palac...    5.405000   \n",
       "\n",
       "     share_of_voice  \n",
       "75         0.221435  \n",
       "462        0.221435  \n",
       "529        0.265722  \n",
       "975        0.177148  \n",
       "845        0.265722  \n",
       "965        0.132861  \n",
       "125        0.088574  \n",
       "517        0.177148  \n",
       "59         0.088574  \n",
       "848        0.088574  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_kol.sort_values(['is_negative_driver','sentiment_negative','most_viral'], ascending = [False,False,False])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71ee42a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:52:56.238470Z",
     "start_time": "2025-05-11T13:52:56.115651Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7697c347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:53:04.012609Z",
     "start_time": "2025-05-11T13:53:04.007366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       10.00\n",
       "1       10.00\n",
       "2       10.00\n",
       "3       10.00\n",
       "4        9.85\n",
       "        ...  \n",
       "2253     0.00\n",
       "2254     0.00\n",
       "2255     0.00\n",
       "2256     0.00\n",
       "2257     0.00\n",
       "Name: influence_score_new, Length: 2258, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kol['influence_score_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c502c63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:53:11.186021Z",
     "start_time": "2025-05-11T13:53:11.158210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>link_post</th>\n",
       "      <th>channel</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>issue</th>\n",
       "      <th>favorites</th>\n",
       "      <th>user_image_url</th>\n",
       "      <th>comments</th>\n",
       "      <th>viral_score</th>\n",
       "      <th>influence_score</th>\n",
       "      <th>...</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "      <th>retweets</th>\n",
       "      <th>user_influence_score</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>replies</th>\n",
       "      <th>user_category</th>\n",
       "      <th>link_user</th>\n",
       "      <th>user_connections</th>\n",
       "      <th>influence_score_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@jejouw</td>\n",
       "      <td>https://www.tiktok.com/@jejouw/video/750085887...</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Interview with Boby Kertanegara</td>\n",
       "      <td>28600.0</td>\n",
       "      <td>https://p16-sign-useast2a.tiktokcdn.com/tos-us...</td>\n",
       "      <td>14900.0</td>\n",
       "      <td>0.529764</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>754700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://www.tiktok.com/@jejouw</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@detikcom</td>\n",
       "      <td>https://www.tiktok.com/@detikcom/video/7499050...</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>positive</td>\n",
       "      <td>Joko Widodo ready to help Prabowo</td>\n",
       "      <td>19600.0</td>\n",
       "      <td>https://p16-sign-useast2a.tiktokcdn.com/tos-us...</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>0.170174</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>572900</td>\n",
       "      <td>12700000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://www.tiktok.com/@detikcom</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@tempodotco</td>\n",
       "      <td>https://twitter.com/tempodotco/status/19105296...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>negative</td>\n",
       "      <td>Government neglects press from cyberattacks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/189501138...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3511</td>\n",
       "      <td>36997.0</td>\n",
       "      <td>1689.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2300000.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>News Account</td>\n",
       "      <td>https://x.com/tempodotco</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@tanyarlfes</td>\n",
       "      <td>https://twitter.com/tanyarlfes/status/19106427...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>positive</td>\n",
       "      <td>Prabowo wants to evacuate Gazans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/161061380...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>...</td>\n",
       "      <td>7653</td>\n",
       "      <td>888085.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>2239.0</td>\n",
       "      <td>Influencer</td>\n",
       "      <td>https://x.com/tanyarlfes</td>\n",
       "      <td>0</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@b4ng.yudin</td>\n",
       "      <td>https://www.tiktok.com/@b4ng.yudin/video/74945...</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>negative</td>\n",
       "      <td>Konoha busy with religion and taxes</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>https://p16-sign-va.tiktokcdn.com/tos-maliva-a...</td>\n",
       "      <td>5954.0</td>\n",
       "      <td>0.130110</td>\n",
       "      <td>9.85</td>\n",
       "      <td>...</td>\n",
       "      <td>527300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.324633</td>\n",
       "      <td>3519.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://www.tiktok.com/@b4ng.yudin</td>\n",
       "      <td>0</td>\n",
       "      <td>9.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>rri__takengon</td>\n",
       "      <td>https://www.instagram.com/rri__takengon/p/DIWc...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Prabowo offers to evacuate Gazans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://scontent-cgk1-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358093</td>\n",
       "      <td>8299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News Account</td>\n",
       "      <td>https://www.instagram.com/rri__takengon</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>rri__takengon</td>\n",
       "      <td>https://www.instagram.com/rri__takengon/p/DIWU...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Gayo Lues Regent appointed echelon III officials</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://scontent-cgk1-2.cdninstagram.com/v/t51...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358093</td>\n",
       "      <td>8299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News Account</td>\n",
       "      <td>https://www.instagram.com/rri__takengon</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>@tom_millari</td>\n",
       "      <td>https://www.tiktok.com/@tom_millari/video/7500...</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>positive</td>\n",
       "      <td>#TPPBersamaMenteriDesa assisting and facilitating</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://www.tiktok.com/@tom_millari</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>@indnsiaemas</td>\n",
       "      <td>https://www.tiktok.com/@indnsiaemas/video/7494...</td>\n",
       "      <td>tiktok</td>\n",
       "      <td>positive</td>\n",
       "      <td>11 strategic programs</td>\n",
       "      <td>0.0</td>\n",
       "      <td>https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>https://www.tiktok.com/@indnsiaemas</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>@agus_whyd15</td>\n",
       "      <td>https://www.youtube.com/watch?v=q1Hk7lOE0XA</td>\n",
       "      <td>youtube</td>\n",
       "      <td>positive</td>\n",
       "      <td>More business opportunities, villages benefit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.ytimg.com/vi/pkiEjn0iX40/hqdefault.j...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>@agus_whyd15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2258 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           username                                          link_post  \\\n",
       "0           @jejouw  https://www.tiktok.com/@jejouw/video/750085887...   \n",
       "1         @detikcom  https://www.tiktok.com/@detikcom/video/7499050...   \n",
       "2       @tempodotco  https://twitter.com/tempodotco/status/19105296...   \n",
       "3       @tanyarlfes  https://twitter.com/tanyarlfes/status/19106427...   \n",
       "4       @b4ng.yudin  https://www.tiktok.com/@b4ng.yudin/video/74945...   \n",
       "...             ...                                                ...   \n",
       "2253  rri__takengon  https://www.instagram.com/rri__takengon/p/DIWc...   \n",
       "2254  rri__takengon  https://www.instagram.com/rri__takengon/p/DIWU...   \n",
       "2255   @tom_millari  https://www.tiktok.com/@tom_millari/video/7500...   \n",
       "2256   @indnsiaemas  https://www.tiktok.com/@indnsiaemas/video/7494...   \n",
       "2257   @agus_whyd15        https://www.youtube.com/watch?v=q1Hk7lOE0XA   \n",
       "\n",
       "        channel sentiment                                              issue  \\\n",
       "0        tiktok   neutral                    Interview with Boby Kertanegara   \n",
       "1        tiktok  positive                  Joko Widodo ready to help Prabowo   \n",
       "2       twitter  negative        Government neglects press from cyberattacks   \n",
       "3       twitter  positive                   Prabowo wants to evacuate Gazans   \n",
       "4        tiktok  negative                Konoha busy with religion and taxes   \n",
       "...         ...       ...                                                ...   \n",
       "2253  instagram   neutral                  Prabowo offers to evacuate Gazans   \n",
       "2254  instagram   neutral   Gayo Lues Regent appointed echelon III officials   \n",
       "2255     tiktok  positive  #TPPBersamaMenteriDesa assisting and facilitating   \n",
       "2256     tiktok  positive                              11 strategic programs   \n",
       "2257    youtube  positive      More business opportunities, villages benefit   \n",
       "\n",
       "      favorites                                     user_image_url  comments  \\\n",
       "0       28600.0  https://p16-sign-useast2a.tiktokcdn.com/tos-us...   14900.0   \n",
       "1       19600.0  https://p16-sign-useast2a.tiktokcdn.com/tos-us...   40000.0   \n",
       "2           NaN  https://pbs.twimg.com/profile_images/189501138...       NaN   \n",
       "3           NaN  https://pbs.twimg.com/profile_images/161061380...       NaN   \n",
       "4       21600.0  https://p16-sign-va.tiktokcdn.com/tos-maliva-a...    5954.0   \n",
       "...         ...                                                ...       ...   \n",
       "2253        NaN  https://scontent-cgk1-2.cdninstagram.com/v/t51...       0.0   \n",
       "2254        NaN  https://scontent-cgk1-2.cdninstagram.com/v/t51...       0.0   \n",
       "2255        0.0  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...       0.0   \n",
       "2256        0.0  https://p16-sign-sg.tiktokcdn.com/tos-alisg-av...       0.0   \n",
       "2257        NaN  https://i.ytimg.com/vi/pkiEjn0iX40/hqdefault.j...       0.0   \n",
       "\n",
       "      viral_score  influence_score  ...   likes       views  retweets  \\\n",
       "0        0.529764            10.00  ...  754700         NaN       NaN   \n",
       "1        0.170174            10.00  ...  572900  12700000.0       NaN   \n",
       "2        0.000000            10.00  ...    3511     36997.0    1689.0   \n",
       "3        0.000000            10.00  ...    7653    888085.0     613.0   \n",
       "4        0.130110             9.85  ...  527300         NaN       NaN   \n",
       "...           ...              ...  ...     ...         ...       ...   \n",
       "2253     0.000000             0.00  ...       0         NaN       NaN   \n",
       "2254     0.000000             0.00  ...       0         NaN       NaN   \n",
       "2255     0.000000             0.00  ...       0         NaN       NaN   \n",
       "2256     0.000000             0.00  ...       0         NaN       NaN   \n",
       "2257     0.000000             0.00  ...       0         0.0       NaN   \n",
       "\n",
       "      user_influence_score  user_followers  replies  user_category  \\\n",
       "0                 0.000000             0.0      NaN                  \n",
       "1                 0.000000             0.0      NaN                  \n",
       "2                 0.700000       2300000.0    201.0   News Account   \n",
       "3                 0.700000       4000000.0   2239.0     Influencer   \n",
       "4                 0.324633          3519.0      NaN                  \n",
       "...                    ...             ...      ...            ...   \n",
       "2253              0.358093          8299.0      NaN   News Account   \n",
       "2254              0.358093          8299.0      NaN   News Account   \n",
       "2255              0.002723            42.0      NaN                  \n",
       "2256              0.000000             0.0      NaN                  \n",
       "2257              0.000000             0.0      NaN                  \n",
       "\n",
       "                                    link_user  user_connections  \\\n",
       "0              https://www.tiktok.com/@jejouw                 0   \n",
       "1            https://www.tiktok.com/@detikcom                 0   \n",
       "2                    https://x.com/tempodotco                 0   \n",
       "3                    https://x.com/tanyarlfes                 0   \n",
       "4          https://www.tiktok.com/@b4ng.yudin                 0   \n",
       "...                                       ...               ...   \n",
       "2253  https://www.instagram.com/rri__takengon                 0   \n",
       "2254  https://www.instagram.com/rri__takengon                 0   \n",
       "2255      https://www.tiktok.com/@tom_millari                 0   \n",
       "2256      https://www.tiktok.com/@indnsiaemas                 0   \n",
       "2257                             @agus_whyd15                 0   \n",
       "\n",
       "     influence_score_new  \n",
       "0                  10.00  \n",
       "1                  10.00  \n",
       "2                  10.00  \n",
       "3                  10.00  \n",
       "4                   9.85  \n",
       "...                  ...  \n",
       "2253                0.00  \n",
       "2254                0.00  \n",
       "2255                0.00  \n",
       "2256                0.00  \n",
       "2257                0.00  \n",
       "\n",
       "[2258 rows x 23 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kol"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
