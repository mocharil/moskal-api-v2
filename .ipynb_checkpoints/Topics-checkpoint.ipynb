{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "388be4d4",
   "metadata": {},
   "source": [
    "Idenya adalah\n",
    "1. jika topics belum ada,maka ambil 500 post popular pertama di 7 hari kebelakang lalu di clustering \n",
    "2. return ke UI\n",
    "3. hasil dari point pertama di masukan ke ES in background\n",
    "---\n",
    "4. jika sudah ada, maka API hanya akan send hasil yg ada di ES saja, tidak perlu bikin ulang\n",
    "---\n",
    "5. terdapat JOB yg akan melakukan point pertama berulang ulang agar data semakin kaya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41def54f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T23:24:51.315153Z",
     "start_time": "2025-05-22T23:24:51.268938Z"
    }
   },
   "source": [
    "Contoh Output API\n",
    "\n",
    "\n",
    "{\n",
    "        \"unified_issue\": \"Banking and Financial Institution Activities\",\n",
    "        \"description\": \"Performance, strategies, and business activities of various banks and financial institutions in Indonesia.\",\n",
    "        \"list_issue\": [\n",
    "            \"Microfinance bank association members' performance\",\n",
    "            \"Morning Gymnastics at Bank Indonesia Kediri\"\n",
    "        ],\n",
    "        \"total_posts\": 291,\n",
    "        \"viral_score\": 71.81342340121046,\n",
    "        \"reach_score\": 800.3937775007216,\n",
    "        \"positive\": 149,\n",
    "        \"negative\": 7,\n",
    "        \"neutral\": 135,\n",
    "        \"share_of_voice\": 5.752124925874679\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ff0ef",
   "metadata": {},
   "source": [
    "# untuk topics yg baru dibuat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345d51f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:59:11.505915Z",
     "start_time": "2025-05-23T16:59:11.440364Z"
    },
    "code_folding": [
     16,
     39,
     44,
     68,
     93
    ]
   },
   "outputs": [],
   "source": [
    "#check apakah project sudah ada?\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from typing import List, Dict, Any, Optional\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from utils.gemini import call_gemini\n",
    "from utils.list_of_mentions import get_mentions\n",
    "from utils.topics.es_operations import upsert_documents\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "ES_HOST = os.getenv('ES_HOST','http://34.101.178.71:9200')\n",
    "ES_USERNAME = os.getenv('ES_USERNAME','elastic')\n",
    "ES_PASSWORD = os.getenv('ES_PASSWORD','elasticpassword')\n",
    "# Inisialisasi client\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth = (ES_USERNAME, ES_PASSWORD))\n",
    "\n",
    "def get_data_topics(project_name):\n",
    "    query_body = {\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\":{\n",
    "                \"match\":{\n",
    "                    \"project_name\":project_name\n",
    "                }\n",
    "\n",
    "            }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    data = es.search(\n",
    "        index = index_name,\n",
    "        body = query_body,\n",
    "        size = 10000\n",
    "    )\n",
    "\n",
    "    topics_overview = [i['_source'] for i in data['hits']['hits']]\n",
    "    return topics_overview\n",
    "\n",
    "def create_uuid(keyword: str) -> str:\n",
    "    \"\"\"Generate UUID for a given keyword\"\"\"\n",
    "    namespace = uuid.NAMESPACE_DNS\n",
    "    return str(uuid.uuid5(namespace, keyword))\n",
    "\n",
    "def ingest_topic(results, project_name, es = None) -> None:\n",
    "    \"\"\"Ingest topic results into Elasticsearch\"\"\"\n",
    "    if es is None:\n",
    "        es = get_elasticsearch_client()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df['project_name'] = project_name\n",
    "    df['index'] = range(df.shape[0])\n",
    "    df['uuid'] = df.apply(lambda s: create_uuid(f\"{s['project_name']},{s['unified_issue']}\" if len(s['list_issue'])<100\\\n",
    "                                                else f\"{s['project_name']},{s['unified_issue']},{s['index']}\"), axis=1)\n",
    "\n",
    "    data_ingest = df[['unified_issue', 'description', 'list_issue', 'uuid', 'project_name']].to_dict(orient='records')\n",
    "\n",
    "    updated, created, errors = upsert_documents(\n",
    "        es,\n",
    "        data_ingest,\n",
    "        \"topics_overview\",\n",
    "        id_field=\"uuid\",\n",
    "        fields_to_update=None,\n",
    "        chunk_size=1000\n",
    "    )\n",
    "\n",
    "    print(f\"Results: {updated} documents updated, {created} documents created, {len(errors)} errors\")\n",
    "    \n",
    "def split_rows_by_list_length(df, column='list_issue', max_len=100):\n",
    "    \"\"\"\n",
    "    Membelah baris jika panjang list dalam kolom tertentu melebihi max_len.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame dengan kolom list.\n",
    "        column (str): Nama kolom yang berisi list.\n",
    "        max_len (int): Jumlah maksimum elemen per baris.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame baru dengan baris terbelah.\n",
    "    \"\"\"\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        items = row[column]\n",
    "        # Membagi list menjadi potongan dengan panjang maksimal max_len\n",
    "        chunks = [items[i:i + max_len] for i in range(0, len(items), max_len)]\n",
    "        for chunk in chunks:\n",
    "            new_row = row.copy()\n",
    "            new_row[column] = chunk\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(new_rows).reset_index(drop=True)    \n",
    "\n",
    "def get_date_range(date_filter=\"last 30 days\", custom_start_date=None, custom_end_date=None):\n",
    "\n",
    "    today = datetime.now().date()\n",
    "    \n",
    "    if date_filter == \"custom\" and custom_start_date and custom_end_date:\n",
    "        return custom_start_date, custom_end_date\n",
    "    \n",
    "    if date_filter == \"yesterday\":\n",
    "        yesterday = today - timedelta(days=1)\n",
    "        return yesterday.strftime(\"%Y-%m-%d\"), yesterday.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    elif date_filter == \"this week\":\n",
    "        start_of_week = today - timedelta(days=today.weekday())\n",
    "        return start_of_week.strftime(\"%Y-%m-%d\"), today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    elif date_filter == \"last 14 days\":\n",
    "        return (today - timedelta(days=14)).strftime(\"%Y-%m-%d\"), today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    elif date_filter == \"last 7 days\":\n",
    "        return (today - timedelta(days=7)).strftime(\"%Y-%m-%d\"), today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    elif date_filter == \"last 30 days\":\n",
    "        return (today - timedelta(days=30)).strftime(\"%Y-%m-%d\"), today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    elif date_filter == \"last 3 months\":\n",
    "        return (today - timedelta(days=90)).strftime(\"%Y-%m-%d\"), today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    elif date_filter == \"this year\":\n",
    "        return f\"{today.year}-01-01\", today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    elif date_filter == \"last year\":\n",
    "        return f\"{today.year-1}-01-01\", f\"{today.year-1}-12-31\"\n",
    "    \n",
    "    else:  # \"all time\" or fallback\n",
    "        return \"2000-01-01\", today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "def regenerate_topics(suggestion_unified_issue,df_issue):\n",
    "    #call gemini untuk clustering\n",
    "    issue_list = []\n",
    "    for idx, row in df_issue\\\n",
    "                    .sort_values(['total_post','negative_posts','total_reach_score'],\n",
    "                                ascending = [False,False,False])[:100].iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a Social Media Analysis Expert specializing in thematic clustering of issues.\n",
    "\n",
    "    # TASK\n",
    "    Analyze and group the following list of social media issues into meaningful thematic clusters.\n",
    "\n",
    "    # IMPORTANT INSTRUCTIONS\n",
    "    1. Each issue ID must belong to EXACTLY ONE group (mutually exclusive).\n",
    "    2. Avoid any overlap of issues between groups.\n",
    "    3. Focus on identifying the main topic or theme of each issue.\n",
    "    4. Create a unified_issue name that is **clear, specific, and truly represents the essence** of the grouped issues.\n",
    "    5. Avoid using overly generic group names like \"General Issues\" or \"Various Topics.\"\n",
    "    6. Provide an informative and comprehensive description for each unified_issue that:\n",
    "       - Accurately summarizes its main theme,\n",
    "       - Highlights prevalent sentiment(s) (e.g., positive, negative, neutral),\n",
    "       - Mentions any notable patterns, concerns, or emotional tones observed,\n",
    "       - Includes relevant contextual details or recurring keywords if applicable.\n",
    "    7. If an issue is not related to the topic of \"{project_name}\", group it under the name **\"Other\"** and provide a description that clearly reflects the non-relevance to \"{project_name}.\"\n",
    "    8. Use the following suggestion list of possible unified_issue names and descriptions as references. If any suggestion fits well for a group of issues, feel free to use that unified_issue name and description exactly as provided.\n",
    "    \n",
    "    ## SUGGESTED UNIFIED_ISSUES (unified_issue and description)\n",
    "    ```\n",
    "    {suggestion_unified_issue}\n",
    "    ```\n",
    "\n",
    "    # LIST OF ISSUES\n",
    "    ```\n",
    "    {issue_list}\n",
    "    ```\n",
    "\n",
    "    # OUTPUT FORMAT\n",
    "    Return the grouped results strictly in the following JSON format:\n",
    "    ```\n",
    "    [\n",
    "    {{\n",
    "    \"unified_issue\": \"Name of Grouped Issue 1\",\n",
    "    \"description\": \"Concise description summarizing the main theme of Group 1\",\n",
    "    \"list_issue_id\": [1, 5, 10, 15]\n",
    "    }},\n",
    "    {{\n",
    "    \"unified_issue\": \"Name of Grouped Issue 2\",\n",
    "    \"description\": \"Concise description summarizing the main theme of Group 2\",\n",
    "    \"list_issue_id\": [2, 6, 11, 16]\n",
    "    }},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    # QUALITY PARAMETERS\n",
    "    - Group based on semantic similarity of topics and keywords.\n",
    "    - Choose a **specific** and **meaningful** name for each unified_issue that reflects the core topic clearly.\n",
    "\n",
    "    # HARD RULES:\n",
    "    - Return the output ONLY in pure JSON format, without any explanation or additional comments.\n",
    "    - The unified_issue name and description must be in English.\n",
    "    \"\"\"\n",
    "\n",
    "    centrality = call_gemini(prompt)\n",
    "    try:\n",
    "        data_center = pd.DataFrame(eval(re.findall(r'\\[.*\\]', centrality, flags=re.I|re.S)[0]))\n",
    "    except:\n",
    "        unified_issue = [i.strip('\\n ,\"') for i in re.findall(r'unified_issue[\\\"\\,\\s\\:]+(.*?)description',centrality, flags=re.I|re.S)]\n",
    "        description = [i.strip('\\n ,\"') for i in re.findall(r'description[\\\"\\,\\s\\:]+(.*?)list_issue_id',centrality, flags=re.I|re.S)]\n",
    "        list_issue_id = [eval(i.strip('\\n ,\"')) for i in re.findall(r'list_issue_id[\\\"\\,\\s\\:]+(.*?)\\}',centrality, flags=re.I|re.S)]\n",
    "\n",
    "        data = []\n",
    "        for u,d,l in zip(unified_issue, description, list_issue_id):\n",
    "            data.append({'unified_issue':u, 'description':d,'list_issue_id':l})\n",
    "        data_center = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    df_exploded = data_center.explode('list_issue_id')\n",
    "    df_exploded = df_exploded.rename(columns={\"list_issue_id\": \"index\"})\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_exploded[['index', 'unified_issue', 'description']],\n",
    "        df_issue,\n",
    "        on='index',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    df_final = df_merged.groupby(['unified_issue', 'description']).agg(\n",
    "        list_issue=('issue', list),\n",
    "        total_posts=('total_post', 'sum'),\n",
    "        viral_score=('total_viral_score', 'sum'),\n",
    "        reach_score=('total_reach_score', 'sum'),\n",
    "        positive=('positive_posts', 'sum'),\n",
    "        negative=('negative_posts', 'sum'),\n",
    "        neutral=('neutral_posts', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_final['share_of_voice'] = (df_final['total_posts']/df_final['total_posts'].sum())*100\n",
    "    df_final = df_final.sort_values('share_of_voice', ascending=False)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47590683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:59:12.234036Z",
     "start_time": "2025-05-23T16:59:12.188320Z"
    },
    "code_folding": [
     184,
     263
    ]
   },
   "outputs": [],
   "source": [
    "def get_data_issue(owner_id: Optional[str] = 1,\n",
    "    project_name: Optional[str] = None,\n",
    "    es: Optional[Elasticsearch] = None,\n",
    "    keywords: Optional[str] = [],\n",
    "    search_keyword: Optional[str] = [],\n",
    "    search_exact_phrases: bool = False,\n",
    "    case_sensitive: bool = False,\n",
    "    sentiment: Optional[str] = ['positive','negative','neutral'],\n",
    "    start_date: Optional[str] = None,\n",
    "    end_date: Optional[str] = None,\n",
    "    date_filter: str = \"last 30 days\",\n",
    "    custom_start_date: Optional[str] = None,\n",
    "    custom_end_date: Optional[str] = None,\n",
    "    channels: Optional[List[str]] = None,\n",
    "    importance: str = \"all mentions\",\n",
    "    influence_score_min: Optional[float] = 0,\n",
    "    influence_score_max: Optional[float] = 1000,\n",
    "    region: Optional[str] = [],\n",
    "    language: Optional[str] = [],\n",
    "    domain: Optional[str] = [],\n",
    "    limit=1000\n",
    "              ):\n",
    "\n",
    "    list_keywords = [\n",
    "                        {\n",
    "                          \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                              \"query\": i,\n",
    "                              \"operator\": \"AND\"\n",
    "                            }\n",
    "                          }\n",
    "                        } for i in keywords\n",
    "                      ]\n",
    "    list_keywords.extend([ {\n",
    "                          \"match\": {\n",
    "                            \"issue\": {\n",
    "                              \"query\": i,\n",
    "                              \"operator\": \"AND\"\n",
    "                            }\n",
    "                          }\n",
    "                        } for i in keywords\n",
    "\n",
    "    ])\n",
    "    \n",
    "\n",
    "    if not start_date or not end_date:\n",
    "        start_date, end_date = get_date_range(\n",
    "            date_filter=date_filter,\n",
    "            custom_start_date=custom_start_date,\n",
    "            custom_end_date=custom_end_date\n",
    "        )\n",
    "    \n",
    "    \n",
    "    query = {\n",
    "      \"size\": 0,\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"post_created_at\": {\n",
    "                    \"gte\": start_date,\n",
    "                    \"lte\": end_date\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"bool\": {\n",
    "                \"must\": [\n",
    "                  {\n",
    "                    \"bool\": {\n",
    "                      \"should\": list_keywords,\n",
    "\n",
    "\n",
    "\n",
    "                      \"minimum_should_match\": 1\n",
    "                    }\n",
    "                  },\n",
    "                  {\n",
    "                    \"bool\": {\n",
    "                      \"should\": [\n",
    "                        {\n",
    "                          \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                              \"query\": f\"{i}\",\n",
    "                              \"operator\": \"AND\"\n",
    "                            }\n",
    "                          }\n",
    "                        } for i in search_keyword\n",
    "                      ],\n",
    "                      \"minimum_should_match\": 1\n",
    "                    }\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "                \n",
    "            \"bool\":{\n",
    "                \"must_not\":{\n",
    "                    \"match\":{\n",
    "                        \"issue\":\"Not Specified\"\n",
    "                        \n",
    "                    }\n",
    "                    \n",
    "                    \n",
    "                }\n",
    "                \n",
    "            }\n",
    "                \n",
    "                \n",
    "            }\n",
    "          ],\n",
    "          \"filter\": [\n",
    "            {\n",
    "              \"terms\": {\n",
    "                \"sentiment\": sentiment\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"influence_score\": {\n",
    "                  \"gte\": influence_score_min,\n",
    "                  \"lte\": influence_score_max\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"bool\": {\n",
    "                \"should\": [\n",
    "                  {\n",
    "                    \"wildcard\": {\n",
    "                      \"region\": f\"*{i}*\"\n",
    "                    }\n",
    "                  } for i in region\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"bool\": {\n",
    "                \"should\": [\n",
    "                  {\n",
    "                    \"wildcard\": {\n",
    "                      \"language\": f\"*{i}*\"\n",
    "                    }\n",
    "                  } for i in language\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"bool\": {\n",
    "                \"should\": [\n",
    "                  {\n",
    "                    \"wildcard\": {\n",
    "                      \"link_post\": f\"*{i}*\"\n",
    "                    }\n",
    "                  }\n",
    "                    for i in domain\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"bool\": {\n",
    "                \"must\": [\n",
    "                  {\n",
    "                    \"exists\": {\n",
    "                      \"field\": \"viral_score\"\n",
    "                    }\n",
    "                  },\n",
    "                  {\n",
    "                    \"exists\": {\n",
    "                      \"field\": \"sentiment\"\n",
    "                    }\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"from\": 0,\n",
    "      \"aggs\": {\n",
    "        \"by_issue\": {\n",
    "          \"terms\": {\n",
    "            \"field\": \"issue.keyword\",\n",
    "            \"size\": limit,\n",
    "            \"order\": {\n",
    "              \"_count\": \"desc\"\n",
    "            }\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"total_viral_score\": {\n",
    "              \"sum\": { \"field\": \"viral_score\" }\n",
    "            },\n",
    "            \"total_reach_score\": {\n",
    "              \"sum\": { \"field\": \"reach_score\" }\n",
    "            },\n",
    "            \"total_post\": {\n",
    "              \"value_count\": { \"field\": \"issue.keyword\" }\n",
    "            },\n",
    "            \"positive_posts\": {\n",
    "              \"filter\": { \"term\": { \"sentiment\": \"positive\" } }\n",
    "            },\n",
    "            \"negative_posts\": {\n",
    "              \"filter\": { \"term\": { \"sentiment\": \"negative\" } }\n",
    "            },\n",
    "            \"neutral_posts\": {\n",
    "              \"filter\": { \"term\": { \"sentiment\": \"neutral\" } }\n",
    "            },\n",
    "            \"sample_posts\": {\n",
    "              \"top_hits\": {\n",
    "                \"size\": 2,\n",
    "                \"_source\": {\n",
    "                  \"includes\": [\n",
    "                    \"post_caption\",\n",
    "                      \"link_post\"\n",
    "                  ]\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "\n",
    "    if not channels:\n",
    "        channels = ['tiktok','instagram','news','reddit','facebook','twitter','linkedin','youtube']\n",
    "    \n",
    "    index = ','.join([i+'_data' for i in channels])\n",
    "    hasil = es.search(index = index,\n",
    "             body = query)\n",
    "    \n",
    "    buckets = hasil['aggregations']['by_issue']['buckets']\n",
    "    \n",
    "    data = []\n",
    "    for bucket in buckets:\n",
    "        sample_posts_list = []\n",
    "        for hit in bucket['sample_posts']['hits']['hits']:\n",
    "            post = hit['_source']\n",
    "            sample_posts_list.append({\n",
    "                \"post_caption\": post.get(\"post_caption\", \"\"),\n",
    "                \"link_post\": post.get(\"link_post\", \"\")\n",
    "            })\n",
    "\n",
    "        data.append({\n",
    "            \"issue\": bucket['key'],\n",
    "            \"total_viral_score\": bucket['total_viral_score']['value'],\n",
    "            \"total_reach_score\": bucket['total_reach_score']['value'],\n",
    "            \"total_post\": bucket['total_post']['value'],\n",
    "            \"positive_posts\": bucket['positive_posts']['doc_count'],\n",
    "            \"negative_posts\": bucket['negative_posts']['doc_count'],\n",
    "            \"neutral_posts\": bucket['neutral_posts']['doc_count'],\n",
    "            \"sample_posts\": sample_posts_list\n",
    "        })\n",
    "\n",
    "    # Buat DataFrame\n",
    "    df_issue = pd.DataFrame(data).reset_index()\n",
    "    return df_issue    \n",
    "\n",
    "def new_topics(owner_id: Optional[str] = 1,\n",
    "    project_name: Optional[str] = None,\n",
    "    es: Optional[Elasticsearch] = None,\n",
    "    keywords: Optional[str] = [],\n",
    "    search_keyword: Optional[str] = [],\n",
    "    search_exact_phrases: bool = False,\n",
    "    case_sensitive: bool = False,\n",
    "    sentiment: Optional[str] = ['positive','negative','neutral'],\n",
    "    start_date: Optional[str] = None,\n",
    "    end_date: Optional[str] = None,\n",
    "    date_filter: str = \"last 30 days\",\n",
    "    custom_start_date: Optional[str] = None,\n",
    "    custom_end_date: Optional[str] = None,\n",
    "    channels: Optional[List[str]] = None,\n",
    "    importance: str = \"all mentions\",\n",
    "    influence_score_min: Optional[float] = 0,\n",
    "    influence_score_max: Optional[float] = 1000,\n",
    "    region: Optional[str] = [],\n",
    "    language: Optional[str] = [],\n",
    "    domain: Optional[str] = []\n",
    "              ):\n",
    "\n",
    "    # Buat DataFrame\n",
    "    df_issue = get_data_issue(\n",
    "        owner_id = owner_id,\n",
    "        es = es,\n",
    "        project_name = project_name,\n",
    "        keywords=keywords,\n",
    "        search_keyword=search_keyword,\n",
    "        search_exact_phrases=search_exact_phrases,\n",
    "        case_sensitive=case_sensitive,\n",
    "        sentiment=sentiment,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        date_filter=date_filter,\n",
    "        custom_start_date=custom_start_date,\n",
    "        custom_end_date=custom_end_date,\n",
    "        channels=channels,\n",
    "        importance=importance,\n",
    "        influence_score_min=influence_score_min,\n",
    "        influence_score_max=influence_score_max,\n",
    "        region=region,\n",
    "        language=language,\n",
    "        domain=domain\n",
    "    \n",
    "    )\n",
    "    \n",
    "    #call gemini untuk clustering\n",
    "    issue_list = []\n",
    "    for idx, row in df_issue\\\n",
    "                    .sort_values(['total_post','negative_posts','total_reach_score'],\n",
    "                                ascending = [False,False,False])[:50].iterrows():\n",
    "        issue_list.append({\"id\": row['index'], \"issue\": row['issue']})\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a Social Media Analysis Expert specializing in thematic clustering of issues.\n",
    "\n",
    "    # TASK\n",
    "    Analyze and group the following list of social media issues into meaningful thematic clusters.\n",
    "\n",
    "    # IMPORTANT INSTRUCTIONS\n",
    "    1. Each issue ID must belong to EXACTLY ONE group (mutually exclusive).\n",
    "    2. Avoid any overlap of issues between groups.\n",
    "    3. Focus on identifying the main topic or theme of each issue.\n",
    "    4. Create a unified_issue name that is **clear, specific, and truly represents the essence** of the grouped issues.\n",
    "    5. Avoid using overly generic group names like \"General Issues\" or \"Various Topics.\"\n",
    "    6. Provide an informative and comprehensive description for each unified_issue that:\n",
    "       - Accurately summarizes its main theme,\n",
    "       - Highlights prevalent sentiment(s) (e.g., positive, negative, neutral),\n",
    "       - Mentions any notable patterns, concerns, or emotional tones observed,\n",
    "       - Includes relevant contextual details or recurring keywords if applicable.\n",
    "\n",
    "    # LIST OF ISSUES\n",
    "    ```\n",
    "    {issue_list}\n",
    "    ```\n",
    "\n",
    "    # OUTPUT FORMAT\n",
    "    Return the grouped results strictly in the following JSON format:\n",
    "    ```\n",
    "    [\n",
    "    {{\n",
    "    \"unified_issue\": \"Name of Grouped Issue 1\",\n",
    "    \"description\": \"Concise description summarizing the main theme of Group 1\",\n",
    "    \"list_issue_id\": [1, 5, 10, 15]\n",
    "    }},\n",
    "    {{\n",
    "    \"unified_issue\": \"Name of Grouped Issue 2\",\n",
    "    \"description\": \"Concise description summarizing the main theme of Group 2\",\n",
    "    \"list_issue_id\": [2, 6, 11, 16]\n",
    "    }},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    # QUALITY PARAMETERS\n",
    "    - Group based on semantic similarity of topics and keywords.\n",
    "    - Choose a **specific** and **meaningful** name for each unified_issue that reflects the core topic clearly.\n",
    "\n",
    "    # HARD RULES:\n",
    "    - Return the output ONLY in pure JSON format, without any explanation or additional comments.\n",
    "    - The unified_issue name and description must be in English.\n",
    "    \"\"\"\n",
    "\n",
    "    centrality = call_gemini(prompt)\n",
    "    try:\n",
    "        data_center = pd.DataFrame(eval(re.findall(r'\\[.*\\]', centrality, flags=re.I|re.S)[0]))\n",
    "    except:\n",
    "        unified_issue = [i.strip('\\n ,\"') for i in re.findall(r'unified_issue[\\\"\\,\\s\\:]+(.*?)description',centrality, flags=re.I|re.S)]\n",
    "        description = [i.strip('\\n ,\"') for i in re.findall(r'description[\\\"\\,\\s\\:]+(.*?)list_issue_id',centrality, flags=re.I|re.S)]\n",
    "        list_issue_id = [eval(i.strip('\\n ,\"')) for i in re.findall(r'list_issue_id[\\\"\\,\\s\\:]+(.*?)\\}',centrality, flags=re.I|re.S)]\n",
    "\n",
    "        data = []\n",
    "        for u,d,l in zip(unified_issue, description, list_issue_id):\n",
    "            data.append({'unified_issue':u, 'description':d,'list_issue_id':l})\n",
    "        data_center = pd.DataFrame(data)\n",
    "\n",
    "        \n",
    "    df_exploded = data_center.explode('list_issue_id')\n",
    "    df_exploded = df_exploded.rename(columns={\"list_issue_id\": \"index\"})\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_exploded[['index', 'unified_issue', 'description']],\n",
    "        df_issue,\n",
    "        on='index',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    df_final = df_merged.groupby(['unified_issue', 'description']).agg(\n",
    "        list_issue=('issue', list),\n",
    "        total_posts=('total_post', 'sum'),\n",
    "        viral_score=('total_viral_score', 'sum'),\n",
    "        reach_score=('total_reach_score', 'sum'),\n",
    "        positive=('positive_posts', 'sum'),\n",
    "        negative=('negative_posts', 'sum'),\n",
    "        neutral=('neutral_posts', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_final['share_of_voice'] = (df_final['total_posts']/df_final['total_posts'].sum())*100\n",
    "    df_final = df_final.sort_values('share_of_voice', ascending=False)\n",
    "\n",
    "    results = df_final.to_dict(orient = 'records')\n",
    "    \n",
    "    #--------------- sisa df_final di background ----------------\n",
    "    print('DIBACKGROUND')\n",
    "    ingest_topic(results, project_name, es = es)\n",
    "    \n",
    "    loop=2\n",
    "    for i in range(loop):\n",
    "        suggestion_unified_issue = df_final[['unified_issue','description']].to_dict(orient = 'records')\n",
    "        df_issue = df_issue[~df_issue['issue'].isin([j for i in df_final['list_issue'] for j in i])]\n",
    "        if df_issue.empty:\n",
    "            break\n",
    "        df_final2 = regenerate_topics(suggestion_unified_issue,df_issue)\n",
    "        df_final = pd.concat([df_final, df_final2])\n",
    "        df_final = df_final.groupby(['unified_issue']).agg(\n",
    "                description=('description', 'max'),\n",
    "                list_issue=('list_issue', lambda s: [j for i in s for j in i if not pd.isna(j)]),\n",
    "                total_posts=('total_posts', 'sum'),\n",
    "                viral_score=('viral_score', 'sum'),\n",
    "                reach_score=('reach_score', 'sum'),\n",
    "                positive=('positive', 'sum'),\n",
    "                negative=('negative', 'sum'),\n",
    "                neutral=('neutral', 'sum')).reset_index()\n",
    "        df_final['share_of_voice'] = (df_final['total_posts']/df_final['total_posts'].sum())*100\n",
    "        df_final = split_rows_by_list_length(df_final)\n",
    "\n",
    "        print(df_final.shape, df_final['unified_issue'].nunique(),len(set([j for i in df_final['list_issue'] for j in i])))\n",
    "        results = df_final.to_dict(orient = 'records')\n",
    "        ingest_topic(results, project_name,es = es)\n",
    "    # --------------------- --------------------------------------\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cc3442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:59:13.449813Z",
     "start_time": "2025-05-23T16:59:13.438201Z"
    },
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "index_name = \"topics_overview\"\n",
    "project_name = \"danantara\"\n",
    "keywords = ['danantara',\"investasi indonesia\"]\n",
    "owner_id = 5\n",
    "search_keyword = []\n",
    "domain = []\n",
    "language = []\n",
    "region = []\n",
    "influence_score_min= 0\n",
    "influence_score_max= 1000\n",
    "sentiment = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"neutral\"\n",
    "  ]\n",
    "start_date = None\n",
    "end_date = None\n",
    "date_filter = 'last 7 days'\n",
    "custom_start_date=None\n",
    "custom_end_date = None\n",
    "channels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ad7602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:59:15.762854Z",
     "start_time": "2025-05-23T16:59:15.636610Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics_overview/_search [status:200 duration:0.090s]\n"
     ]
    }
   ],
   "source": [
    "#apakah topics overview sudah ada?\n",
    "#jika sudah maka return else, olah per 500 data popular\n",
    "topics_overview = get_data_topics(project_name)\n",
    "if not topics_overview:\n",
    "    print('belum ada')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a93c15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T16:25:24.783601Z",
     "start_time": "2025-05-23T16:24:54.732150Z"
    }
   },
   "outputs": [],
   "source": [
    "new_topics(project_name = project_name,\n",
    "           keywords = keywords,\n",
    "           es = es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9baa2",
   "metadata": {},
   "source": [
    "# misalnya nyari di date lain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4410670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:01:36.816745Z",
     "start_time": "2025-05-23T17:01:36.806188Z"
    }
   },
   "outputs": [],
   "source": [
    "index_name = \"topics_overview\"\n",
    "project_name = \"danantara\"\n",
    "keywords = ['danantara',\"investasi indonesia\"]\n",
    "owner_id = 5\n",
    "search_keyword = []\n",
    "domain = []\n",
    "language = []\n",
    "region = []\n",
    "influence_score_min= 0\n",
    "influence_score_max= 1000\n",
    "sentiment = [\n",
    "    \"positive\",\n",
    "    \"negative\",\n",
    "    \"neutral\"\n",
    "  ]\n",
    "start_date = None\n",
    "end_date = None\n",
    "date_filter = 'last 7 days'\n",
    "custom_start_date=None\n",
    "custom_end_date = None\n",
    "channels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74f5ef7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:03:52.564268Z",
     "start_time": "2025-05-23T17:03:52.387774Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics_overview/_search [status:200 duration:0.136s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudah ada\n"
     ]
    }
   ],
   "source": [
    "#apakah topics overview sudah ada?\n",
    "#jika sudah maka return else, olah per 500 data popular\n",
    "topics_overview = get_data_topics(project_name)\n",
    "if not topics_overview:\n",
    "    print('belum ada')\n",
    "    hasil = new_topics(project_name = project_name,\n",
    "               keywords = keywords,\n",
    "               es = es)\n",
    "else:\n",
    "    print('sudah ada')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3af55536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:04:01.615598Z",
     "start_time": "2025-05-23T17:03:59.330197Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data/_search [status:200 duration:2.169s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unified_issue</th>\n",
       "      <th>description</th>\n",
       "      <th>list_issue</th>\n",
       "      <th>total_posts</th>\n",
       "      <th>viral_score</th>\n",
       "      <th>reach_score</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>share_of_voice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Other</td>\n",
       "      <td>This cluster includes issues that are not dire...</td>\n",
       "      <td>[BPKH manages hajj funds effectively, BPKH man...</td>\n",
       "      <td>289</td>\n",
       "      <td>55.277300</td>\n",
       "      <td>1197.214998</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>192</td>\n",
       "      <td>42.128280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Indonesian Economic Performance and Outlook</td>\n",
       "      <td>This cluster encompasses discussions and repor...</td>\n",
       "      <td>[IHSG (Jakarta Composite Index) movement, Indo...</td>\n",
       "      <td>21</td>\n",
       "      <td>4.626327</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>3.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Danantara Initiatives and Support</td>\n",
       "      <td>This cluster focuses on Danantara's role, stra...</td>\n",
       "      <td>[Will Danantara become a successful force?, Da...</td>\n",
       "      <td>19</td>\n",
       "      <td>9.511562</td>\n",
       "      <td>137.681667</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.769679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Economic and Financial Market Updates</td>\n",
       "      <td>This cluster covers various aspects of Indones...</td>\n",
       "      <td>[IHSG (Jakarta Composite Index) movement, Indo...</td>\n",
       "      <td>17</td>\n",
       "      <td>4.135826</td>\n",
       "      <td>73.700000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2.478134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Government and State Official Activities</td>\n",
       "      <td>This cluster includes activities and statement...</td>\n",
       "      <td>[Farry Francis, President's envoy, Finance Min...</td>\n",
       "      <td>15</td>\n",
       "      <td>5.252781</td>\n",
       "      <td>64.310000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2.186589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Corporate Social Responsibility and Investment...</td>\n",
       "      <td>This cluster includes news about companies eng...</td>\n",
       "      <td>[CNBC Indonesia presents investment ideas]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092715</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>E-commerce and Logistics Challenges</td>\n",
       "      <td>This cluster focuses on challenges faced by e-...</td>\n",
       "      <td>[E-commerce players complain about high shipping]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.145773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Government Policies and Social Programs</td>\n",
       "      <td>This cluster includes discussions related to g...</td>\n",
       "      <td>[Free Nutritious Meal Program Prabowo-Gibran]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636746</td>\n",
       "      <td>4.505000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Insurance Industry Pressure</td>\n",
       "      <td>This cluster highlights the considerable press...</td>\n",
       "      <td>[Considerable pressure on the insurance industry]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.145773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Finance Ministry Plans</td>\n",
       "      <td>This cluster focuses on the Finance Ministry's...</td>\n",
       "      <td>[Finance Ministry plans 2026 budget deficit]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.145773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        unified_issue  \\\n",
       "75                                              Other   \n",
       "58        Indonesian Economic Performance and Outlook   \n",
       "28                  Danantara Initiatives and Support   \n",
       "39              Economic and Financial Market Updates   \n",
       "56           Government and State Official Activities   \n",
       "..                                                ...   \n",
       "24  Corporate Social Responsibility and Investment...   \n",
       "33                E-commerce and Logistics Challenges   \n",
       "55            Government Policies and Social Programs   \n",
       "61                        Insurance Industry Pressure   \n",
       "45                             Finance Ministry Plans   \n",
       "\n",
       "                                          description  \\\n",
       "75  This cluster includes issues that are not dire...   \n",
       "58  This cluster encompasses discussions and repor...   \n",
       "28  This cluster focuses on Danantara's role, stra...   \n",
       "39  This cluster covers various aspects of Indones...   \n",
       "56  This cluster includes activities and statement...   \n",
       "..                                                ...   \n",
       "24  This cluster includes news about companies eng...   \n",
       "33  This cluster focuses on challenges faced by e-...   \n",
       "55  This cluster includes discussions related to g...   \n",
       "61  This cluster highlights the considerable press...   \n",
       "45  This cluster focuses on the Finance Ministry's...   \n",
       "\n",
       "                                           list_issue  total_posts  \\\n",
       "75  [BPKH manages hajj funds effectively, BPKH man...          289   \n",
       "58  [IHSG (Jakarta Composite Index) movement, Indo...           21   \n",
       "28  [Will Danantara become a successful force?, Da...           19   \n",
       "39  [IHSG (Jakarta Composite Index) movement, Indo...           17   \n",
       "56  [Farry Francis, President's envoy, Finance Min...           15   \n",
       "..                                                ...          ...   \n",
       "24         [CNBC Indonesia presents investment ideas]            1   \n",
       "33  [E-commerce players complain about high shipping]            1   \n",
       "55      [Free Nutritious Meal Program Prabowo-Gibran]            1   \n",
       "61  [Considerable pressure on the insurance industry]            1   \n",
       "45       [Finance Ministry plans 2026 budget deficit]            1   \n",
       "\n",
       "    viral_score  reach_score  positive  negative  neutral  share_of_voice  \n",
       "75    55.277300  1197.214998        70        27      192       42.128280  \n",
       "58     4.626327    48.000000         3         7       11        3.061224  \n",
       "28     9.511562   137.681667         3         0       16        2.769679  \n",
       "39     4.135826    73.700000         2         5       10        2.478134  \n",
       "56     5.252781    64.310000         0         0       15        2.186589  \n",
       "..          ...          ...       ...       ...      ...             ...  \n",
       "24     0.092715     7.700000         0         0        1        0.145773  \n",
       "33     0.500000     1.550000         0         1        0        0.145773  \n",
       "55     0.636746     4.505000         0         0        1        0.145773  \n",
       "61     0.400000     0.800000         0         1        0        0.145773  \n",
       "45     0.000000     1.800000         0         0        1        0.145773  \n",
       "\n",
       "[86 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_issue = get_data_issue(\n",
    "    es = es,\n",
    "    project_name = project_name,\n",
    "    keywords=keywords,\n",
    "    search_keyword=search_keyword,\n",
    "    sentiment=sentiment,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    date_filter=date_filter,\n",
    "    custom_start_date=custom_start_date,\n",
    "    custom_end_date=custom_end_date,\n",
    "    channels=channels,\n",
    "    influence_score_min=influence_score_min,\n",
    "    influence_score_max=influence_score_max,\n",
    "    region=region,\n",
    "    language=language,\n",
    "    domain=domain,\n",
    "    limit=500\n",
    ")\n",
    "#ini yg direturn\n",
    "data_center = pd.DataFrame(topics_overview)\n",
    "data_center\n",
    "\n",
    "df_exploded = data_center.explode('list_issue')\n",
    "df_exploded = df_exploded.rename(columns={\"list_issue\": \"issue\"})\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_exploded[['issue', 'unified_issue', 'description']],\n",
    "    df_issue,\n",
    "    on='issue'\n",
    ")\n",
    "\n",
    "df_final = df_merged.groupby(['unified_issue', 'description']).agg(\n",
    "    list_issue=('issue', list),\n",
    "    total_posts=('total_post', 'sum'),\n",
    "    viral_score=('total_viral_score', 'sum'),\n",
    "    reach_score=('total_reach_score', 'sum'),\n",
    "    positive=('positive_posts', 'sum'),\n",
    "    negative=('negative_posts', 'sum'),\n",
    "    neutral=('neutral_posts', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "df_final['share_of_voice'] = (df_final['total_posts']/df_final['total_posts'].sum())*100\n",
    "df_final = df_final.sort_values('share_of_voice', ascending=False)\n",
    "\n",
    "\n",
    "results = df_final.to_dict(orient = 'records')\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd08d410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:04:14.618392Z",
     "start_time": "2025-05-23T17:04:14.610292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "686"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['total_posts'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3d56c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:03:08.187845Z",
     "start_time": "2025-05-23T17:03:08.162251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------in background --------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unified_issue</th>\n",
       "      <th>description</th>\n",
       "      <th>list_issue</th>\n",
       "      <th>uuid</th>\n",
       "      <th>project_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Economic Challenges and Discussions</td>\n",
       "      <td>This cluster focuses on economic challenges an...</td>\n",
       "      <td>[Kadin discusses economic challenges, Kadin In...</td>\n",
       "      <td>6ee868af-d771-5cd5-bc72-0205a0e993b1</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Economic Discussions and Government Policies</td>\n",
       "      <td>This cluster includes discussions from key gov...</td>\n",
       "      <td>[Minister of Energy's statement, Minister of F...</td>\n",
       "      <td>311e05e0-5c22-52c9-8baf-a01ca0241746</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efforts to achieve food self-sufficiency</td>\n",
       "      <td>This cluster focuses on efforts to achieve foo...</td>\n",
       "      <td>[Efforts to achieve food self-sufficiency, Coo...</td>\n",
       "      <td>0ca51090-fc99-53d8-b8b9-229d6dcb72fc</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Financial Institution Initiatives</td>\n",
       "      <td>This cluster focuses on initiatives by financi...</td>\n",
       "      <td>[Kiwoom Sekuritas daily research, Bank Mandiri...</td>\n",
       "      <td>5ad2018c-a298-54e2-9f68-198b63950499</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>International Cooperation and Agreements</td>\n",
       "      <td>This cluster focuses on Indonesia's cooperatio...</td>\n",
       "      <td>[Indonesia invites Japan to strengthen green e...</td>\n",
       "      <td>fd74a861-d88c-5c58-86fe-92dd3626e2b4</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>IHSG and Capital Market Updates</td>\n",
       "      <td>This cluster focuses on updates and prediction...</td>\n",
       "      <td>[IHSG movement prediction and recommendations,...</td>\n",
       "      <td>5963f3ff-81fa-5cc8-937a-0758715d3b80</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>International Relations and Visits</td>\n",
       "      <td>This cluster focuses on international relation...</td>\n",
       "      <td>[Australian PM Anthony Albanese's visit to Ind...</td>\n",
       "      <td>4c18161b-69c8-5664-bff4-a4c790d700f9</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Investment and Economic Discussions</td>\n",
       "      <td>This cluster centers on investment-related top...</td>\n",
       "      <td>[CEO of BPI Daya discusses investment, CEO of ...</td>\n",
       "      <td>5a6ee063-c5f9-5e6b-8637-e5a09b294ecc</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Other</td>\n",
       "      <td>This cluster includes issues that are not dire...</td>\n",
       "      <td>[Personal opinion article, Appreciation for lo...</td>\n",
       "      <td>3cf3a3d2-34a8-597a-8eb8-54086fbb24bb</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Other</td>\n",
       "      <td>This cluster includes issues that are not dire...</td>\n",
       "      <td>[DPR RI member comments on legal issues, BRI h...</td>\n",
       "      <td>2feb79fc-a2a7-5d6c-bcd5-febcb3b32032</td>\n",
       "      <td>danantara</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   unified_issue  \\\n",
       "0            Economic Challenges and Discussions   \n",
       "1   Economic Discussions and Government Policies   \n",
       "2       Efforts to achieve food self-sufficiency   \n",
       "3              Financial Institution Initiatives   \n",
       "4       International Cooperation and Agreements   \n",
       "..                                           ...   \n",
       "84               IHSG and Capital Market Updates   \n",
       "85            International Relations and Visits   \n",
       "86           Investment and Economic Discussions   \n",
       "87                                         Other   \n",
       "88                                         Other   \n",
       "\n",
       "                                          description  \\\n",
       "0   This cluster focuses on economic challenges an...   \n",
       "1   This cluster includes discussions from key gov...   \n",
       "2   This cluster focuses on efforts to achieve foo...   \n",
       "3   This cluster focuses on initiatives by financi...   \n",
       "4   This cluster focuses on Indonesia's cooperatio...   \n",
       "..                                                ...   \n",
       "84  This cluster focuses on updates and prediction...   \n",
       "85  This cluster focuses on international relation...   \n",
       "86  This cluster centers on investment-related top...   \n",
       "87  This cluster includes issues that are not dire...   \n",
       "88  This cluster includes issues that are not dire...   \n",
       "\n",
       "                                           list_issue  \\\n",
       "0   [Kadin discusses economic challenges, Kadin In...   \n",
       "1   [Minister of Energy's statement, Minister of F...   \n",
       "2   [Efforts to achieve food self-sufficiency, Coo...   \n",
       "3   [Kiwoom Sekuritas daily research, Bank Mandiri...   \n",
       "4   [Indonesia invites Japan to strengthen green e...   \n",
       "..                                                ...   \n",
       "84  [IHSG movement prediction and recommendations,...   \n",
       "85  [Australian PM Anthony Albanese's visit to Ind...   \n",
       "86  [CEO of BPI Daya discusses investment, CEO of ...   \n",
       "87  [Personal opinion article, Appreciation for lo...   \n",
       "88  [DPR RI member comments on legal issues, BRI h...   \n",
       "\n",
       "                                    uuid project_name  \n",
       "0   6ee868af-d771-5cd5-bc72-0205a0e993b1    danantara  \n",
       "1   311e05e0-5c22-52c9-8baf-a01ca0241746    danantara  \n",
       "2   0ca51090-fc99-53d8-b8b9-229d6dcb72fc    danantara  \n",
       "3   5ad2018c-a298-54e2-9f68-198b63950499    danantara  \n",
       "4   fd74a861-d88c-5c58-86fe-92dd3626e2b4    danantara  \n",
       "..                                   ...          ...  \n",
       "84  5963f3ff-81fa-5cc8-937a-0758715d3b80    danantara  \n",
       "85  4c18161b-69c8-5664-bff4-a4c790d700f9    danantara  \n",
       "86  5a6ee063-c5f9-5e6b-8637-e5a09b294ecc    danantara  \n",
       "87  3cf3a3d2-34a8-597a-8eb8-54086fbb24bb    danantara  \n",
       "88  2feb79fc-a2a7-5d6c-bcd5-febcb3b32032    danantara  \n",
       "\n",
       "[89 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('--------------in background --------------')\n",
    "data_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8463995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:03:09.836274Z",
     "start_time": "2025-05-23T17:03:09.829447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 10) 66 234\n"
     ]
    }
   ],
   "source": [
    "print(df_final.shape, df_final['unified_issue'].nunique(),len(set([j for i in df_final['list_issue'] for j in i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76edba1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T17:03:40.802709Z",
     "start_time": "2025-05-23T17:03:16.590423Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIBACKGROUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics_overview/_mget?_source=false [status:200 duration:0.042s]\n",
      "INFO:utils.topics.es_operations:Found 88 existing documents out of 102\n",
      "INFO:utils.topics.es_operations:Preparing to upsert 102 documents in topics_overview\n",
      "INFO:utils.topics.es_operations:Processing chunk 1/1 (102 documents, {'update': 88, 'index': 14})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 3) 101 692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.295s]\n",
      "INFO:utils.topics.es_operations:Chunk processed successfully: 102 documents\n",
      "INFO:utils.topics.es_operations:Upsert complete: 88 updated, 14 created, 0 failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 88 documents updated, 14 documents created, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics_overview/_mget?_source=false [status:200 duration:0.058s]\n",
      "INFO:utils.topics.es_operations:Found 100 existing documents out of 103\n",
      "INFO:utils.topics.es_operations:Preparing to upsert 103 documents in topics_overview\n",
      "INFO:utils.topics.es_operations:Processing chunk 1/1 (103 documents, {'update': 100, 'index': 3})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 3) 102 788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.245s]\n",
      "INFO:utils.topics.es_operations:Chunk processed successfully: 103 documents\n",
      "INFO:utils.topics.es_operations:Upsert complete: 100 updated, 3 created, 0 failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 100 documents updated, 3 documents created, 0 errors\n"
     ]
    }
   ],
   "source": [
    "#--------------- sisa df_final di background ----------------\n",
    "print('DIBACKGROUND')\n",
    "df_final = pd.DataFrame(topics_overview)\n",
    "\n",
    "\n",
    "loop=2\n",
    "for i in range(loop):\n",
    "    suggestion_unified_issue = df_final[['unified_issue','description']].to_dict(orient = 'records')\n",
    "    df_issue = df_issue[~df_issue['issue'].isin([j for i in df_final['list_issue'] for j in i])]\n",
    "    if df_issue.empty:\n",
    "        break\n",
    "    df_final2 = regenerate_topics(suggestion_unified_issue,df_issue)\n",
    "    df_final = pd.concat([df_final, df_final2])\n",
    "    df_final = df_final.groupby(['unified_issue']).agg(\n",
    "            description=('description', 'max'),\n",
    "            list_issue=('list_issue', lambda s: [j for i in s for j in i if not pd.isna(j)])).reset_index()\n",
    "    df_final = split_rows_by_list_length(df_final)\n",
    "\n",
    "    print(df_final.shape, df_final['unified_issue'].nunique(),len(set([j for i in df_final['list_issue'] for j in i])))\n",
    "    results = df_final.to_dict(orient = 'records')\n",
    "    ingest_topic(results, project_name,es = es)\n",
    "# --------------------- --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667eb8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:14:26.587341Z",
     "start_time": "2025-05-24T16:14:26.581694Z"
    }
   },
   "source": [
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31fdfd1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:33:55.255500Z",
     "start_time": "2025-05-26T14:33:55.226332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'unified_issue': 'Banking and Financial Institution Activities',\n",
       "  'description': 'Performance, strategies, and business activities of various banks and financial institutions in Indonesia.',\n",
       "  'list_issue': [\"Microfinance bank association members' performance\",\n",
       "   'Morning Gymnastics at Bank Indonesia Kediri'],\n",
       "  'total_posts': 291,\n",
       "  'viral_score': 71.81342340121046,\n",
       "  'reach_score': 800.3937775007216,\n",
       "  'positive': 149,\n",
       "  'negative': 7,\n",
       "  'neutral': 135,\n",
       "  'share_of_voice': 5.752124925874679},)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "{\n",
    "        \"unified_issue\": \"Banking and Financial Institution Activities\",\n",
    "        \"description\": \"Performance, strategies, and business activities of various banks and financial institutions in Indonesia.\",\n",
    "        \"list_issue\": [\n",
    "            \"Microfinance bank association members' performance\",\n",
    "            \"Morning Gymnastics at Bank Indonesia Kediri\"\n",
    "        ],\n",
    "        \"total_posts\": 291,\n",
    "        \"viral_score\": 71.81342340121046,\n",
    "        \"reach_score\": 800.3937775007216,\n",
    "        \"positive\": 149,\n",
    "        \"negative\": 7,\n",
    "        \"neutral\": 135,\n",
    "        \"share_of_voice\": 5.752124925874679\n",
    "    },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f26cd12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:42:46.717619Z",
     "start_time": "2025-05-26T14:42:46.709102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = ['tiktok','instagram','news','reddit','facebook','twitter','linkedin','youtube']\n",
    "\n",
    "index = ','.join([i+'_data' for i in channels])\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d296658",
   "metadata": {},
   "source": [
    "# scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ed5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "      {\n",
    "        \"_source\": {\n",
    "          \"unified_issue\": \"BPKH and Indonesian Finance\",\n",
    "          \"description\": \"This cluster focuses on BPKH's role in Indonesian finance. Sentiment is generally neutral, reporting on financial activities.\",\n",
    "          \"list_issue\": [\n",
    "            \"BPKH head on Indonesian Finance\",\n",
    "            \"BPKH manages hajj funds effectively\"\n",
    "          ],\n",
    "          \"project_name\": \"danantara\"\n",
    "        }\n",
    "      },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "06d8f6af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:59:42.625710Z",
     "start_time": "2025-05-26T16:59:42.542638Z"
    },
    "code_folding": [
     5,
     42,
     85,
     90,
     183,
     271,
     323
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "class About_MySQL:\n",
    "    def __init__(self):\n",
    "        \n",
    "        db_host = os.getenv('DB_HOST')\n",
    "        db_port = os.getenv('DB_PORT')\n",
    "        db_user = os.getenv('DB_USER') \n",
    "        db_password = os.getenv('DB_PASSWORD') \n",
    "        db_name = os.getenv('DB_NAME')\n",
    "        # Format URL koneksi ke MySQL\n",
    "        self.db_url = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "        \n",
    "        # Membuat SQLAlchemy engine\n",
    "        self.engine = create_engine(\n",
    "            self.db_url,\n",
    "            pool_pre_ping=True,   # Auto reconnect if connection lost\n",
    "            pool_recycle=3600     # Recycle connections every hour\n",
    "        )\n",
    "\n",
    "    def to_pull_data(self, query):\n",
    "        # Menjalankan query dan mengembalikan hasil sebagai DataFrame\n",
    "        with self.engine.connect() as connection:\n",
    "            df = pd.read_sql(text(query), connection)\n",
    "        return df\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from utils.gemini import call_gemini\n",
    "from utils.topics.es_operations import upsert_documents\n",
    "from utils.script_score import script_score\n",
    "\n",
    "ES_HOST = os.getenv('ES_HOST','http://34.101.178.71:9200')\n",
    "ES_USERNAME = os.getenv('ES_USERNAME','elastic')\n",
    "ES_PASSWORD = os.getenv('ES_PASSWORD','elasticpassword')\n",
    "# Inisialisasi client\n",
    "es = Elasticsearch(hosts=ES_HOST, http_auth = (ES_USERNAME, ES_PASSWORD))\n",
    "    \n",
    "def parse_elasticsearch_response_to_dataframe(es_response):\n",
    "    \"\"\"\n",
    "    Parse Elasticsearch aggregation response to pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Extract buckets from the response\n",
    "    buckets = es_response['aggregations']['by_issue']['buckets']\n",
    "    \n",
    "    # Initialize list to store parsed data\n",
    "    parsed_data = []\n",
    "    \n",
    "    for bucket in buckets:\n",
    "        # Extract basic information\n",
    "        issue = bucket['key']\n",
    "        doc_count = bucket['doc_count']\n",
    "        avg_influence_score = bucket['avg_influence_score']['value']\n",
    "        \n",
    "        # Extract sample posts information\n",
    "        sample_posts = []\n",
    "        post_captions = []\n",
    "        link_posts = []\n",
    "        \n",
    "        if 'sample_posts' in bucket and bucket['sample_posts']['hits']['hits']:\n",
    "            for hit in bucket['sample_posts']['hits']['hits']:\n",
    "                source = hit['_source']\n",
    "                post_captions.append(source.get('post_caption', ''))\n",
    "                link_posts.append(source.get('link_post', ''))\n",
    "                sample_posts.append({\n",
    "                    'post_caption': source.get('post_caption', '')\n",
    "                })\n",
    "        \n",
    "        # Create record for this issue\n",
    "        record = {\n",
    "            'issue': issue,\n",
    "            'sample_posts_raw': sample_posts  # Keep raw data if needed\n",
    "        }\n",
    "        \n",
    "        parsed_data.append(record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def ingest_data_to_es(\n",
    "    es,\n",
    "    index_name,\n",
    "    data,\n",
    "    id_field = None\n",
    "):\n",
    "\n",
    "    def gen_actions():\n",
    "        for doc in data:\n",
    "            action = {\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": doc\n",
    "            }\n",
    "            if id_field and id_field in doc:\n",
    "                action[\"_id\"] = doc[id_field]\n",
    "            yield action\n",
    "    \n",
    "    try:\n",
    "        helpers.bulk(es, gen_actions())\n",
    "        print(f\"Berhasil ingest {len(data)} dokumen ke index '{index_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error ingest data ke Elasticsearch: {e}\")\n",
    "        \n",
    "def get_data_post(list_issue, keywords):\n",
    "    \n",
    "\n",
    "    BOOL = {\n",
    "           \n",
    "           \"must\": [\n",
    "               \n",
    "               {\"bool\":{\n",
    "                               \n",
    "                   \"should\":[\n",
    "                        {\n",
    "                          \"match\": {\n",
    "                            \"post_caption\": {\n",
    "                              \"query\": i,\n",
    "                              \"operator\": \"AND\"\n",
    "                            }\n",
    "                          }\n",
    "                        } for i in keywords\n",
    "                      ]\n",
    "                   \n",
    "               }}\n",
    "           ]\n",
    "       } \n",
    "    \n",
    " \n",
    "    if list_issue:\n",
    "        BOOL['must_not'] = [{\n",
    "                              \"terms\": {\n",
    "                                \"issue.keyword\": list_issue[:1000]\n",
    "                              }\n",
    "                            }]\n",
    "    \n",
    "    \n",
    "    query = {\n",
    "      \"size\":0,\n",
    "      \"query\": {\n",
    "       \"bool\":BOOL\n",
    "      }, \n",
    "      \"aggs\": {\n",
    "        \"by_issue\": {\n",
    "          \"terms\": {\n",
    "            \"field\": \"issue.keyword\",\n",
    "            \"size\": 1000,\n",
    "            \"order\": {\n",
    "              \"avg_influence_score\": \"desc\"\n",
    "            }\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"avg_influence_score\": {\n",
    "              \"avg\": {\n",
    "                \"script\": script_score\n",
    "              }\n",
    "            },\n",
    "            \"sample_posts\": {\n",
    "              \"top_hits\": {\n",
    "                \"size\": 2,\n",
    "                \"_source\": {\n",
    "                  \"includes\": [\n",
    "                    \"post_caption\"\n",
    "                  ]\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    channels = ['tiktok','instagram','news','reddit','facebook','twitter','linkedin','youtube']\n",
    "\n",
    "    index = ','.join([i+'_data' for i in channels])\n",
    "\n",
    "    data_topics = es.search(index = index, body = query)\n",
    "\n",
    "    df_issue = parse_elasticsearch_response_to_dataframe(data_topics).reset_index()\n",
    "    return df_issue\n",
    "\n",
    "def parse_topics_overview_to_dataframe(es_response):\n",
    "    \"\"\"\n",
    "    Parse Elasticsearch topics overview response menjadi pandas DataFrame\n",
    "    Diurutkan berdasarkan jumlah unique issues (terbanyak ke paling sedikit)\n",
    "    \n",
    "    Args:\n",
    "        es_response: Response dari Elasticsearch query\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame dengan kolom unified_issue, description, list_issue, dll\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validasi response\n",
    "    if not es_response or 'aggregations' not in es_response:\n",
    "        print(\"âŒ No aggregations found in Elasticsearch response\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract buckets\n",
    "    buckets = es_response.get('aggregations', {}).get('by_unified_issue', {}).get('buckets', [])\n",
    "    \n",
    "    if not buckets:\n",
    "        print(\"âŒ No buckets found in aggregation response\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"âœ… Found {len(buckets)} unified issues to process\")\n",
    "    \n",
    "    # Parse setiap bucket\n",
    "    parsed_data = []\n",
    "    \n",
    "    for idx, bucket in enumerate(buckets):\n",
    "        try:\n",
    "            # Basic information\n",
    "            unified_issue = bucket.get('key', 'Unknown')\n",
    "            doc_count = bucket.get('doc_count', 0)\n",
    "            \n",
    "            # Get description dari sample\n",
    "            description = \"\"\n",
    "            sample_desc = bucket.get('sample_description', {}).get('hits', {}).get('hits', [])\n",
    "            if sample_desc:\n",
    "                description = sample_desc[0].get('_source', {}).get('description', '')\n",
    "            \n",
    "            # Get unique list issues\n",
    "            unique_issues_buckets = bucket.get('unique_list_issues', {}).get('buckets', [])\n",
    "            list_issues = [issue_bucket.get('key', '') for issue_bucket in unique_issues_buckets]\n",
    "            \n",
    "            # Get total unique issues count\n",
    "            total_unique_count = bucket.get('total_issues_count', {}).get('value', 0)\n",
    "            \n",
    "            # Create record\n",
    "            record = {\n",
    "                'rank': idx + 1,  # Ranking berdasarkan jumlah issues\n",
    "                'unified_issue': unified_issue,\n",
    "                'description': description,\n",
    "                'list_issue': list_issues,\n",
    "                'unique_issues_count': total_unique_count,\n",
    "                'total_documents': doc_count,\n",
    "                'issues_preview': ', '.join(list_issues[:3]) + ('...' if len(list_issues) > 3 else ''),\n",
    "                'description_preview': description[:100] + '...' if len(description) > 100 else description\n",
    "            }\n",
    "            \n",
    "            parsed_data.append(record)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error parsing bucket {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not parsed_data:\n",
    "        print(\"âŒ No valid data parsed from response\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(parsed_data)\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    df['unique_issues_count'] = pd.to_numeric(df['unique_issues_count'], errors='coerce').fillna(0).astype(int)\n",
    "    df['total_documents'] = pd.to_numeric(df['total_documents'], errors='coerce').fillna(0).astype(int)\n",
    "    df['rank'] = df['rank'].astype(int)\n",
    "    \n",
    "    # Sort by unique_issues_count descending (should already be sorted from ES, but double check)\n",
    "    df = df.sort_values('unique_issues_count', ascending=False).reset_index(drop=True)\n",
    "    df['rank'] = range(1, len(df) + 1)  # Update rank after sorting\n",
    "    \n",
    "    print(f\"âœ… Successfully parsed {len(df)} unified issues\")\n",
    "    print(f\"ðŸ“Š Total unique issues: {df['unique_issues_count'].sum()}\")\n",
    "    print(f\"ðŸ“„ Total documents: {df['total_documents'].sum()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_topics_map(project_name):\n",
    "    query = {\n",
    "      \"size\": 0,\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"project_name.keyword\": project_name\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"by_unified_issue\": {\n",
    "          \"terms\": {\n",
    "            \"field\": \"unified_issue.keyword\",\n",
    "            \"size\": 100,\n",
    "            \"order\": {\n",
    "              \"total_issues_count\": \"desc\"\n",
    "            }\n",
    "          },\n",
    "          \"aggs\": {\n",
    "            \"sample_description\": {\n",
    "              \"top_hits\": {\n",
    "                \"size\": 1,\n",
    "                \"_source\": {\n",
    "                  \"includes\": [\"description\"]\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"unique_list_issues\": {\n",
    "              \"terms\": {\n",
    "                \"field\": \"list_issue.keyword\",\n",
    "                \"size\": 10000\n",
    "              }\n",
    "            },\n",
    "            \"total_issues_count\": {\n",
    "              \"cardinality\": {\n",
    "                \"field\": \"list_issue.keyword\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    list_issue = es.search(index = 'topics-overview',body = query)\n",
    "\n",
    "    topics_map = parse_topics_overview_to_dataframe(list_issue)\n",
    "    return topics_map\n",
    "\n",
    "def process_issue(project_name, df_issue, issue_map):\n",
    "    if issue_map:\n",
    "        issue_map = f\"\"\"\n",
    "    **OPTION NAMA KELOMPOK**\n",
    "    {issue_map}\n",
    "        \n",
    "        \"\"\"\n",
    "    else:\n",
    "        issue_map = ''\n",
    "    \n",
    "    prompt = f\"\"\"Anda adalah ahli analisis data media sosial yang berspesialisasi dalam pengelompokan konten dan analisis sentimen.\n",
    "\n",
    "    **TUGAS:**\n",
    "    Analisis issue media sosial berikut yang terkait dengan topik \"{project_name.upper()}\" dan kelompokkan menjadi cluster yang bermakna berdasarkan kesamaan tema, pola sentimen, dan relevansi konten.\n",
    "\n",
    "    **DATA YANG DIANALISIS:**\n",
    "    {df_issue.to_dict(orient = 'records')[:500]}\n",
    "\n",
    "    **ATURAN PENGELOMPOKAN:**\n",
    "    1. **Pengelompokan Tematik**: Gabungkan issue dengan topik, kekhawatiran, atau tema yang serupa\n",
    "    2. **Filter Relevansi**: Issue yang tidak terkait dengan \"{project_name.upper()}\" â†’ masukkan ke kategori \"Other\"\n",
    "    3. **Pertimbangan Pengaruh**: Skor pengaruh yang tinggi menunjukkan issue yang lebih penting\n",
    "    4. **Penugasan Unik**: Setiap issue hanya boleh masuk ke SATU cluster saja\n",
    "    5. **Cluster Optimal**: Buat maksimal 8 cluster untuk kejelasan\n",
    "    6. **Option Nama Kelompok**: Gunakan Option ini sebagai option dari penamaan kelompok, jika tidak match maka buat nama kelompok baru\n",
    "\n",
    "    {issue_map}\n",
    " \n",
    "\n",
    "\n",
    "    **FORMAT OUTPUT:**\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama cluster 1 yang jelas dan deskriptif (2-10 kata)\",\n",
    "        \"description\": \"Ringkasan komprehensif yang mencakup: tema utama, analisis sentimen, potensi dampak, dan kekhawatiran utama dalam cluster ini\",\n",
    "        \"list_issue_id\": [\"List Issue Id yang masuk ke cluster ini\"]\n",
    "      }},\n",
    "      {{\n",
    "        \"unified_issue\": \"Nama cluster 2yang jelas dan deskriptif (2-10 kata)\",\n",
    "        \"description\": \"Ringkasan komprehensif yang mencakup: tema utama, analisis sentimen, potensi dampak, dan kekhawatiran utama dalam cluster ini\",\n",
    "        \"list_issue_id\": [\"List Issue Id yang masuk ke cluster ini\"]\n",
    "      }}, ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    **PANDUAN ANALISIS:**\n",
    "    - **Nama Cluster**: Gunakan terminologi yang jelas dan profesional dalam bahasa Indonesia\n",
    "    - **Deskripsi**: Sertakan analisis sentimen, potensi risiko, dan ringkasan tematik (50-100 kata)\n",
    "    - **Relevansi**: Saring ketat konten yang tidak terkait {project_name.upper()} masuk ke \"Other\"\n",
    "    - **Option Nama Kelompok**: Gunakan Option ini sebagai option dari penamaan kelompok, jika tidak match maka buat nama kelompok baru\n",
    "\n",
    "\n",
    "    **PENTING:** \n",
    "    Berikan HANYA output JSON tanpa penjelasan tambahan, komentar, atau teks lainnya. Pastikan JSON valid dan dapat di-parse langsung.\n",
    "\n",
    "    Mulai analisis sekarang:\"\"\"\n",
    "\n",
    "    prediction = call_gemini(prompt)\n",
    "\n",
    "    try:\n",
    "        data_center = pd.DataFrame(eval(re.findall(r'\\[.*\\]', prediction, flags=re.I|re.S)[0]))\n",
    "    except:\n",
    "        unified_issue = [i.strip('\\n ,\"') for i in re.findall(r'unified_issue[\\\"\\,\\s\\:]+(.*?)description',prediction, flags=re.I|re.S)]\n",
    "        description = [i.strip('\\n ,\"') for i in re.findall(r'description[\\\"\\,\\s\\:]+(.*?)list_issue_id',prediction, flags=re.I|re.S)]\n",
    "        list_issue_id = [eval(i.strip('\\n ,\"]')+'\"]') for i in re.findall(r'list_issue_id[\\\"\\,\\s\\:]+(.*?)(?:\\}|$)',prediction, flags=re.I|re.S)]\n",
    "        data = []\n",
    "        for u,d,l in zip(unified_issue, description, list_issue_id):\n",
    "            data.append({'unified_issue':u, 'description':d,'list_issue_id':l})\n",
    "        data_center = pd.DataFrame(data)\n",
    "\n",
    "    df_exploded = data_center.explode('list_issue_id')\n",
    "    df_exploded = df_exploded.rename(columns={\"list_issue_id\": \"index\"})\n",
    "    df_exploded['index'] = df_exploded['index'].astype(str)\n",
    "    df_merged = pd.merge(\n",
    "        df_exploded[['index', 'unified_issue', 'description']],\n",
    "        df_issue,\n",
    "        on='index'\n",
    "    )\n",
    "\n",
    "    df_final = df_merged.groupby(['unified_issue', 'description']).agg(\n",
    "        list_issue=('issue', list)\n",
    "    ).reset_index()\n",
    "    df_final['project_name'] = project_name\n",
    "\n",
    "    data_ingest = df_final.to_dict(orient = 'records')\n",
    "\n",
    "    ingest_data_to_es(es, 'topics-overview',data = data_ingest)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a155c788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:54:15.114481Z",
     "start_time": "2025-05-26T14:54:14.557269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MySQL = About_MySQL()\n",
    "df = MySQL.to_pull_data('select project_name, relevan_keyword from keyword_projects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f661222a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T14:54:15.130300Z",
     "start_time": "2025-05-26T14:54:15.115873Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['project_name'] = df['project_name'].str.lower()\n",
    "df = df.groupby('project_name').agg({'relevan_keyword':lambda s: list(set([i.lower() for i in s]))}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7393bf81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:50:03.853100Z",
     "start_time": "2025-05-26T16:47:22.076775Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.140s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.097s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 13 unified issues to process\n",
      "âœ… Successfully parsed 13 unified issues\n",
      "ðŸ“Š Total unique issues: 1099\n",
      "ðŸ“„ Total documents: 27\n",
      "âœ… Found 13 unified issues to process\n",
      "âœ… Successfully parsed 13 unified issues\n",
      "ðŸ“Š Total unique issues: 1002\n",
      "ðŸ“„ Total documents: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.096s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.103s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.102s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 11 unified issues to process\n",
      "âœ… Successfully parsed 11 unified issues\n",
      "ðŸ“Š Total unique issues: 1044\n",
      "ðŸ“„ Total documents: 19\n",
      "âœ… Found 13 unified issues to process\n",
      "âœ… Successfully parsed 13 unified issues\n",
      "ðŸ“Š Total unique issues: 1093\n",
      "ðŸ“„ Total documents: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.099s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 10 unified issues to process\n",
      "âœ… Successfully parsed 10 unified issues\n",
      "ðŸ“Š Total unique issues: 1078\n",
      "ðŸ“„ Total documents: 22\n",
      "âœ… Found 12 unified issues to process\n",
      "âœ… Successfully parsed 12 unified issues\n",
      "ðŸ“Š Total unique issues: 1077\n",
      "ðŸ“„ Total documents: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.097s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.094s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 13 unified issues to process\n",
      "âœ… Successfully parsed 13 unified issues\n",
      "ðŸ“Š Total unique issues: 1001\n",
      "ðŸ“„ Total documents: 15\n",
      "âœ… Found 11 unified issues to process\n",
      "âœ… Successfully parsed 11 unified issues\n",
      "ðŸ“Š Total unique issues: 1002\n",
      "ðŸ“„ Total documents: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.107s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 8 unified issues to process\n",
      "âœ… Successfully parsed 8 unified issues\n",
      "ðŸ“Š Total unique issues: 1026\n",
      "ðŸ“„ Total documents: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.219s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 12 unified issues to process\n",
      "âœ… Successfully parsed 12 unified issues\n",
      "ðŸ“Š Total unique issues: 1390\n",
      "ðŸ“„ Total documents: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.228s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.176s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 9 unified issues to process\n",
      "âœ… Successfully parsed 9 unified issues\n",
      "ðŸ“Š Total unique issues: 1016\n",
      "ðŸ“„ Total documents: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.160s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 15 unified issues to process\n",
      "âœ… Successfully parsed 15 unified issues\n",
      "ðŸ“Š Total unique issues: 1207\n",
      "ðŸ“„ Total documents: 21\n",
      "âœ… Found 13 unified issues to process\n",
      "âœ… Successfully parsed 13 unified issues\n",
      "ðŸ“Š Total unique issues: 1298\n",
      "ðŸ“„ Total documents: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.163s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.148s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 13 unified issues to process\n",
      "âœ… Successfully parsed 13 unified issues\n",
      "ðŸ“Š Total unique issues: 1038\n",
      "ðŸ“„ Total documents: 22\n",
      "âœ… Found 19 unified issues to process\n",
      "âœ… Successfully parsed 19 unified issues\n",
      "ðŸ“Š Total unique issues: 1019\n",
      "ðŸ“„ Total documents: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.121s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.148s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 10 unified issues to process\n",
      "âœ… Successfully parsed 10 unified issues\n",
      "ðŸ“Š Total unique issues: 1260\n",
      "ðŸ“„ Total documents: 18\n",
      "âœ… Found 14 unified issues to process\n",
      "âœ… Successfully parsed 14 unified issues\n",
      "ðŸ“Š Total unique issues: 1036\n",
      "ðŸ“„ Total documents: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.081s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.016s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 9 unified issues to process\n",
      "âœ… Successfully parsed 9 unified issues\n",
      "ðŸ“Š Total unique issues: 573\n",
      "ðŸ“„ Total documents: 9\n",
      "âŒ No buckets found in aggregation response\n",
      "----- sandiaga uno -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data/_search [status:200 duration:4.709s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process (1000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.090s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 6 dokumen ke index 'topics-overview'\n",
      "mapped 6 kelompok 960\n",
      "process (40, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.036s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.035s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 8 dokumen ke index 'topics-overview'\n",
      "mapped 8 kelompok 40\n",
      "âŒ No buckets found in aggregation response\n",
      "----- testing -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data/_search [status:200 duration:3.483s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process (1000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.076s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 8 dokumen ke index 'topics-overview'\n",
      "mapped 8 kelompok 494\n",
      "process (506, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.152s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 6 dokumen ke index 'topics-overview'\n",
      "mapped 6 kelompok 435\n",
      "process (71, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.091s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 8 dokumen ke index 'topics-overview'\n",
      "mapped 8 kelompok 38\n",
      "process (33, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.039s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 7 dokumen ke index 'topics-overview'\n",
      "mapped 7 kelompok 32\n",
      "process (1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.016s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.043s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 1 dokumen ke index 'topics-overview'\n",
      "mapped 1 kelompok 1\n",
      "âŒ No buckets found in aggregation response\n",
      "----- testing32 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data/_search [status:200 duration:4.537s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process (1000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.100s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 2 dokumen ke index 'topics-overview'\n",
      "mapped 2 kelompok 811\n",
      "process (189, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.048s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 8 dokumen ke index 'topics-overview'\n",
      "mapped 8 kelompok 183\n",
      "process (6, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.024s]\n",
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.126s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 4 dokumen ke index 'topics-overview'\n",
      "mapped 4 kelompok 6\n",
      "âœ… Found 18 unified issues to process\n",
      "âœ… Successfully parsed 18 unified issues\n",
      "ðŸ“Š Total unique issues: 1006\n",
      "ðŸ“„ Total documents: 23\n"
     ]
    }
   ],
   "source": [
    "for _,i in df.iterrows():\n",
    "    project_name = i['project_name']\n",
    "    keywords = i['relevan_keyword']\n",
    "    if project_name not in keywords:\n",
    "        keywords.append(project_name)\n",
    "        \n",
    "    #check project baru atau lama\n",
    "    topics_map = get_topics_map(project_name)\n",
    "    #PROCESS HERE\n",
    "    #-------------------------------    \n",
    "    if topics_map.empty:\n",
    "        print(\">>>> New Project\")\n",
    "        print('-----',project_name,'-----')\n",
    "        df_issue = get_data_post([],keywords)\n",
    "        df_issue['index'] = df_issue['index'].astype(str)\n",
    "\n",
    "        issue_map = []\n",
    "        while True:\n",
    "            print('process', df_issue.shape)\n",
    "            df_final = process_issue(project_name,df_issue, issue_map)\n",
    "            print('mapped', df_final.shape[0],'kelompok',len(set([j for i in df_final['list_issue'] for j in i])))\n",
    "            df_issue = df_issue[~df_issue['issue'].isin([j for i in df_final['list_issue'] for j in i])]\n",
    "            if df_issue.empty:\n",
    "                break\n",
    "            issue_map.extend(df_final[['unified_issue','description']].to_dict(orient = 'records'))\n",
    "    else:\n",
    "        print(\"Append Data <<<<<<\")\n",
    "        print('-----',project_name,'-----')\n",
    "        issue_map = topics_map[['unified_issue','description']].to_dict(orient = 'records')\n",
    "        list_issue_done = list(set([j for i in topics_map['list_issue'] for j in i]))\n",
    "        \n",
    "        df_issue = get_data_post(list_issue_done,keywords)\n",
    "        df_issue['index'] = df_issue['index'].astype(str)\n",
    "\n",
    "        while True:\n",
    "            print('process', df_issue.shape)\n",
    "            df_final = process_issue(project_name,df_issue, issue_map)\n",
    "            print('mapped', df_final.shape[0],'kelompok',len(set([j for i in df_final['list_issue'] for j in i])))\n",
    "            df_issue = df_issue[~df_issue['issue'].isin([j for i in df_final['list_issue'] for j in i])]\n",
    "            if df_issue.empty:\n",
    "                break\n",
    "            issue_map.extend(df_final[['unified_issue','description']].to_dict(orient = 'records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd9872",
   "metadata": {},
   "source": [
    "# jika sudah ada?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f0a80",
   "metadata": {},
   "source": [
    "ambil data di ES yg udah ke map dulu\n",
    "\n",
    "\n",
    "ambil data dari ES yg belum termapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "48212348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:52:17.810234Z",
     "start_time": "2025-05-26T16:52:17.713628Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/topics-overview/_search [status:200 duration:0.087s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 18 unified issues to process\n",
      "âœ… Successfully parsed 18 unified issues\n",
      "ðŸ“Š Total unique issues: 1006\n",
      "ðŸ“„ Total documents: 23\n"
     ]
    }
   ],
   "source": [
    "topics_map = get_topics_map(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d5e88710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:52:34.141135Z",
     "start_time": "2025-05-26T16:52:34.134500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "efe1de2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:52:37.140600Z",
     "start_time": "2025-05-26T16:52:37.134314Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'unified_issue': 'Vaksin COVID-19: Perkembangan, Efek, dan Respons Pemerintah',\n",
       "  'description': 'Cluster ini membahas vaksin COVID-19, termasuk efikasi, efek samping, dan respons pemerintah terhadap pandemi. Sentimennya beragam, dari dukungan terhadap vaksinasi hingga kekhawatiran tentang efektivitas dan keamanan. Dampaknya adalah potensi peningkatan kekebalan kelompok dan pengendalian pandemi, namun ada risiko distrust dan polarisasi.'},\n",
       " {'unified_issue': 'Vaksinasi TBC: Uji Coba, Keamanan, dan Efektivitas di Indonesia',\n",
       "  'description': \"Cluster ini berfokus pada uji coba vaksin TBC di Indonesia, khususnya yang terkait dengan Bill Gates dan Gates Foundation. Sentimennya beragam, dari dukungan terhadap potensi manfaat uji coba hingga kekhawatiran tentang keamanan dan Indonesia menjadi 'kelinci percobaan'. Dampak potensialnya adalah peningkatan akses vaksin TBC dan data genetik, namun ada risiko distrust dan eksploitasi.\"},\n",
       " {'unified_issue': 'Lain-lain',\n",
       "  'description': 'Cluster ini mencakup isu-isu yang tidak secara langsung terkait dengan vaksin, atau terlalu beragam untuk dikelompokkan ke dalam cluster lain. Isu-isu ini mencakup album musik, acara hiburan, dan topik yang tidak relevan. Sentimennya bervariasi, tergantung pada isu spesifik. Dampaknya minimal terhadap topik utama vaksin.'},\n",
       " {'unified_issue': 'Vaksin TBC: Uji Coba, Keamanan, dan Kontroversi',\n",
       "  'description': \"Cluster ini berfokus pada uji coba vaksin TBC di Indonesia, khususnya yang terkait dengan Bill Gates dan Gates Foundation. Sentimennya beragam, dari dukungan terhadap potensi manfaat uji coba hingga kekhawatiran tentang keamanan, motif tersembunyi, dan Indonesia menjadi 'kelinci percobaan'. Dampak potensialnya adalah peningkatan akses vaksin TBC, namun ada risiko distrust dan penolakan.\"},\n",
       " {'unified_issue': 'Imunisasi dan Vaksinasi pada Bayi dan Anak-Anak',\n",
       "  'description': 'Cluster ini berfokus pada imunisasi rutin dan vaksinasi pada bayi dan anak-anak, termasuk polio, DPT, dan HPV. Sentimennya umumnya positif, menekankan pentingnya pencegahan penyakit dan kesehatan anak. Dampaknya adalah peningkatan cakupan imunisasi dan perlindungan terhadap penyakit menular pada anak-anak.'},\n",
       " {'unified_issue': 'Imunisasi dan Vaksinasi Rutin: Kesehatan Anak & Ibu',\n",
       "  'description': 'Cluster ini berfokus pada pentingnya imunisasi dan vaksinasi untuk anak-anak dan ibu hamil, mencakup berbagai penyakit seperti rabies, tetanus, varisela, dan influenza. Sentimennya umumnya positif, menekankan manfaat vaksin dalam mencegah penyakit dan melindungi kesehatan. Dampaknya adalah peningkatan kesadaran dan partisipasi dalam program imunisasi.'},\n",
       " {'unified_issue': 'Keamanan dan Efek Samping Vaksin: Kekhawatiran dan Klarifikasi',\n",
       "  'description': 'Cluster ini membahas kekhawatiran tentang keamanan vaksin secara umum, termasuk potensi efek samping, penyakit bawaan, dan penolakan vaksin. Sentimennya cenderung negatif atau netral, dengan fokus pada risiko dan kebutuhan akan informasi yang jelas. Dampaknya adalah potensi penurunan kepercayaan publik terhadap vaksinasi jika kekhawatiran tidak ditangani dengan baik.'},\n",
       " {'unified_issue': 'Vaksinasi Hewan: Kesehatan Ternak dan Hewan Peliharaan',\n",
       "  'description': 'Cluster ini membahas vaksinasi pada hewan ternak (sapi, ayam) dan hewan peliharaan (anjing, kucing) untuk mencegah penyakit seperti PMK, rabies, dan penyakit unggas. Sentimennya netral hingga positif, menekankan pentingnya vaksinasi untuk kesehatan hewan dan mencegah penularan penyakit ke manusia. Dampaknya adalah perlindungan populasi hewan dan stabilitas ekonomi peternakan.'},\n",
       " {'unified_issue': 'Vaksin untuk Perjalanan: Haji dan Umrah',\n",
       "  'description': 'Cluster ini membahas persyaratan vaksinasi (meningitis, polio) untuk jamaah haji dan umrah, serta informasi terkait paket perjalanan dan kemudahan yang ditawarkan. Sentimennya netral hingga positif, berfokus pada informasi praktis dan persiapan perjalanan ibadah. Dampaknya adalah kelancaran perjalanan ibadah dan pencegahan penyebaran penyakit di Tanah Suci.'},\n",
       " {'unified_issue': 'Vaksin dan Program Pemerintah: Dukungan dan Penolakan',\n",
       "  'description': 'Cluster ini mencakup isu-isu terkait dukungan pemerintah terhadap program vaksinasi, penolakan vaksin oleh sebagian masyarakat (terutama di Papua), dan efisiensi anggaran yang berdampak pada layanan kesehatan. Sentimennya bervariasi, dari positif (dukungan program) hingga negatif (penolakan, kekhawatiran anggaran). Dampaknya adalah keberhasilan program kesehatan vs. resistensi dan kesenjangan layanan.'},\n",
       " {'unified_issue': 'Vaksin HPV: Harga, Keperluan, dan Kesadaran Masyarakat',\n",
       "  'description': 'Cluster ini membahas vaksin HPV, termasuk harga, kebutuhan, dan kesadaran masyarakat tentang pentingnya vaksin ini untuk mencegah kanker serviks. Sentimennya umumnya positif, menekankan pentingnya pencegahan daripada pengobatan. Dampaknya adalah peningkatan kesadaran dan cakupan vaksinasi HPV, terutama pada wanita.'},\n",
       " {'unified_issue': 'Kerjasama Internasional: Investasi dan Produksi Vaksin',\n",
       "  'description': 'Cluster ini membahas kerjasama antara Indonesia dan negara lain (Vietnam, Turki) dalam pengembangan dan produksi vaksin hewan, serta investasi asing (Bill Gates, Sanofi) di sektor kesehatan Indonesia. Sentimennya umumnya positif, menekankan manfaat kerjasama untuk kemajuan teknologi dan kesehatan. Dampaknya adalah peningkatan kapasitas produksi vaksin dalam negeri dan transfer teknologi.'},\n",
       " {'unified_issue': 'Isu Negatif Vaksin: Hoax, Pelecehan, dan Kejahatan',\n",
       "  'description': 'Cluster ini mencakup isu-isu negatif terkait vaksin, seperti disinformasi online, kasus pelecehan seksual oleh dokter dengan modus vaksin, dan pemalsuan vaksin. Sentimennya sangat negatif, menyoroti potensi bahaya dan penyalahgunaan vaksin. Dampaknya adalah penurunan kepercayaan publik terhadap vaksin dan profesi medis.'},\n",
       " {'unified_issue': 'Kondisi Kesehatan dan Kelayakan Vaksinasi',\n",
       "  'description': 'Cluster ini membahas kondisi kesehatan tertentu (HBsAg positif, HIV/AIDS) dan dampaknya terhadap kelayakan vaksinasi. Sentimennya netral, berfokus pada informasi medis dan pertimbangan kesehatan. Dampaknya adalah pemahaman yang lebih baik tentang kontraindikasi vaksinasi.'},\n",
       " {'unified_issue': 'Program Vaksinasi Lokal dan Inisiatif Pemerintah',\n",
       "  'description': 'Cluster ini berfokus pada pelaksanaan program vaksinasi di tingkat lokal, seperti di kelurahan, dan regulasi pemerintah terkait vaksin. Sentimennya netral, berfokus pada implementasi dan kebijakan. Dampaknya adalah peningkatan akses vaksin dan kepatuhan terhadap regulasi.'},\n",
       " {'unified_issue': 'Fasilitas Kesehatan: Klinik Vaksin Ramah Anak',\n",
       "  'description': 'Cluster ini berfokus pada promosi klinik yang menyediakan layanan vaksinasi yang nyaman dan ramah anak. Sentimennya positif, menekankan kenyamanan dan aksesibilitas. Dampaknya adalah peningkatan kunjungan vaksinasi.'},\n",
       " {'unified_issue': 'Kebingungan dan Informasi Vaksin di Masyarakat',\n",
       "  'description': 'Cluster ini mencakup kebingungan masyarakat terkait vaksin, termasuk informasi yang simpang siur dan pertanyaan seputar vaksinasi. Sentimen cenderung netral hingga negatif karena ketidakpastian. Dampaknya adalah potensi keraguan terhadap vaksin dan perlunya edukasi yang lebih baik.'},\n",
       " {'unified_issue': 'Tokoh Publik dan Vaksin: Dukungan dan Kontroversi',\n",
       "  'description': 'Cluster ini mencakup vaksinasi yang dilakukan oleh tokoh publik dan kontroversi terkait, seperti vaksin Nusantara. Sentimennya beragam, dari dukungan hingga skeptisisme. Dampaknya adalah potensi pengaruh terhadap opini publik tentang vaksin.'}]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_unified_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ed3adfa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:59:51.659843Z",
     "start_time": "2025-05-26T16:59:47.913655Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data/_search [status:200 duration:3.723s]\n"
     ]
    }
   ],
   "source": [
    "data = get_data_post(list_issue_done,keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5357ec",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-26T17:02:40.948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- vaksin -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST http://34.101.178.71:9200/tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data/_search [status:200 duration:5.465s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process (1000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT http://34.101.178.71:9200/_bulk [status:200 duration:0.059s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil ingest 4 dokumen ke index 'topics-overview'\n",
      "mapped 4 kelompok 110\n",
      "process (890, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        print('-----',project_name,'-----')\n",
    "        issue_map = topics_map[['unified_issue','description']].to_dict(orient = 'records')\n",
    "        list_issue_done = list(set([j for i in topics_map['list_issue'] for j in i]))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_issue = get_data_post(list_issue_done,keywords)\n",
    "        df_issue['index'] = df_issue['index'].astype(str)\n",
    "\n",
    "        while True:\n",
    "            print('process', df_issue.shape)\n",
    "            df_final = process_issue(project_name,df_issue, issue_map)\n",
    "            print('mapped', df_final.shape[0],'kelompok',len(set([j for i in df_final['list_issue'] for j in i])))\n",
    "            df_issue = df_issue[~df_issue['issue'].isin([j for i in df_final['list_issue'] for j in i])]\n",
    "            if df_issue.empty:\n",
    "                break\n",
    "            issue_map.extend(df_final[['unified_issue','description']].to_dict(orient = 'records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c8067ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T17:00:01.346618Z",
     "start_time": "2025-05-26T17:00:01.337186Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ex-minister urges vaccine transparency'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['issue'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c12941a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T16:56:09.405221Z",
     "start_time": "2025-05-26T16:56:09.400204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktok_data,instagram_data,news_data,reddit_data,facebook_data,twitter_data,linkedin_data,youtube_data'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = ['tiktok','instagram','news','reddit','facebook','twitter','linkedin','youtube']\n",
    "\n",
    "index = ','.join([i+'_data' for i in channels])\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba048e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "  \"size\":0,\n",
    "  \"query\": {\n",
    "   \"bool\":{\n",
    "       \"should\":[\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"post_caption\": {\n",
    "                  \"query\": i,\n",
    "                  \"operator\": \"AND\"\n",
    "                }\n",
    "              }\n",
    "            } for i in keywords\n",
    "          ]\n",
    "   } \n",
    "  }, \n",
    "  \"aggs\": {\n",
    "    \"by_issue\": {\n",
    "      \"terms\": {\n",
    "        \"field\": \"issue.keyword\",\n",
    "        \"size\": 1000,\n",
    "        \"order\": {\n",
    "          \"avg_influence_score\": \"desc\"\n",
    "        }\n",
    "      },\n",
    "      \"aggs\": {\n",
    "        \"avg_influence_score\": {\n",
    "          \"avg\": {\n",
    "            \"script\": script_score\n",
    "          }\n",
    "        },\n",
    "        \"sample_posts\": {\n",
    "          \"top_hits\": {\n",
    "            \"size\": 2,\n",
    "            \"_source\": {\n",
    "              \"includes\": [\n",
    "                \"post_caption\"\n",
    "              ]\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
